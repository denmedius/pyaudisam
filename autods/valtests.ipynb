{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Auto table of contents -->\n",
    "<h1 class='tocIgnore'>AutoDS : Validation tests</h1>\n",
    "<p>(for the <b>autods</b> module, a python interface to MCDS.exe, http://distancesampling.org/)</p>\n",
    "<div style=\"overflow-y: auto\">\n",
    "  <h2 class='tocIgnore'>Table of contents</h2>\n",
    "  <div id=\"toc\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib as pl\n",
    "import importlib as implib\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import OrderedDict as odict, namedtuple as ntuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML, Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as plygo\n",
    "import plotly.express as plyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autods as ads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration.\n",
    "ads.log.configure(handlers=[sys.stdout, 'tmp/valtst.log'], verbose=True, reset=True)\n",
    "\n",
    "ads.logger('matplotlib', level=ads.WARNING, reset=True)\n",
    "\n",
    "ads.logger('ads', level=ads.INFO1, reset=True)\n",
    "#ads.logger('ads.eng', level=ads.WARNING, reset=True)  # Needs to be forced ('cause its level is set before ads, see engine.py)\n",
    "#ads.logger('ads.exr', level=ads.DEBUG, reset=True)\n",
    "#ads.logger('ads.rep', level=ads.DEBUG, reset=True)\n",
    "#ads.logger('ads.opn', level=ads.DEBUG, reset=True)\n",
    "#ads.logger('ads.opr', level=ads.DEBUG, reset=True)\n",
    "#ads.logger('ads.anr', level=ads.DEBUG, reset=True)\n",
    "\n",
    "logger = ads.logger('valtst', level=ads.DEBUG, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Warnings as Exceptions\n",
    "if False:\n",
    "    \n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(action='error')\n",
    "\n",
    "    # pd.read_excel\n",
    "    warnings.filterwarnings(action='default', module='etree')\n",
    "    warnings.filterwarnings(action='default', module='xlrd')\n",
    "    warnings.filterwarnings(action='default', module='defusedxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short string for sample \"identification\"\n",
    "def sampleAbbrev(sSample):\n",
    "    \n",
    "    abrvSpe = ''.join(word[:4].title() for word in sSample['Espèce'].split(' ')[:2])\n",
    "    \n",
    "    sampAbbrev = '{}-{}-{}-{}'.format(abrvSpe, sSample.Passage.replace('+', ''),\n",
    "                                      sSample.Adulte.replace('+', ''), sSample['Durée'])\n",
    "    \n",
    "    return sampAbbrev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short string for analysis \"identification\"\n",
    "def analysisAbbrev(sAnlys):\n",
    "    \n",
    "    # Sample abbreviation\n",
    "    abbrevs = [sampleAbbrev(sAnlys)]\n",
    "\n",
    "    # Model + Parameters abbreviation\n",
    "    abbrevs += [sAnlys['FonctionClé'][:3].lower(), sAnlys['SérieAjust'][:3].lower()]\n",
    "    dTroncAbrv = { 'l': 'TrGche' if 'TrGche' in sAnlys.index else 'TroncGche',\n",
    "                   'r': 'TrDrte' if 'TrDrte' in sAnlys.index else 'TroncDrte',\n",
    "                   'm': 'NbTrches' if 'NbTrches' in sAnlys.index else 'NbTrModel'\n",
    "                                   if 'NbTrModel' in sAnlys.index else  'NbTrchMod',\n",
    "                   'd': 'NbTrDiscr' }\n",
    "    for abrv, name in dTroncAbrv.items():\n",
    "        if name in sAnlys.index and not pd.isnull(sAnlys[name]):\n",
    "            abbrevs.append('{}{}'.format(abrv, sAnlys[name][0].lower() if isinstance(sAnlys[name], str)\n",
    "                                               else int(sAnlys[name])))\n",
    "   \n",
    "    return '-'.join(abbrevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Run analyses with real life field data (1/2 : long code, long run)\n",
    "\n",
    "With MCDSAnalysis class.\n",
    "\n",
    "(for comparison to manually issued analyses with Distance 7.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load analyses set specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load refout results table\n",
    "refFileName = 'ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx'\n",
    "dfRefRes = pd.read_excel(pl.Path('refout', refFileName))\n",
    "dfRefRes.reset_index(inplace=True) # Generate analysis # (later need for original cases order)\n",
    "dfRefRes.rename(columns=dict(index='AnlysNum', Name='Model'), inplace=True)\n",
    "\n",
    "dfRefRes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate test cases definition code from refout results file (don't cheat : only input columns :-)\n",
    "modelIdCols = ['Model']\n",
    "modelParamCols = ['LTrunc', 'RTrunc', 'FitDistCuts', 'DiscrDistCuts']\n",
    "sampleSelCols = ['Species', 'Periods', 'Prec.', 'Duration']\n",
    "caseIdCols = ['AnlysNum', 'SampNum'] + sampleSelCols + modelIdCols\n",
    "\n",
    "dfRefRes['SampNum'] = dfRefRes.groupby(sampleSelCols, sort=False).ngroup()\n",
    "\n",
    "dfAnlysCases = dfRefRes[caseIdCols + modelParamCols].copy()\n",
    "\n",
    "dfAnlysCases['KeyFn'] = \\\n",
    "    dfAnlysCases.Model.apply(lambda s: 'UNIFORM' if s.startswith('Unif') \\\n",
    "                                                 else 'HNORMAL' if s.startswith('Half') else 'HAZARD')\n",
    "dfAnlysCases['AdjSer'] = \\\n",
    "    dfAnlysCases.Model.apply(lambda s: 'COSINE' if s.find(' Cos') > 0 \\\n",
    "                                                else 'POLY' if s.find(' SimPoly') > 0 else 'HERMITE')\n",
    "dfAnlysCases['InFileName'] = \\\n",
    "    dfAnlysCases.apply(lambda sRow: 'ACDC2019-Papyrus-{}-{}-{}mn-{}dec-dist.txt' \\\n",
    "                                    .format(sRow.Species,\n",
    "                                            'AB' if 'A+B' in sRow.Periods else 'A' if 'A' in sRow.Periods else 'B',\n",
    "                                            sRow.Duration.split(' ')[0], sRow['Prec.'].split(' ')[0]),\n",
    "                       axis='columns')\n",
    "dfAnlysCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def nan2None(v):\n",
    "#    return None if pd.isnull(v) else v\n",
    "def distCutsFromSpecs(v):\n",
    "    if pd.isnull(v):\n",
    "        return None\n",
    "    if isinstance(v, int):\n",
    "        return v\n",
    "    return [float(x) for x in v.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimalFields = ['Point transect*Survey effort', 'Observation*Radial distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis engine (sequential)\n",
    "mcds = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'),\n",
    "                      executor=None, # Non-parallel: ~7.5s elapsed on a Lenovo P52 (6-core i7-8850H with PCI-e SSD)\n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen analysis parameters (a choice here)\n",
    "KEstimCriterion = 'AIC'\n",
    "KCVInterval = 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results object construction\n",
    "sampCols = [('sample', col, 'Value') for col in sampleSelCols]\n",
    "miSampCols = pd.MultiIndex.from_tuples(sampCols)\n",
    "\n",
    "custCols = [('sample', 'AnlysNum', 'Value'), ('sample', 'SampNum', 'Value')] + sampCols + [('model', 'Model', 'Value')]\n",
    "miCustCols = pd.MultiIndex.from_tuples(custCols)\n",
    "\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=caseIdCols, fr=['NumAnlys', 'NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Modèle']))\n",
    "\n",
    "results = ads.MCDSAnalysisResultsSet(miCustomCols=miCustCols, miSampleCols=miSampCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                     distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                     surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Or : Really run analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten test cases and reference results lists, to go faster\n",
    "# Warning: If you don't retain entire samples, later comparison will fail on Delta AIC values.\n",
    "#selCaseInds = [0, 5, 7, 22, 31] # Some random cases, with uncomplete samples.\n",
    "#selCaseInds = dfAnlysCases[dfAnlysCases.Sample.isin([3, 4])].index # A shorter selection, with complete samples.\n",
    "selCaseInds = range(len(dfAnlysCases)) # All of them.\n",
    "\n",
    "nOrigAnlysCases = len(dfAnlysCases)\n",
    "dfAnlysCases = dfAnlysCases.loc[selCaseInds]\n",
    "dfRefRes = dfRefRes.loc[selCaseInds]\n",
    "\n",
    "logger.info(f'Retained {len(selCaseInds)} out of {nOrigAnlysCases}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run all analyses\n",
    "lastInFileName = None\n",
    "for _, sCase in dfAnlysCases.iterrows():\n",
    "    \n",
    "    nCase = sCase.AnlysNum\n",
    "    name = sCase.InFileName[len('ACDC2019-Papyrus')+1:-len('-dist.txt')]\n",
    "    name += '-' + sCase.Model.lower().translate(str.maketrans({c:'-' for c in ' ,.:;()/'}))\n",
    "    logger.info(f'#{nCase+1:3d} {name} {sCase.KeyFn} {sCase.AdjSer}')\n",
    "    \n",
    "    # Create data set if not already done.\n",
    "    if lastInFileName != sCase.InFileName:\n",
    "        sds = ads.SampleDataSet(pl.Path('refin', sCase.InFileName), decimalFields=decimalFields)\n",
    "        lastInFileName = sCase.InFileName\n",
    "        \n",
    "    # Run analysis and get results\n",
    "    anlys = ads.MCDSAnalysis(engine=mcds, sampleDataSet=sds, name=name, logData=True,\n",
    "                             estimKeyFn=sCase.KeyFn, estimAdjustFn=sCase.AdjSer,\n",
    "                             estimCriterion=KEstimCriterion, cvInterval=KCVInterval,\n",
    "                             minDist=sCase.LTrunc, maxDist=sCase.RTrunc,\n",
    "                             fitDistCuts=distCutsFromSpecs(sCase.FitDistCuts),\n",
    "                             discrDistCuts=distCutsFromSpecs(sCase.DiscrDistCuts))\n",
    "\n",
    "    anlys.submit()\n",
    "    sResult = anlys.getResults()\n",
    "\n",
    "    # Save results\n",
    "    sHead = pd.Series(data=[sCase[col] for col in sCase.index[:len(caseIdCols)]], index=miCustCols)\n",
    "\n",
    "    results.append(sResult, sCustomHead=sHead)\n",
    "    \n",
    "# shutdown analysis engine\n",
    "mcds.shutdown()\n",
    "\n",
    "# Done.\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results in case need for not recomputing them\n",
    "resFileName = os.path.join(mcds.workDir, 'autods-validation-results.xlsx')\n",
    "\n",
    "results.toExcel(resFileName, sheetName='AutoDSVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resFileName = os.path.join(mcds.workDir, 'autods-validation-results-en.xlsx')\n",
    "\n",
    "results.toExcel(resFileName, sheetName='Auto', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check translation\n",
    "dfActTrRes = results.dfTransData('fr')\n",
    "\n",
    "dfActTrRes.head().T.iloc[:30] #.at['TroncGche', 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    resFileName = os.path.join(mcds.workDir, 'autods-validation-results.xlsx')\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName, sheetName='AutoDSVal')\n",
    "    \n",
    "    # shutdown analysis engine\n",
    "    mcds.shutdown()\n",
    "\n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Compare actual results to reference\n",
    "\n",
    "(reference = manually run analyses with Distance software)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract actual results to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Analysis results\n",
    "dfActRes = results.dfData\n",
    "\n",
    "dfActRes.head().T[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of auto-results and match them with reference ones, for comparison.\n",
    "dCompCols = \\\n",
    "{\n",
    "    ('sample', 'AnlysNum', 'Value'):  'AnlysNum',\n",
    "    ('sample', 'SampNum', 'Value'):   'SampNum',\n",
    "    ('sample', 'Species', 'Value'):   'Species',\n",
    "    ('sample', 'Periods', 'Value'):   'Periods',\n",
    "    ('sample', 'Prec.', 'Value'):     'Prec.',\n",
    "    ('sample', 'Duration', 'Value'):  'Duration',\n",
    "    \n",
    "    ('model',  'Model', 'Value'):         'Model',\n",
    "    ('parameters', 'left truncation distance', 'Value'):           'LTrunc',\n",
    "    ('parameters', 'right truncation distance', 'Value'):          'RTrunc',\n",
    "    ('parameters', 'model fitting distance cut points', 'Value'):  'FitDistCuts',\n",
    "    ('parameters', 'distance discretisation cut points', 'Value'): 'DiscrDistCuts',\n",
    "    \n",
    "    ('run output', 'run status', 'Value') : 'Status',\n",
    "    #('run output', 'run time', 'Value') : 'Run', # Only for unintests ref. generation just below\n",
    "    \n",
    "    ('detection probability', 'total number of parameters (m)', 'Value'): '# params',\n",
    "    ('encounter rate', 'number of observations (n)', 'Value'): '# obs',\n",
    "    \n",
    "    #('detection probability', 'Delta AIC', 'Value'): 'Delta AIC',\n",
    "    ('detection probability', 'AIC value', 'Value'): 'AIC',\n",
    "    ('detection probability', 'chi-square test probability determined', 'Value')               : 'GOF Chi-p',\n",
    "    ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value')                  : 'GOF K-S p',\n",
    "    ('detection probability', 'Cramér-von Mises (uniform weighting) test probability', 'Value'): 'GOF CvM (unif) p',\n",
    "    ('detection probability', 'Cramér-von Mises (cosine weighting) test probability', 'Value') : 'GOF CvM (cos) p',\n",
    "    \n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value'): 'ESW/EDR',\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Lcl')  : 'ESW/EDR LCL',\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Ucl')  : 'ESW/EDR UCL',\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Cv')   : 'ESW/EDR CV',\n",
    "    \n",
    "    ('density/abundance', 'density of animals', 'Value'): 'D',\n",
    "    ('density/abundance', 'density of animals', 'Lcl')  : 'D LCL',\n",
    "    ('density/abundance', 'density of animals', 'Ucl')  : 'D UCL',\n",
    "    ('density/abundance', 'density of animals', 'Cv')   : 'D CV',\n",
    "    \n",
    "    ('detection probability', 'probability of detection (Pw)', 'Value'): 'P',\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Lcl')  : 'P LCL',\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Ucl')  : 'P UCL',\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Cv')   : 'P CV',\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Df')   : 'P DF',\n",
    "}\n",
    "len(dCompCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Unused columns (full of NaNs) have been atomatically removed\n",
    "# (see last line of AnalysisResultsSet.dfData getter)\n",
    "dCompCols = { k: v for k, v in dCompCols.items() if k in dfActRes.columns }\n",
    "len(dCompCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we need to cleanup modelParamCols too\n",
    "modelParamCols = [id_ for id_ in modelParamCols if id_ in dCompCols.values()]\n",
    "len(modelParamCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safe stringification of model params\n",
    "# * needed for use in indexes (hashability)\n",
    "# * needed to cope with to_excel/read_excel unconsistent None management\n",
    "def modelParam2Str(par):\n",
    "    #print(par)\n",
    "    if isinstance(par, list):\n",
    "        spar = str([float(v) for v in par])\n",
    "    elif pd.isnull(par):\n",
    "        spar = 'None'\n",
    "    elif isinstance(par, str):\n",
    "        if ',' in par: # Assumed already somewhat stringified list\n",
    "            spar = str([float(v) for v in par.strip('[]').split(',')])\n",
    "    else:\n",
    "        spar = str(par)\n",
    "    return spar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select results columns and rename them as the reference is, for easier comparison\n",
    "dfActRes4c = dfActRes[list(dCompCols.keys())].copy()\n",
    "dfActRes4c.columns = [dCompCols[col] for col in dCompCols]\n",
    "dfActRes4c[modelParamCols] = dfActRes4c[modelParamCols].applymap(modelParam2Str) # Hashable mandatory for indexing\n",
    "dfActRes4c.set_index(caseIdCols + modelParamCols, inplace=True)\n",
    "\n",
    "dfActRes4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select usefull reference columns for comparison\n",
    "dfRefRes4c = dfRefRes.copy()\n",
    "dfRefRes4c[modelParamCols] = dfRefRes4c[modelParamCols].applymap(modelParam2Str) # Hashable mandatory for indexing\n",
    "dfRefRes4c.set_index(caseIdCols + modelParamCols, inplace=True)\n",
    "dfRefRes4c = dfRefRes4c.reindex(columns=dfActRes4c.columns)\n",
    "\n",
    "dfRefRes4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfActRes4c.to_excel('tmp/act-res.xlsx')\n",
    "#dfRefRes4c.to_excel('tmp/ref-res.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Automated diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checks : equality of test case lists (index) and of column names (columns)\n",
    "assert sorted(dfActRes4c.index)   == sorted(dfRefRes4c.index)\n",
    "assert sorted(dfActRes4c.columns) == sorted(dfRefRes4c.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual / reference closeness measure : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)\n",
    "# = Compute the order of magnitude that separate the difference from the absolute max. of the two values\n",
    "# The greater it is, the lower the relative difference\n",
    "#    Ex: 3 = 10**3 ratio between difference absolue max. of the two,\n",
    "#        +inf = NO difference at all,\n",
    "#        0 = bad, one of the two is 0, and the other not,\n",
    "# See unitary test below.\n",
    "def closeness(sRefAct):\n",
    "    \n",
    "    x, y = sRefAct.to_list()\n",
    "    \n",
    "    # Special cases with 1 NaN, or 1 or more inf => all different\n",
    "    if np.isnan(x):\n",
    "        if not np.isnan(y):\n",
    "            return 0 # All different\n",
    "    elif np.isnan(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    if np.isinf(x) or np.isinf(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    # Normal case\n",
    "    c = abs(x - y)\n",
    "    if not np.isnan(c) and c != 0:\n",
    "        c = c / max(abs(x), abs(y))\n",
    "    \n",
    "    return np.inf if c == 0 else round(-np.log10(c), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Actual / reference comparison : compute closeness indicator\n",
    "dfRelDif = dfRefRes4c.copy()\n",
    "for col in dfRelDif.columns:\n",
    "    dfRelDif['act'] = dfActRes4c[col]\n",
    "    dfRelDif[col] = dfRelDif[[col, 'act']].apply(closeness, axis='columns')\n",
    "    dfRelDif.drop(columns='act', inplace=True)\n",
    "    \n",
    "dfRelDif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnosis : we only keep lines and columns with some relevant differences.\n",
    "dfBadRelDif = dfRelDif.copy()\n",
    "len(dfBadRelDif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Suppress rows : Same status and NaNs ibn the remainder (if status == 0/3/4, execution error or no execution)\n",
    "valCols = [col for col in dfRelDif.columns if col != 'Status']\n",
    "dfBadRelDif.drop(dfBadRelDif[(dfBadRelDif.Status.abs() == np.inf) & dfBadRelDif[valCols].isnull().all(axis='columns')].index,\n",
    "                 axis='index', inplace=True)\n",
    "assert len(dfBadRelDif) == 29, len(dfBadRelDif)\n",
    "anlysNums = dfBadRelDif.index.get_level_values('AnlysNum').to_list()\n",
    "assert anlysNums == [0, 1, 2, 3, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 17,\n",
    "                     18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], anlysNums\n",
    "print(len(dfBadRelDif), 'analyses:', ', '.join(map(str, anlysNums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Suppress rows : Status and all other columns == inf (<=> strict equality)\n",
    "#    NB. Some very small differences observed when results have just been computed or when they have been\n",
    "#        loaded from a previously saved Excel file (above 10**15 closeness value)\n",
    "dfBadRelDif.drop(dfBadRelDif[dfBadRelDif.apply(np.isinf, axis='columns').all(axis='columns')].index,\n",
    "                 axis='index', inplace=True)\n",
    "assert (computed and len(dfBadRelDif) == 26) or (not computed and len(dfBadRelDif) == 17), len(dfBadRelDif)\n",
    "anlysNums = dfBadRelDif.index.get_level_values('AnlysNum').to_list()\n",
    "assert (computed and anlysNums == [0, 1, 2, 5, 6, 7, 8, 9, 13, 14, 15, 17, 18,\n",
    "                                   19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]) \\\n",
    "       or (not computed and anlysNums == [0, 1, 2, 7, 8, 9, 13, 14, 15, 19, 20, 23, 25, 27, 28, 29, 30]), \\\n",
    "       anlysNums\n",
    "print(len(dfBadRelDif), 'analyses:', ', '.join(map(str, anlysNums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Suppress rows : Status and all other columns >= à 15 (<=> nearly strict equality)\n",
    "dfBadRelDif.drop(dfBadRelDif[(dfBadRelDif >= 15).all(axis='columns')].index, axis='index', inplace=True)\n",
    "assert len(dfBadRelDif) == 5, len(dfBadRelDif)\n",
    "anlysNums = dfBadRelDif.index.get_level_values('AnlysNum').to_list()\n",
    "assert anlysNums == [9, 20, 28, 29, 30], anlysNums\n",
    "print(len(dfBadRelDif), 'analyses:', ', '.join(map(str, anlysNums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Suppress rows : Same status and all other columns >= 4 (<=> close to equality)\n",
    "dfBadRelDif.drop(dfBadRelDif[(dfBadRelDif >= 4).all(axis='columns')].index, axis='index', inplace=True)\n",
    "assert len(dfBadRelDif) == 4, len(dfBadRelDif)\n",
    "anlysNums = dfBadRelDif.index.get_level_values('AnlysNum').to_list()\n",
    "assert anlysNums == [9, 20, 28, 30], anlysNums\n",
    "print(len(dfBadRelDif), 'analyses:', ', '.join(map(str, anlysNums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Suppress rows : Same status and all other columns >= 4 (<=> close to equality)\n",
    "#                    except for GOF KS and CvM, equal to NaN, because not computed when distances are discretised.\n",
    "if 'DiscrDistCuts' in dfBadRelDif.index.names:\n",
    "    discrCols = [col for col in dfRelDif.columns if not col.startswith('GOF') or col.find('Chi') > 0]\n",
    "    df2Drop = (dfBadRelDif.index.get_level_values('DiscrDistCuts') != -1) & (dfBadRelDif[discrCols] >= 4).all(axis='columns')\n",
    "    dfBadRelDif.drop(dfBadRelDif[df2Drop].index, axis='index', inplace=True)\n",
    "assert len(dfBadRelDif) == 2, len(dfBadRelDif)\n",
    "anlysNums = dfBadRelDif.index.get_level_values('AnlysNum').to_list()\n",
    "assert anlysNums == [9, 30], anlysNums\n",
    "print(len(dfBadRelDif), 'analyses:', ', '.join(map(str, anlysNums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verdict (Cf. refFileName Excel file, sheet \"DiffAuto\" for explanations about the 2 different rows between Act/Ref)\n",
    "dfBadRelDif.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRefRes4c.loc[dfBadRelDif.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFails = len(dfBadRelDif.index)\n",
    "if nFails > 0:\n",
    "    print(f'Warning: {nFails} test case(s) failed ;')\n",
    "    print(f' ... see sheet \"DiffAuto\" of {refFileName} for possible explanations.')\n",
    "else:\n",
    "    print('All test cases succeeded !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save results to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resCompFileName = os.path.join(mcds.workDir, 'autods-validation-rescomp.xlsx')\n",
    "\n",
    "with pd.ExcelWriter(resCompFileName) as xlsxWriter:\n",
    "\n",
    "    dfRefRes.to_excel(xlsxWriter, sheet_name='RefResults', index=True)\n",
    "    dfActRes4c.reset_index().to_excel(xlsxWriter, sheet_name='ActResults', index=False)\n",
    "    dfRelDif.reset_index().to_excel(xlsxWriter, sheet_name='Diff2Ref', index=False)\n",
    "    dfBadRelDif.reset_index().to_excel(xlsxWriter, sheet_name='BadDiff2Ref', index=False)\n",
    "    dfRefRes4c.loc[dfBadRelDif.index].reset_index().to_excel(xlsxWriter, sheet_name='RefResWithDiff', index=False)\n",
    "    dfActRes4c.loc[dfBadRelDif.index].reset_index().to_excel(xlsxWriter, sheet_name='ActResWithDiff', index=False)\n",
    "    dfActRes.to_excel(xlsxWriter, sheet_name='RawActResults', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfActRes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Parallel run of same analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare analyses\n",
    "\n",
    "(same test cases and input data as previously, for easy comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis executor : 6, 8, None threads => min elapsed = ~2s on a Lenovo P52 (6-core i7-8850H with PCI-e SSD)\n",
    "parallelExecutor = ads.Executor(threads=6)\n",
    "\n",
    "# Analysis engine\n",
    "mcds = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'), executor=parallelExecutor, \n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results object construction\n",
    "parResults = ads.MCDSAnalysisResultsSet(miCustomCols=miCustCols, miSampleCols=miSampCols, dfCustomColTrans=dfCustColTrans, \n",
    "                                        distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                        surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Or : Really run analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten test cases and reference results lists, to go faster\n",
    "# Warning: If you don't retain entire samples, later comparison will fail on Delta AIC values.\n",
    "#selCaseInds = [0, 5, 7, 22, 31] # Some random cases, with uncomplete samples.\n",
    "#selCaseInds = dfAnlysCases[dfAnlysCases.Sample.isin([3, 4])].index # A shorter selection, with complete samples.\n",
    "selCaseInds = range(len(dfAnlysCases)) # All of them.\n",
    "\n",
    "nOrigAnlysCases = len(dfAnlysCases)\n",
    "dfAnlysCases = dfAnlysCases.loc[selCaseInds]\n",
    "dfRefRes = dfRefRes.loc[selCaseInds]\n",
    "\n",
    "logger.info(f'Retained {len(selCaseInds)} out of {nOrigAnlysCases}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Start running all analyses\n",
    "lastInFileName = None\n",
    "analyses = dict()\n",
    "for _, sCase in dfAnlysCases.iterrows():\n",
    "    \n",
    "    nCase = sCase.AnlysNum\n",
    "    name = sCase.InFileName[len('ACDC2019-Papyrus')+1:-len('-dist.txt')]\n",
    "    name += '-' + sCase.Model.lower().translate(str.maketrans({c:'-' for c in ' ,.:;()/'}))\n",
    "    logger.info(f'#{nCase+1:3d} {name} {sCase.KeyFn} {sCase.AdjSer}')\n",
    "    \n",
    "    # Create data set if not already done.\n",
    "    if lastInFileName != sCase.InFileName:\n",
    "        sds = ads.SampleDataSet(pl.Path('refin', sCase.InFileName), decimalFields=decimalFields)\n",
    "        lastInFileName = sCase.InFileName\n",
    "        \n",
    "    # Start running analysis in parallel (don't wait for it's finished, go on)\n",
    "    sResHead = pd.Series(data=[sCase[col] for col in sCase.index[:len(caseIdCols)]], index=miCustCols)\n",
    "\n",
    "    anlys = ads.MCDSAnalysis(engine=mcds, sampleDataSet=sds, name=name, customData=sResHead, logData=True,\n",
    "                             estimKeyFn=sCase.KeyFn, estimAdjustFn=sCase.AdjSer,\n",
    "                             estimCriterion=KEstimCriterion, cvInterval=KCVInterval,\n",
    "                             minDist=sCase.LTrunc, maxDist=sCase.RTrunc,\n",
    "                             #minDist=nan2None(sCase.LTrunc), maxDist=nan2None(sCase.RTrunc),\n",
    "                             fitDistCuts=distCutsFromSpecs(sCase.FitDistCuts), # TODO: do this when building dfAnlysCases\n",
    "                             discrDistCuts=distCutsFromSpecs(sCase.DiscrDistCuts))\n",
    "    anlysFut = anlys.submit()\n",
    "    \n",
    "    # Store analysis object and associated \"future\" for later use (should be running soon or later).\n",
    "    analyses[anlysFut] = anlys\n",
    "    \n",
    "logger.info('All analyses started ; now waiting for their end, and results ...')\n",
    "\n",
    "# For each analysis as it gets completed (first completed => first yielded)\n",
    "for anlysFut in parallelExecutor.asCompleted(analyses):\n",
    "\n",
    "    # Retrieve analysis object from its associated future object\n",
    "    anlys = analyses[anlysFut]\n",
    "    \n",
    "    # Get analysis results\n",
    "    sResult = anlys.getResults()\n",
    "\n",
    "    # Save results with header\n",
    "    parResults.append(sResult, sCustomHead=anlys.customData)\n",
    "    \n",
    "# shutdown analysis engine\n",
    "mcds.shutdown()\n",
    "\n",
    "# Done.\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results in case need for not recomputing them\n",
    "resFileName = os.path.join(mcds.workDir, 'autods-validation-parallel-results.xlsx')\n",
    "\n",
    "parResults.toExcel(resFileName, sheetName='AutoDSVal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    resFileName = os.path.join(mcds.workDir, 'autods-validation-parallel-results.xlsx')\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    parResults.fromExcel(resFileName, sheetName='AutoDSVal')\n",
    "    \n",
    "    # shutdown analysis engine\n",
    "    mcds.shutdown()\n",
    "\n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print(f'... {len(parResults)} analyses to compare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare parallel results to sequential ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare sequential results for comparison\n",
    "dfSeqCmpRes = results.dfTransData('en')\n",
    "\n",
    "dfSeqCmpRes.fillna(-9999, inplace=True) # Get rid of the Nan pb (because NaN != NaN :-)\n",
    "\n",
    "dfSeqCmpRes.drop(columns=['RunTime', 'RunFolder'], inplace=True) # Run date-time and folder can never be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare parallel results for comparison\n",
    "dfParCmpRes = parResults.dfTransData('en')\n",
    "\n",
    "dfParCmpRes.sort_values(by='AnlysNum', inplace=True) # Back to original test case order = sequential run order\n",
    "\n",
    "dfParCmpRes.reset_index(inplace=True, drop=True) # Enforce same index as a consequence\n",
    "\n",
    "dfParCmpRes.fillna(-9999, inplace=True) # And get rid of the Nan pb (because NaN != Nan :-)\n",
    "\n",
    "dfParCmpRes.drop(columns=['RunTime', 'RunFolder'], inplace=True) # Run date-time and folder can never be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Doesn't work if 1 of the 2 sets (not the 2) was loaded from disk (Excel numerical rounding stuff)\n",
    "assert (dfSeqCmpRes == dfParCmpRes).all().all(), \\\n",
    "       'Oh, oh, something went differently when run parallely ... but due to one results set loaded from disk ?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = parResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Excel and HTML reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes pour les tableaux de synthèse du rapport\n",
    "synthRepCols = \\\n",
    "[\n",
    "    ('sample', 'AnlysNum', 'Value'),\n",
    "    ('sample', 'SampNum', 'Value'),\n",
    "    ('sample', 'Species', 'Value'),\n",
    "    ('sample', 'Periods', 'Value'),\n",
    "    ('sample', 'Prec.', 'Value'),\n",
    "    ('sample', 'Duration', 'Value'),\n",
    "    \n",
    "    ('model', 'Model', 'Value'),\n",
    "    ('parameters', 'left truncation distance', 'Value'),\n",
    "    ('parameters', 'right truncation distance', 'Value'),\n",
    "    ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "    ('parameters', 'distance discretisation cut points', 'Value'),\n",
    "    \n",
    "    ('run output', 'run status', 'Value'),\n",
    "    \n",
    "    ('encounter rate', 'number of observations (n)', 'Value'),\n",
    "    ('encounter rate', 'effort (L or K or T)', 'Value'),\n",
    "    \n",
    "    ('detection probability', 'Delta AIC', 'Value'),\n",
    "    ('detection probability', 'AIC value', 'Value'),\n",
    "    ('detection probability', 'chi-square test probability determined', 'Value'),\n",
    "    ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value'),\n",
    "    ('density/abundance', 'density of animals', 'Delta Cv'),\n",
    "    ('density/abundance', 'density of animals', 'Cv'),\n",
    "    \n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value'),\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Lcl'),\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Ucl'),\n",
    "    \n",
    "    ('density/abundance', 'density of animals', 'Value'),\n",
    "    ('density/abundance', 'density of animals', 'Lcl'),\n",
    "    ('density/abundance', 'density of animals', 'Ucl'),\n",
    "    \n",
    "    ('detection probability', 'probability of detection (Pw)', 'Value'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Lcl'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Ucl'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Df'),\n",
    "\n",
    "    ('run output', 'run folder', 'Value'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortRepCols = \\\n",
    "[('sample', 'SampNum', 'Value')] \\\n",
    "+ [('sample', col, 'Value') for col in sampleSelCols] \\\n",
    "+ [('parameters', 'left truncation distance', 'Value'),\n",
    "   ('parameters', 'right truncation distance', 'Value'),\n",
    "   ('detection probability', 'Delta AIC', 'Value')]\n",
    "#   ('density/abundance', 'density of animals', 'Delta Cv')]\n",
    "\n",
    "sortRepAscend = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ads.MCDSResultsFullReport(resultsSet=results, synthCols=synthRepCols,\n",
    "                                   sortCols=sortRepCols, sortAscend=sortRepAscend,\n",
    "                                   title='Validation AutoDS : Analyses', subTitle='Rapport d\\'analyse global',\n",
    "                                   anlysSubTitle='Rapport détaillé', description='Qu\\'ajouter de plus ?',\n",
    "                                   keywords='autods, validation', pySources=['valtests.ipynb'],\n",
    "                                   lang='fr', plotImgSize=(640, 320),\n",
    "                                   tgtFolder=mcds.workDir, tgtPrefix='autods-validation-report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "htmlRep = report.toHtml() #generators=6) # Parallelism does not work for full reports (while it does for pre-reports !?)\n",
    "\n",
    "HTML(f'Rapport HTML : <a href=\"{htmlRep}\" target=\"blank\">{htmlRep}</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlsxRep = report.toExcel()\n",
    "\n",
    "HTML(f'Rapport Excel : <a href=\"{xlsxRep}\" target=\"blank\">{xlsxRep}</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Run and report pre-analyses (1/3 : long code, long duration)\n",
    "\n",
    "(to help users to setup the full analyses plan : run first try simple analyses and show PDF and few results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Determine samples from input data\n",
    "\n",
    "* in real life, we'd simply load field collected data, and deduce individual \"samples\" from it ;\n",
    "* but there, for testing, it's easier to deduce samples from manual analysis specification file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create sample table from refout results table\n",
    "refFileName = 'ACDC2019-Papyrus-ALAARV-TURMER-resultats-distance-73.xlsx'\n",
    "\n",
    "sampleSelCols = ['Species', 'Periods', 'Prec.', 'Duration']\n",
    "\n",
    "dfSamples = pd.read_excel(pl.Path('refout', refFileName), usecols=sampleSelCols)\n",
    "dfSamples.rename(columns=dict(Name='Model'), inplace=True)\n",
    "dfSamples.drop_duplicates(inplace=True)\n",
    "dfSamples.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfSamples.reset_index(inplace=True) # Generate sample # (later need for original sample order)\n",
    "dfSamples.rename(columns=dict(index='SampleNum'), inplace=True)\n",
    "\n",
    "sampleSelCols = ['SampleNum'] + sampleSelCols\n",
    "\n",
    "dfSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare pre-analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decimalFields = ['Point transect*Survey effort', 'Observation*Radial distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis engine: Non-parallel executor here, 'cause MCDSPreAnalysis takes care of this !\n",
    "mcds = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-pout'), \n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results object construction\n",
    "custCols = [('sample', col, 'Value') for col in sampleSelCols]\n",
    "miCustCols = pd.MultiIndex.from_tuples(custCols)\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=sampleSelCols, fr=['NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée']))\n",
    "\n",
    "results = ads.MCDSAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans, \n",
    "                                     distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                     surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPreEstimCrit = 'AIC'\n",
    "KPreCVInterval = 95\n",
    "KPreEstimModStrat = [dict(keyFn=kf, adjSr='COSINE', estCrit=KPreEstimCrit, cvInt=KPreCVInterval) \\\n",
    "                     for kf in['HNORMAL', 'HAZARD', 'UNIFORM', 'NEXPON']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Or : Really run pre-analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run all analyses\n",
    "lastInFileName = None\n",
    "for _, sSamp in dfSamples.iterrows():\n",
    "    \n",
    "    nSamp = sSamp.SampleNum\n",
    "    sampId = '{}-{}-{}mn-{}dec' \\\n",
    "             .format(sSamp.Species,\n",
    "                     'AB' if 'A+B' in sSamp.Periods else 'A' if 'A' in sSamp.Periods else 'B',\n",
    "                     sSamp.Duration.split(' ')[0], sSamp['Prec.'].split(' ')[0])\n",
    "    logger.info(f'#{nSamp+1:3d} {sampId}')\n",
    "    \n",
    "    # Create data set if not already done.\n",
    "    inFileName = 'ACDC2019-Papyrus-{}-dist.txt'.format(sampId)\n",
    "    if lastInFileName != inFileName:\n",
    "        sds = ads.SampleDataSet(pl.Path('refin', inFileName), decimalFields=decimalFields)\n",
    "        lastInFileName = inFileName\n",
    "        \n",
    "    # Run analysis: Not parallel runs for now ... see below.\n",
    "    preAnlys = ads.MCDSPreAnalysis(engine=mcds, sampleDataSet=sds, name=sampId, executor=None,\n",
    "                                   logData=False, modelStrategy=KPreEstimModStrat)\n",
    "    preAnlys.submit()\n",
    "    \n",
    "    # Get results (wait for it's finished)\n",
    "    sResult = preAnlys.getResults()\n",
    "\n",
    "    # Save results\n",
    "    sResHead = sSamp.copy()\n",
    "    sResHead.index = miCustCols\n",
    "    results.append(sResult, sCustomHead=sResHead)\n",
    "    \n",
    "# shutdown analysis engine\n",
    "mcds.shutdown()\n",
    "\n",
    "# Done.\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "results.dfTransData('fr')[['NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Fn Clé',\n",
    "                           'Sér Ajust', 'CodEx', 'NObs', 'AIC', 'Chi2 P', 'KS P', \n",
    "                           'Densité', 'CoefVar Densité', 'Min Densité', 'Max Densité']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results in case need for not recomputing them\n",
    "resFileName = os.path.join(mcds.workDir, 'autods-validation-preresults.xlsx')\n",
    "\n",
    "results.toExcel(resFileName, sheetName='AutoDSVal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Or : Load pre-analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    resFileName = os.path.join(mcds.workDir, 'autods-validation-preresults.xlsx')\n",
    "    print(f'Loading pre-results from {resFileName} ...')\n",
    "\n",
    "    results.fromExcel(resFileName, sheetName='AutoDSVal')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print(f'... {len(results)} pre-analyses loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "results.dfTransData('fr')[['NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Fn Clé',\n",
    "                           'Sér Ajust', 'CodEx', 'NObs', 'AIC', 'Chi2 P', 'KS P', \n",
    "                           'Densité', 'CoefVar Densité', 'Min Densité', 'Max Densité']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate HTML and Excel pre-analyses reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des colonnes pour les tableaux de synthèse du rapport\n",
    "synthCols = \\\n",
    "[\n",
    "    ('sample', 'SampleNum', 'Value'),\n",
    "    ('sample', 'Species', 'Value'),\n",
    "    ('sample', 'Periods', 'Value'),\n",
    "    ('sample', 'Prec.', 'Value'),\n",
    "    ('sample', 'Duration', 'Value'),\n",
    "    \n",
    "    ('parameters', 'estimator key function', 'Value'),\n",
    "    ('parameters', 'estimator adjustment series', 'Value'),\n",
    "    ('parameters', 'CV interval', 'Value'),\n",
    "    ('parameters', 'left truncation distance', 'Value'),\n",
    "    ('parameters', 'right truncation distance', 'Value'),\n",
    "    ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "    ('parameters', 'distance discretisation cut points', 'Value'),\n",
    "    \n",
    "    ('run output', 'run status', 'Value'),\n",
    "    \n",
    "    ('encounter rate', 'number of observations (n)', 'Value'),\n",
    "    ('encounter rate', 'effort (L or K or T)', 'Value'),\n",
    "    \n",
    "    ('detection probability', 'AIC value', 'Value'),\n",
    "    ('detection probability', 'chi-square test probability determined', 'Value'),\n",
    "    ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value'),\n",
    "    ('density/abundance', 'density of animals', 'Cv'),\n",
    "    \n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value'),\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Lcl'),\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Ucl'),\n",
    "    \n",
    "    ('density/abundance', 'density of animals', 'Value'),\n",
    "    ('density/abundance', 'density of animals', 'Lcl'),\n",
    "    ('density/abundance', 'density of animals', 'Ucl'),\n",
    "    \n",
    "    ('detection probability', 'probability of detection (Pw)', 'Value'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Lcl'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Ucl'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Df'),\n",
    "\n",
    "    ('run output', 'run folder', 'Value'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select analysis results columns for the 3 textual columns of the synthesis pre-report\n",
    "sampleCols = \\\n",
    "[\n",
    "    ('sample', 'SampleNum', 'Value'),\n",
    "    ('sample', 'Species', 'Value'),\n",
    "    ('sample', 'Periods', 'Value'),\n",
    "    ('sample', 'Prec.', 'Value'),\n",
    "    ('sample', 'Duration', 'Value')\n",
    "]\n",
    "\n",
    "paramCols = \\\n",
    "[\n",
    "    ('parameters', 'estimator key function', 'Value'),\n",
    "    ('parameters', 'estimator adjustment series', 'Value'),\n",
    "    ('parameters', 'CV interval', 'Value')\n",
    "]\n",
    "    \n",
    "resultCols = \\\n",
    "[\n",
    "    ('run output', 'run status', 'Value'),\n",
    "    \n",
    "    ('encounter rate', 'number of observations (n)', 'Value'),\n",
    "    ('encounter rate', 'effort (L or K or T)', 'Value'),\n",
    "    \n",
    "    ('detection probability', 'AIC value', 'Value'),\n",
    "    ('detection probability', 'chi-square test probability determined', 'Value'),\n",
    "    ('detection probability', 'Kolmogorov-Smirnov test probability', 'Value'),\n",
    "    ('detection probability', 'probability of detection (Pw)', 'Value'),\n",
    "    ('detection probability', 'effective strip width (ESW) or effective detection radius (EDR)', 'Value'),\n",
    "\n",
    "    ('density/abundance', 'density of animals', 'Cv'),\n",
    "    ('density/abundance', 'density of animals', 'Value'),\n",
    "    ('density/abundance', 'density of animals', 'Lcl'),\n",
    "    ('density/abundance', 'density of animals', 'Ucl'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results = parResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preReport = ads.MCDSResultsPreReport(resultsSet=results,\n",
    "                                     title='Validation AutoDS : Pré-analyses', subTitle='Rapport de pré-analyse',\n",
    "                                     anlysSubTitle='Détail des pré-analyses', description='Qu\\'ajouter de plus ?',\n",
    "                                     keywords='autods, validation', synthPlotsHeight=384, lang='fr',\n",
    "                                     sampleCols=sampleCols, paramCols=paramCols,\n",
    "                                     resultCols=resultCols, anlysSynthCols=synthCols,\n",
    "                                     tgtFolder=mcds.workDir, tgtPrefix='autods-validation-prereport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "htmlPreRep = preReport.toHtml(generators=6)\n",
    "\n",
    "HTML(f'Pré-rapport HTML : <a href=\"{htmlPreRep}\" target=\"blank\">{htmlPreRep}</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xlsxPreRep = preReport.toExcel()\n",
    "\n",
    "HTML(f'Rapport Excel : <a href=\"{xlsxPreRep}\" target=\"blank\">{xlsxPreRep}</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Parallel run of same pre-analyses (2/3 : long code, short duration)\n",
    "\n",
    "And compare results to sequential run's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis engine : Non-parallel executor here: MCDSPreAnalysis takes care of this !\n",
    "mcds = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-ppout'), \n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results object construction\n",
    "parResults = ads.MCDSAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans, \n",
    "                                        distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                        surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Or : Really run pre-analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-analysis executor (kind of overkill here, with only 5 pre-analyses ... but still works twice as rapidly !).\n",
    "parallelExecutor = ads.Executor(threads=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run all analyses\n",
    "lastInFileName = None\n",
    "preAnalyses = dict()\n",
    "for _, sSamp in dfSamples.iterrows():\n",
    "    \n",
    "    nSamp = sSamp.SampleNum\n",
    "    sampId = '{}-{}-{}mn-{}dec' \\\n",
    "             .format(sSamp.Species,\n",
    "                     'AB' if 'A+B' in sSamp.Periods else 'A' if 'A' in sSamp.Periods else 'B',\n",
    "                     sSamp.Duration.split(' ')[0], sSamp['Prec.'].split(' ')[0])\n",
    "    logger.info(f'#{nSamp+1:3d} {sampId}')\n",
    "    \n",
    "    # Create data set if not already done.\n",
    "    inFileName = 'ACDC2019-Papyrus-{}-dist.txt'.format(sampId)\n",
    "    if lastInFileName != inFileName:\n",
    "        sds = ads.SampleDataSet(pl.Path('refin', inFileName), decimalFields=decimalFields)\n",
    "        lastInFileName = inFileName\n",
    "        \n",
    "    # Start running analysis (but don't wait for it's finished)\n",
    "    sResHead = sSamp.copy()\n",
    "    sResHead.index = miCustCols\n",
    "    \n",
    "    preAnlys = ads.MCDSPreAnalysis(engine=mcds, sampleDataSet=sds, name=sampId,\n",
    "                                   customData=sResHead, executor=parallelExecutor,\n",
    "                                   logData=False, modelStrategy=KPreEstimModStrat)\n",
    "    preAnlysFut = preAnlys.submit()\n",
    "    \n",
    "    # Store analysis object and associated \"future\" for later use (should be running soon or later).\n",
    "    preAnalyses[preAnlysFut] = preAnlys\n",
    "    \n",
    "logger.info('All pre-analyses started ; now waiting for their end, and results ...')\n",
    "\n",
    "# For each analysis as it gets completed (first completed => first yielded)\n",
    "for preAnlysFut in parallelExecutor.asCompleted(preAnalyses):\n",
    "\n",
    "    # Retrieve pre-analysis object from its associated future object\n",
    "    preAnlys = preAnalyses[preAnlysFut]\n",
    "    \n",
    "    # Get pre-analysis results\n",
    "    sResult = preAnlys.getResults()\n",
    "\n",
    "    # Save results with header\n",
    "    parResults.append(sResult, sCustomHead=preAnlys.customData)\n",
    "    \n",
    "# shutdown executor\n",
    "parallelExecutor.shutdown()\n",
    "\n",
    "# shutdown analysis engine\n",
    "mcds.shutdown()\n",
    "\n",
    "# Done.\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "parResults.dfTransData('fr')[['NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Fn Clé',\n",
    "                              'Sér Ajust', 'CodEx', 'NObs', 'AIC', 'Chi2 P', 'KS P', \n",
    "                              'Densité', 'CoefVar Densité', 'Min Densité', 'Max Densité']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save results in case need for not recomputing them\n",
    "resFileName = os.path.join(mcds.workDir, 'autods-validation-preresults-par.xlsx')\n",
    "\n",
    "parResults.toExcel(resFileName, sheetName='AutoDSVal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    resFileName = os.path.join(mcds.workDir, 'autods-validation-preresults-par.xlsx')\n",
    "    print('Loading pre-results from {} ...'.format(resFileName))\n",
    "\n",
    "    parResults.fromExcel(resFileName, sheetName='AutoDSVal')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} pre-analyses loaded'.format(len(parResults)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at results\n",
    "parResults.dfTransData('fr')[['NumEchant', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Fn Clé',\n",
    "                              'Sér Ajust', 'CodEx', 'NObs', 'AIC', 'Chi2 P', 'KS P', \n",
    "                              'Densité', 'CoefVar Densité', 'Min Densité', 'Max Densité']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare parallel results to sequential ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare sequential results for comparison\n",
    "dfSeqCmpRes = results.dfTransData('en')\n",
    "\n",
    "dfSeqCmpRes.fillna(-9999, inplace=True) # Get rid of the Nan pb (because NaN != NaN :-)\n",
    "\n",
    "dfSeqCmpRes.drop(columns=['RunTime', 'RunFolder'], inplace=True) # Run date-time and folder can never be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare parallel results for comparison\n",
    "dfParCmpRes = parResults.dfTransData('en')\n",
    "\n",
    "dfParCmpRes.sort_values(by='SampleNum', inplace=True) # Back to original test case order = sequential run order\n",
    "\n",
    "dfParCmpRes.reset_index(inplace=True, drop=True) # Enforce same index as a consequence\n",
    "\n",
    "dfParCmpRes.fillna(-9999, inplace=True) # And get rid of the Nan pb (because NaN != Nan :-)\n",
    "\n",
    "dfParCmpRes.drop(columns=['RunTime', 'RunFolder'], inplace=True) # Run date-time and folder can never be the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (dfSeqCmpRes == dfParCmpRes).all().all(), 'Oh, oh, something went differently when run parallely ...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Run analyses with real life field data (2/2 : short code + fast run)\n",
    "\n",
    "Thanks to MCDSAnalyser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectPlaceCols = ['Point']\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDecCols = [effortCol, 'Distance']\n",
    "\n",
    "sampleNumCol = 'NumEchant'\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "\n",
    "varIndCol = 'NumAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuals data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='DonnéesIndiv').dfData\n",
    "len(dfObsIndiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{ col: dfObsIndiv[col].unique() for col in ['Observateur', 'Point', 'Passage', 'Adulte', 'Durée', 'Espèce'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual transects\n",
    "\n",
    "(can't deduce them from data, some points are missing because of data selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='Inventaires').dfData\n",
    "len(dfTransects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyses specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs = ads.Analyser.explicitVariantSpecs('refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx', \n",
    "                                                 keep=['Echant1_impl', 'Echant2_impl', 'Modl_impl',\n",
    "                                                       'Params1_expl', 'Params2_expl'],\n",
    "                                                 varIndCol=varIndCol,\n",
    "                                                 #convertCols={ 'Durée': int }, # float 'cause of Excel\n",
    "                                                 computedCols={ anlysAbbrevCol: analysisAbbrev })\n",
    "\n",
    "len(dfAnlysSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For faster debugging : reduce work.\n",
    "#dfAnlysSpecs = dfAnlysSpecs[(dfAnlysSpecs.Passage == 'a+b') & (dfAnlysSpecs.Adulte == 'm') \\\n",
    "#                            & (dfAnlysSpecs['Durée'] == '10mn') \\\n",
    "#                            & ((dfAnlysSpecs.TrGche.isnull()) | (dfAnlysSpecs.TrGche < 20)) \\\n",
    "#                            & ((dfAnlysSpecs.TrDrte.isnull()) | (dfAnlysSpecs.TrDrte <= 500))]\n",
    "#len(dfAnlysSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall analysis set without truncation params\n",
    "dfAnlysSpecs[['Espèce', 'Passage', 'Adulte', 'Durée', 'FonctionClé', 'SérieAjust']].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-anlr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A. Or : Really run analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. MCDS Analyser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr = ads.MCDSAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                          transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                          sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                          abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                          distanceUnit='Meter', areaUnit='Hectare',\n",
    "                          surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                          resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                               after=[anlysAbbrevCol]),\n",
    "                          workDir=workDir, logProgressEvery=5,\n",
    "                          defEstimCriterion='AIC', defCVInterval=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Check analysis explicit specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(dfExplParamSpecs=dfAnlysSpecs, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfAnlysSpecs) == 48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAnlysSpecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Analyses : min=5, max=11s elapsed for 64 analyses on a Lenovo P52 (6-core i7-8850H with PCI-e SSD)\n",
    "results = anlysr.run(dfAnlysSpecs, threads=6)\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save results for later reload or examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(workDir / 'valtests-mcds-anlyser-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-anlyser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An analyser object knowns how to build an empty results object ...\n",
    "    anlysr = ads.MCDSAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                              resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                   after=[anlysAbbrevCol]),\n",
    "                              transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                              sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                              abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                              distanceUnit='Meter', areaUnit='Hectare',\n",
    "                              surveyType='Point', distanceType='Radial', clustering=False)\n",
    "    \n",
    "    results = anlysr.setupResults()\n",
    "    \n",
    "    anlysr.shutdown()\n",
    "    \n",
    "    # Load results from file.\n",
    "    resFileName = workDir / 'valtests-mcds-anlyser-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare results to reference\n",
    "\n",
    "(reference generated with same kind of \"long\" code like in III above, but on another data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference\n",
    "# 1. Clone results _without_ data.\n",
    "rsRef = results.copy(withData=False)\n",
    "\n",
    "# 2. Load it with reference data\n",
    "rsRef.fromOpenDoc('refout/ACDC2019-Naturalist-ExtraitResultats.ods')\n",
    "\n",
    "rsRef.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare (ignore sample and analysis indexes, no use here).\n",
    "indexCols = [col for col in results.miCustomCols.to_list() if '(sample)' in col[0]] \\\n",
    "            + [('parameters', 'estimator key function', 'Value'),\n",
    "               ('parameters', 'estimator adjustment series', 'Value'),\n",
    "               ('parameters', 'left truncation distance', 'Value'),\n",
    "               ('parameters', 'right truncation distance', 'Value'),\n",
    "               ('parameters', 'model fitting distance cut points', 'Value')]\n",
    "subsetCols = [col for col in results.dfData.columns.to_list() \\\n",
    "              if col not in (indexCols + [col for col in results.miCustomCols.to_list()\n",
    "                                          if '(sample)' not in col[0]]\n",
    "                             + [('parameters', 'estimator selection criterion', 'Value'),\n",
    "                                ('parameters', 'CV interval', 'Value'),\n",
    "                                ('run output', 'run time', 'Value'),\n",
    "                                ('run output', 'run folder', 'Value'),\n",
    "                                ('detection probability', 'key function type', 'Value'),\n",
    "                                ('detection probability', 'adjustment series type', 'Value')])]\n",
    "\n",
    "dfDiff = rsRef.compare(results, indexCols=indexCols, subsetCols=subsetCols, dropCloser=12, dropNans=True)\n",
    "\n",
    "assert dfDiff.empty, 'No, no, no : not the same ...'\n",
    "\n",
    "print('Yessssss !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To be perfectly honnest ... some 10**-12/15 glitches (due to worksheet I/O ?)\n",
    "rsRef.compare(results, indexCols=indexCols, subsetCols=subsetCols, dropCloser=14, dropNans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII. Run pre-analyses (3/3 : short code, short duration)\n",
    "\n",
    "And compare results to sequential run's.\n",
    "\n",
    "Note: 2 modes here, with explicit or implicit sample specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectPlaceCols = ['Point']\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDecCols = [effortCol, 'Distance']\n",
    "\n",
    "sampleNumCol = 'NumEchant'\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "\n",
    "sampleAbbrevCol = 'AbrevEchant'\n",
    "\n",
    "speciesAbbrevCol = 'AbrevEsp'\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuals data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='DonnéesIndiv').dfData\n",
    "dfObsIndiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{ col: dfObsIndiv[col].unique() for col in ['Observateur', 'Point', 'Passage', 'Adulte', 'Durée', 'Espèce'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual transects\n",
    "\n",
    "(can't deduce them from data, some points are missing because of data selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='Inventaires').dfData\n",
    "dfTransects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Samples to pre-analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implicit variants\n",
    "varEspeces = ['Sylvia atricapilla', 'Turdus merula', 'Luscinia megarhynchos'] # 1 variante espèce ... par espèce <8-]\n",
    "\n",
    "varPassages = ['b', 'a+b'] # Passage b ou a+b => 2 variantes\n",
    "varAdultes = ['m'] # Les mâles, et ensuite les mâles et autres adultes (=> 2 variantes)\n",
    "varDurees = ['5mn', '10mn'] # 5 1ères mn, ou toutes les 10 => 2 variantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitation of variants or not\n",
    "# a. Implicites specs\n",
    "dImplSampleSpecs = { 'Espèce': varEspeces, 'Passage': varPassages, 'Adulte': varAdultes, 'Durée':   varDurees }\n",
    "\n",
    "specsAreExplicit = True\n",
    "if specsAreExplicit:\n",
    "    \n",
    "    # b. Explicit combinations\n",
    "    dfExplSampleSpecs = ads.Analyser.explicitVariantSpecs(dict(_impl=dImplSampleSpecs))\n",
    "    #dfExplSampleSpecs = ads.Analyser.explicitPartialVariantSpecs(dImplSampleSpecs) # Just the same, but less generic.\n",
    "\n",
    "    # c. Add sample order columns (usefull for reports, as pre-analyses are run parallely !).\n",
    "    #dfExplSampleSpecs.reset_index(drop=False, inplace=True)\n",
    "    #dfExplSampleSpecs.rename(columns=dict(index=sampleNumCol), inplace=True)\n",
    "\n",
    "    # d. Add sample abreviation column (mainly for analysis traces)\n",
    "    #dfExplSampleSpecs[sampleAbbrevCol] = dfExplSampleSpecs.apply(sampleAbbrev, axis='columns')\n",
    "\n",
    "    # e. Add neutral and pass-through column (from sample specs to results)\n",
    "    dfExplSampleSpecs[speciesAbbrevCol] = dfExplSampleSpecs['Espèce'].apply(lambda s: ''.join(m[:4] for m in s.split()))\n",
    "    \n",
    "    print(dfExplSampleSpecs)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # b. Keep unexplicited : run will do automatically\n",
    "    implSampleSpecs = dict(_impl=dImplSampleSpecs)\n",
    "    \n",
    "    print(implSampleSpecs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-anlr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A. Or : Really run pre-analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. MCDSPreAnalyser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr = ads.MCDSPreAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                             transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                             sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleSpecCustCols=[speciesAbbrevCol],\n",
    "                             abbrevCol=sampleAbbrevCol, abbrevBuilder=sampleAbbrev, sampleIndCol=sampleNumCol,\n",
    "                             distanceUnit='Meter', areaUnit='Hectare',\n",
    "                             surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                             resultsHeadCols=dict(before=[sampleNumCol], sample=sampleSelCols,\n",
    "                                                  after=([speciesAbbrevCol] if specsAreExplicit else []) + [sampleAbbrevCol]),\n",
    "                             workDir=workDir, logProgressEvery=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Check pre-analyses specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExplSampleSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(dfExplParamSpecs=dfExplSampleSpecs if specsAreExplicit else None,\n",
    "                              implParamSpecs=implSampleSpecs if not specsAreExplicit else None,\n",
    "                              dropDupes=True, check=True)\n",
    "\n",
    "print(verdict, reasons, len(dfExplSampleSpecs), userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)\n",
    "\n",
    "assert len(dfExplSampleSpecs) == 12\n",
    "assert userParamSpecCols == [] # No analysis params here (auto. generated by PreAnalyser)\n",
    "assert intParamSpecCols == [] # Idem\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run pre-analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model fall-down strategy\n",
    "dModelStrategy = [dict(keyFn=kf, adjSr=js, estCrit='AIC', cvInt=95) \\\n",
    "                  for js in['COSINE', 'POLY', 'HERMITE']\n",
    "                  for kf in['HNORMAL', 'HAZARD', 'UNIFORM', 'NEXPON']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Analyses : 50s to ~1mn elapsed for 12 samples, 6-12 threads on a Lenovo T490 (4-core i5-8xxx with PCI-e SSD)\n",
    "results = anlysr.run(dfExplSampleSpecs if specsAreExplicit else None,\n",
    "                     implSampleSpecs=implSampleSpecs if not specsAreExplicit else None, \n",
    "                     dModelStrategy=dModelStrategy, threads=12)\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not specsAreExplicit or speciesAbbrevCol in results.dfTransData('fr').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save results for later reload or examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(workDir / 'valtests-mcds-preanlyser-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-preanlyser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B. Or : Load pre-analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An analyser object knowns how to build an empty results object ...\n",
    "    anlysr = ads.MCDSPreAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                                 abbrevCol=sampleAbbrevCol,\n",
    "                                 resultsHeadCols=dict(before=[sampleNumCol], sample=sampleSelCols,\n",
    "                                                      after=([speciesAbbrevCol] if specsAreExplicit else [])\n",
    "                                                            + [sampleAbbrevCol]),\n",
    "                                 transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                                 sampleDecCols=sampleDecCols,\n",
    "                                 distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                 surveyType='Point', distanceType='Radial', clustering=False)\n",
    "    \n",
    "    results = anlysr.setupResults()\n",
    "    \n",
    "    anlysr.shutdown()\n",
    "\n",
    "    # Load resultas from file\n",
    "    resFileName = workDir / 'valtests-mcds-anlyser-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare results to reference\n",
    "\n",
    "(reference generated with same kind of \"long\" code like in III above, but on another data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference\n",
    "# 1. Clone results _without_ data.\n",
    "rsRef = results.copy(withData=False)\n",
    "\n",
    "# 2. Load it with reference data\n",
    "rsRef.fromOpenDoc('refout/ACDC2019-Naturalist-ExtraitPreResultats.ods')\n",
    "\n",
    "rsRef.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare (ignore sample and analysis indexes, no use here).\n",
    "indexCols = [col for col in results.miCustomCols.to_list() if '(sample)' in col[0]] \\\n",
    "            + [('parameters', 'estimator key function', 'Value'),\n",
    "               ('parameters', 'estimator adjustment series', 'Value')]\n",
    "subsetCols = [col for col in results.dfData.columns.to_list() \\\n",
    "              if col not in (indexCols + [col for col in results.miCustomCols.to_list() if '(sample)' not in col[0]]\n",
    "                             + [('parameters', 'estimator selection criterion', 'Value'),\n",
    "                                ('parameters', 'CV interval', 'Value'),\n",
    "                                ('run output', 'run time', 'Value'),\n",
    "                                ('run output', 'run folder', 'Value'),\n",
    "                                ('detection probability', 'key function type', 'Value'),\n",
    "                                ('detection probability', 'adjustment series type', 'Value')])]\n",
    "\n",
    "dfDiff = rsRef.compare(results, indexCols=indexCols, subsetCols=subsetCols, dropCloser=13, dropNans=True)\n",
    "\n",
    "assert dfDiff.empty, 'Oh oh ... some differences !'\n",
    "\n",
    "print('Yessssss !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To be perfectly honest ... there are some 10**-15/-16 glitches (due to worksheet I/O ?)\n",
    "rsRef.compare(results, indexCols=indexCols, subsetCols=subsetCols, dropNans=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IX. Truncation optimisation (short code and fast run)\n",
    "\n",
    "Thanks to MCDSZeroOrderTruncationOptimiser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimAbbrev(sAnlys):\n",
    "    \n",
    "    # Sample abbreviation\n",
    "    spcAbbrev = ''.join(word[:4].title() for word in sAnlys['Espèce'].split(' ')[:2])\n",
    "    sampAbbrev = [str(x) for x in [spcAbbrev, sAnlys.Passage.replace('+', ''),\n",
    "                                   sAnlys.Adulte.replace('+', ''), sAnlys['Durée']]]\n",
    "\n",
    "    # Model + Parameters abbreviation\n",
    "    modParAbbrev = [sAnlys['FonctionClé'][:3].lower(), sAnlys['SérieAjust'][:3].lower()]\n",
    "    \n",
    "    return '-'.join(sampAbbrev + modParAbbrev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Optimiser parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source / Results data\n",
    "transectPlaceCols = ['Point']\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDistCol = 'Distance'\n",
    "sampleDecCols = [effortCol, sampleDistCol]\n",
    "\n",
    "sampleNumCol = 'NumEchant'\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "\n",
    "sampleAbbrevCol = 'AbrevEchant'\n",
    "\n",
    "optIndCol = 'IndOptim'\n",
    "optAbbrevCol = 'AbrevOptim'\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les paramètres généraux d'analyse DS\n",
    "distanceUnit = 'Meter'\n",
    "areaUnit = 'Hectare'\n",
    "surveyType = 'Point'\n",
    "distanceType = 'Radial'\n",
    "clustering = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default optimisation params.\n",
    "defEstimKeyFn = 'HNORMAL'\n",
    "defEstimAdjustFn = 'COSINE'\n",
    "defEstimCriterion = 'AIC'\n",
    "defCVInterval = 95\n",
    "defMinDist = None\n",
    "defMaxDist = None, \n",
    "defFitDistCuts = None\n",
    "defDiscrDistCuts = None\n",
    "\n",
    "defExpr2Optimise = 'chi2'\n",
    "defMinimiseExpr = False\n",
    "defOutliersMethod = 'tucquant'\n",
    "defOutliersQuantCutPct = 7\n",
    "defFitDistCutsFctr = ads.Interval(min=0.6, max=1.4)\n",
    "defDiscrDistCutsFctr = ads.Interval(min=0.5, max=1.2)\n",
    "\n",
    "defSubmitTimes = 1\n",
    "defSubmitOnlyBest = None\n",
    "\n",
    "defCoreEngine = 'zoopt'\n",
    "defCoreMaxIters = 100\n",
    "defCoreTermExprValue = None\n",
    "defCoreAlgorithm = 'racos'\n",
    "defCoreMaxRetries = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[X. Run opt-analyses with real life field data (short code)](#X.-Run-opt-analyses-with-real-life-field-data-(short-code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuals data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='DonnéesIndiv').dfData\n",
    "dfObsIndiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{ col: dfObsIndiv[col].unique() for col in ['Observateur', 'Point', 'Passage', 'Adulte', 'Durée', 'Espèce'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual transects\n",
    "\n",
    "(can't deduce them from data, some points are missing because of data selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects = ads.DataSet('refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods', sheet='Inventaires').dfData\n",
    "dfTransects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Samples and analyses to optimise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-optr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. For testing all optimisation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varOpt = '-all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfOptimExplSpecs = ads.Analyser.explicitVariantSpecs('refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx', \n",
    "                                                     ignore=['Params3_expl'])\n",
    "\n",
    "# No use of these cols, as we'll compute them !\n",
    "dfOptimExplSpecs = dfOptimExplSpecs.drop(columns=['TrGche', 'TrDrte', 'NbTrchMod']).drop_duplicates() \\\n",
    "                    .reset_index(drop=True)\n",
    "\n",
    "nOptimExplSpecs = len(dfOptimExplSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add optim. params\n",
    "dfMoreOptimCols = pd.DataFrame([dict(CritChx='AIC', IntervConf=95,\n",
    "                                     TrGche='auto', TrDrte='auto', MethOutliers='tucquant(2.5)',\n",
    "                                     NbTrchMod='mult(2/3, 3/2)', NbTrDiscr=None,\n",
    "                                     #TroncGche='auto', TroncDrte='auto', MethOutliers='tucquant(2.5)',\n",
    "                                     #NbTrModel='mult(2/3, 3/2)', NbTrDiscr=None,\n",
    "                                     ExprOpt='max(chi2)', MoteurOpt='zoopt(160)')]*len(dfOptimExplSpecs))\n",
    "\n",
    "dfOptimExplSpecs = pd.concat([dfOptimExplSpecs, dfMoreOptimCols], axis='columns')\n",
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nOptimedAnlyses = nOptimExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de dfOptimExplSpecs donnant les paramètres d'analyse / optimisation\n",
    "optimParamSpecCols  = ['FonctionClé', 'SérieAjust', 'CritChx', 'IntervConf',\n",
    "                       'TrGche', 'TrDrte', 'MethOutliers', 'NbTrchMod', 'NbTrDiscr',\n",
    "                       #'TroncGche', 'TroncDrte', 'MethOutliers', 'NbTrModel', 'NbTrDiscr',\n",
    "                       'ExprOpt', 'MoteurOpt']\n",
    "\n",
    "# Et en version interne\n",
    "intOptimParamSpecCols = ['EstimKeyFn', 'EstimAdjustFn', 'EstimCriterion', 'CvInterval',\n",
    "                          'MinDist', 'MaxDist', 'OutliersMethod', 'FitDistCuts', 'DiscrDistCuts',\n",
    "                          'Expr2Optimise', 'OptimisationCore']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Or: Only main optimisation parameters\n",
    "\n",
    "* for comparison with X below,\n",
    "* for comparing results goodness with various optimisation parameters, in XI below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varOpt = '-main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfOptimExplSpecs = ads.Analyser.explicitVariantSpecs('refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx', \n",
    "                                                     ignore=['Params1_expl', 'Params2_expl'])\n",
    "\n",
    "dfOptimExplSpecs.drop(dfOptimExplSpecs[dfOptimExplSpecs[['TrGche', 'TrDrte', 'NbTrchMod', 'MultiOpt']]\n",
    "                                           .isnull().all(axis='columns')].index,\n",
    "                      inplace=True)\n",
    "\n",
    "nOptimExplSpecs = len(dfOptimExplSpecs)\n",
    "\n",
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nOptimedAnlyses = 22  # See MultiOpt col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de dfOptimExplSpecs donnant les paramètres d'analyse / optimisation\n",
    "optimParamSpecCols  = ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod', 'MultiOpt']\n",
    "\n",
    "# Et en version interne\n",
    "intOptimParamSpecCols = ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts', 'SubmitParams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4A. Or : Really run optimisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. MCDS Zeroth Order Truncation Optimiser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoptr = ads.MCDSZerothOrderTruncationOptimiser \\\n",
    "                (dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea, \n",
    "                 transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                 sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                 abbrevCol=optAbbrevCol, abbrevBuilder=optimAbbrev,\n",
    "                 anlysIndCol=optIndCol, sampleIndCol=sampleNumCol,\n",
    "                 distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                 surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                 resultsHeadCols=dict(before=[optIndCol], sample=sampleSelCols, after=optimParamSpecCols),\n",
    "                 workDir='/tmp',\n",
    "                 defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,\n",
    "                 defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,\n",
    "                 defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,\n",
    "                 defOutliersMethod=defOutliersMethod, defOutliersQuantCutPct=defOutliersQuantCutPct,\n",
    "                 defFitDistCutsFctr=defFitDistCutsFctr, defDiscrDistCutsFctr=defDiscrDistCutsFctr,\n",
    "                 defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,\n",
    "                 defCoreMaxIters=defCoreMaxIters, defCoreTermExprValue=defCoreTermExprValue,\n",
    "                 defCoreAlgorithm=defCoreAlgorithm, defCoreMaxRetries=defCoreMaxRetries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Run optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOptimExplSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, checkVerdict, checkErrors = \\\n",
    "    zoptr.explicitParamSpecs(dfExplParamSpecs=dfOptimExplSpecs, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfOptimExplSpecs) == nOptimExplSpecs\n",
    "assert userParamSpecCols == optimParamSpecCols\n",
    "assert intParamSpecCols == intOptimParamSpecCols\n",
    "assert unmUserParamSpecCols == []\n",
    "assert checkVerdict\n",
    "assert not checkErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Analyses\n",
    "results = zoptr.run(dfOptimExplSpecs, threads=12)\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas 3a : 12 analyses specs (12 parallel threads) on a Lenovo T490 4 HT Cores\n",
    "\n",
    "Paramètres dans refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx nettoyé de Param1_expl et Param2_expl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoteurOpt='zoopt(120)'\n",
    "\n",
    "2020-08-21 22:41:56,626 2880 analyses => 22 results, Wall time: 4min 21s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas 3b : 12 analyses specs (6 parallel threads) on a Lenovo T490 4 HT Cores\n",
    "\n",
    "CritChx='AIC', IntervConf=95, TroncGche='auto', TroncDrte='auto', MethOutliers='tucquant(2.5)',\n",
    "NbTrModel='mult(2/3, 3/2)', NbTrDiscr=None, ExprOpt='max(chi2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoteurOpt='zoopt(160)'\n",
    "\n",
    "2020-06-29 21:18:04,727 Wall time: 4min 31s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoteurOpt='zoopt(250, tv=0.6)'\n",
    "\n",
    "2020-06-28 19:23:38,868 Wall time: 9min 19s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cas 3b : 12 analyses specs (12 parallel threads) on a Lenovo T490 4 HT Cores\n",
    "\n",
    "CritChx='AIC', IntervConf=95, TroncGche='auto', TroncDrte='auto', MethOutliers='tucquant(2.5)',\n",
    "NbTrModel='mult(2/3, 3/2)', NbTrDiscr=None, ExprOpt='max(chi2)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MoteurOpt='zoopt(160)'\n",
    "\n",
    "2020-07-18 15:22:47,289 Wall time: 3min 51s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoptr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Save results for later reload or examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(workDir / f'valtests-mcds-optimiser{varOpt}-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-optimiser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4B. Or : Load optimisation results from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An analyser object knowns how to build an empty results object ...\n",
    "    zoptr = ads.MCDSZerothOrderTruncationOptimiser \\\n",
    "                    (dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea, \n",
    "                     transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                     sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                     abbrevCol=optAbbrevCol, abbrevBuilder=optimAbbrev,\n",
    "                     anlysIndCol=optIndCol, sampleIndCol=sampleNumCol,\n",
    "                     distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                     surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                     resultsHeadCols=dict(before=[optIndCol], sample=sampleSelCols, after=optimParamSpecCols))\n",
    "    \n",
    "    results = zoptr.setupResults()\n",
    "    \n",
    "    zoptr.shutdown()\n",
    "    \n",
    "    # Load results from file.\n",
    "    resFileName = workDir / f'valtests-mcds-optimiser{varOpt}-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} optimisations to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Deduce analyses specs from optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIndCol = 'NumAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample and analysis params, and above all optimised truncation param. values from optimiser results.\n",
    "optTgtCols = ['TrGche', 'TrDrte', 'NbTrchMod']\n",
    "#optTgtCols = ['TroncGche', 'TroncDrte', 'NbTrModel']\n",
    "dfAnlysSpecs = results.dfData[['Espèce', 'Passage', 'Adulte', 'Durée', 'FonctionClé', 'SérieAjust',\n",
    "                               'minDist', 'maxDist', 'fitDistCuts'] + optTgtCols].copy()\n",
    "\n",
    "# Add analysis abbreviation from truncation params optim. specs (not from optimised results).\n",
    "dfAnlysSpecs[anlysAbbrevCol] = dfAnlysSpecs.apply(analysisAbbrev, axis='columns')\n",
    "\n",
    "# No need for the truncation params optim. specs anymore\n",
    "dfAnlysSpecs.drop(columns=optTgtCols, inplace=True)\n",
    "\n",
    "# Rename optimised truncation param. columns for analysis\n",
    "dfAnlysSpecs.rename(columns=dict(minDist='TrGche', maxDist='TrDrte', fitDistCuts='NbTrchMod'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But non-optimised truncation parameters are not in optimiser result columns (minDist, maxDist, fitDisCuts, ...) ...\n",
    "# so we have to get them back from optimisation specs (TrGche, TrDrte, NbTrchMod, ...)\n",
    "\n",
    "# String specs are optimisation params, numerical ones are already determined truncation params.\n",
    "bdfToBeKeptSpecCells = results.dfData[optTgtCols].applymap(lambda v: isinstance(v, str))\n",
    "\n",
    "dfAnlysSpecs[optTgtCols] = dfAnlysSpecs[optTgtCols].where(bdfToBeKeptSpecCells,\n",
    "                                                          other=results.dfData[optTgtCols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-anaftopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6A. Or : Really run analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. MCDS Analyser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysParamCols = ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "\n",
    "anlysr = ads.MCDSAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                          transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                          sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                          abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                          distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                          surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                          resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                               after=anlysParamCols + [anlysAbbrevCol]),\n",
    "                          workDir=workDir, logProgressEvery=1,\n",
    "                          defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,\n",
    "                          defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,\n",
    "                          defMinDist=defMinDist, defMaxDist=defMaxDist,\n",
    "                          defFitDistCuts=defFitDistCuts, defDiscrDistCuts=defDiscrDistCuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Check analysis explicit specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(dfExplParamSpecs=dfAnlysSpecs, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfAnlysSpecs) == nOptimedAnlyses, f'{len(dfAnlysSpecs)} != {nOptimedAnlyses}'\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod'], str(userParamSpecCols)\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts'], str(intParamSpecCols)\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons, str(reasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Analyses : min=5, max=11s elapsed for 64 analyses with 6 threads on a Lenovo P52 (6-HT-core i7-8850H with PCI-e SSD)\n",
    "# Analyses : ~2.1s elapsed for 22 analyses with 6-12 threads on a Lenovo T490 (4-HT-core i5-8365U with PCI-e SSD)\n",
    "results = anlysr.run(dfAnlysSpecs, threads=12)\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save results for later reload or examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(workDir / f'valtests-mcds-analyser-afteropt{varOpt}-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-analyser-afteropt-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An analyser object knowns how to build an empty results object ...\n",
    "    anlysr = ads.MCDSAnalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea,\n",
    "                              resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                   after=[anlysAbbrevCol]),\n",
    "                              transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                              sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                              abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                              distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                              surveyType=surveyType, distanceType=distanceType, clustering=clustering)\n",
    "    \n",
    "    results = anlysr.setupResults()\n",
    "    \n",
    "    anlysr.shutdown()\n",
    "    \n",
    "    # Load results from file.\n",
    "    resFileName = workDir / f'valtests-mcds-analyser-afteropt{varOpt}-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X. Run opt-analyses with real life field data (short code)\n",
    "\n",
    "i.e. analyses with ready-to-go (const values) parameters, and some others with to-be-computed parameters (through otpimisation)\n",
    "\n",
    "Thanks to MCDSTruncationOptanalyser class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Optanalyser parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description des données\n",
    "transectPlaceCols = ['Point']\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDistCol = 'Distance'\n",
    "sampleDecCols = [effortCol, sampleDistCol]\n",
    "\n",
    "sampleNumCol = 'NumEchant'\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "\n",
    "sampleAbbrevCol = 'AbrevEchant'\n",
    "\n",
    "varIndCol = 'NumAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'\n",
    "anlysParamCols = ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les analyses à faire (avec specs d'optimisation dedans si nécessaire)\n",
    "optanlysSpecFile = 'refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx'\n",
    "optanlysSpecFile = 'refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.tmp.xlsx'\n",
    "#optanlysSpecFile = '../donnees/acdc/ACDC2019-Naturalist-ExtraitSpecsOptanalyses-reduit.ods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autres paramètres\n",
    "dDefSubmitOtherParams = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ATTENTION** : Exécuter IX.0 pour les autres paramètres !\n",
    "\n",
    "[IX. Truncation optimisation (short code and fast run)](#IX.-Truncation-optimisation-(short-code-and-fast-run))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Individuals data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données individualisées et transects\n",
    "indivObsFile = 'refin/ACDC2019-Naturalist-ExtraitObsIndiv.ods'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = ads.DataSet(indivObsFile, sheet='DonnéesIndiv').dfData\n",
    "len(dfObsIndiv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "{ col: dfObsIndiv[col].unique() for col in ['Observateur', 'Point', 'Passage', 'Adulte', 'Durée', 'Espèce'] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Actual transects\n",
    "\n",
    "(can't deduce them from data, some points are missing because of data selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects = ads.DataSet(indivObsFile, sheet='Inventaires').dfData\n",
    "len(dfTransects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-optanlr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimTruncCol = ads.MCDSTruncationOptanalyser.OptimTruncFlagCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. Or : Really run opt-analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. MCDS Opt-Analyser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optanlr = \\\n",
    "    ads.MCDSTruncationOptanalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea, \n",
    "                                  transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                                  sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                                  abbrevCol=anlysAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                                  anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                                  distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                                  surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                                  resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                       after=anlysParamCols + [optimTruncCol, anlysAbbrevCol]),\n",
    "                                  workDir=workDir, logAnlysProgressEvery=5, logOptimProgressEvery=3,\n",
    "                                  defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,\n",
    "                                  defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,\n",
    "                                  defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,\n",
    "                                  defOutliersMethod=defOutliersMethod, defOutliersQuantCutPct=defOutliersQuantCutPct,\n",
    "                                  defFitDistCutsFctr=defFitDistCutsFctr, defDiscrDistCutsFctr=defDiscrDistCutsFctr,\n",
    "                                  defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,\n",
    "                                  dDefSubmitOtherParams=dDefSubmitOtherParams,\n",
    "                                  dDefOptimCoreParams=dict(core=defCoreEngine, maxIters=defCoreMaxIters,\n",
    "                                                           termExprValue=defCoreTermExprValue,\n",
    "                                                           algorithm=defCoreAlgorithm, maxRetries=defCoreMaxRetries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Check opt-analyses specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    optanlr.explicitParamSpecs(implParamSpecs=optanlysSpecFile, dropDupes=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfAnlysSpecs) == 60\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod', 'MultiOpt']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts', 'SubmitParams']\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfAnlysSpecs))\n",
    "if not verdict:\n",
    "    print(reasons)\n",
    "    print(userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Run opt-analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfAnlysSpecs.loc[51:53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = optanlr.run(implParamSpecs=optanlysSpecFile, threads=12)\n",
    "#results = optanlr.run(dfExplParamSpecs=dfAnlysSpecs, threads=12)  # A small sample, for a quicker check\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optanlr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert optimTruncCol in results.dfTransData('fr').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr').to_excel('tmp/res-tst.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Save results for later reload or examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(workDir / 'valtests-mcds-optanlyser-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-optanlyser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.fromExcel(workDir / 'valtests-mcds-optanlyser-results.xlsx', specs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Or : Load opt-analyses results from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An opt-analyser object knowns how to build an empty results object ...\n",
    "    optanlr = \\\n",
    "        ads.MCDSTruncationOptanalyser(dfObsIndiv, dfTransects=dfTransects, dSurveyArea=dSurveyArea, \n",
    "                                      transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                                      sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                                      sampleDistCol=sampleDistCol,\n",
    "                                      abbrevCol=anlysAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                                      anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                                      distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                                      surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                                      resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                           after=anlysParamCols + [optimTruncCol, anlysAbbrevCol]))\n",
    "\n",
    "    results = optanlr.setupResults()\n",
    "    \n",
    "    optanlr.shutdown()\n",
    "    \n",
    "    # Load results from file.\n",
    "    resFileName = workDir / 'valtests-mcds-optanlyser-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to compare'.format(len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare results to reference\n",
    "\n",
    "(reference generated with same kind of \"long\" code like in III above, but on another data set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load reference unoptimised analyses results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unoptimised reference\n",
    "# 1. Clone results _without_ data.\n",
    "rsUnoptRef = results.copy(withData=False)\n",
    "\n",
    "# 2. Load it with reference data\n",
    "rsUnoptRef.fromOpenDoc('refout/ACDC2019-Naturalist-ExtraitResultats.ods')\n",
    "\n",
    "unoptAnlysAbbrevs = list(rsUnoptRef.dfData[('header (tail)', anlysAbbrevCol, 'Value')])\n",
    "\n",
    "len(unoptAnlysAbbrevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Separate actual optanalysis results in 2 sets : optimised, and unoptimised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unoptimised results.\n",
    "rsUnoptRes = results.copy()\n",
    "\n",
    "rsUnoptRes.dropRows(~rsUnoptRes.dfData[('header (tail)', anlysAbbrevCol, 'Value')].isin(unoptAnlysAbbrevs))\n",
    "\n",
    "#rsUnoptRes.dfTransData('fr').to_excel('tmp/res.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimised results.\n",
    "rsOptRes = results.copy()\n",
    "\n",
    "rsOptRes.dropRows(rsOptRes.dfData[('header (tail)', anlysAbbrevCol, 'Value')].isin(unoptAnlysAbbrevs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(unoptRef=len(rsUnoptRef), unoptRes=len(rsUnoptRes), optRes=len(rsOptRes), allRes=len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Compare \"unoptimised\" analyses results to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare (ignore sample and analysis indexes, no use here).\n",
    "indexCols = [col for col in rsUnoptRes.miCustomCols.to_list() if '(sample)' in col[0]] \\\n",
    "            + [('parameters', 'estimator key function', 'Value'),\n",
    "               ('parameters', 'estimator adjustment series', 'Value'),\n",
    "               ('parameters', 'left truncation distance', 'Value'),\n",
    "               ('parameters', 'right truncation distance', 'Value'),\n",
    "               ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "               ('header (tail)', 'AbrevAnlys', 'Value')]\n",
    "subsetCols = [col for col in rsUnoptRes.columns.to_list() \\\n",
    "              if col not in (indexCols + [col for col in rsUnoptRes.miCustomCols.to_list()\n",
    "                                          if '(sample)' not in col[0]]\n",
    "                             + [('parameters', 'estimator selection criterion', 'Value'),\n",
    "                                ('parameters', 'CV interval', 'Value'),\n",
    "                                ('run output', 'run time', 'Value'),\n",
    "                                ('run output', 'run folder', 'Value'),\n",
    "                                ('detection probability', 'Delta AIC', 'Value'),\n",
    "                                ('detection probability', 'key function type', 'Value'),\n",
    "                                ('detection probability', 'adjustment series type', 'Value')])]\n",
    "\n",
    "dfDiff = rsUnoptRef.compare(rsUnoptRes, indexCols=indexCols, subsetCols=subsetCols, dropCloser=14, dropNans=True)\n",
    "\n",
    "assert dfDiff.empty, 'No, no, no : not the same ...'\n",
    "\n",
    "print('Yessssss !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To be perfectly honest, some 10^-15 differences (when some results loaded from Excel, some other not).\n",
    "rsUnoptRef.compare(rsUnoptRes, indexCols=indexCols, subsetCols=subsetCols, dropCloser=15, dropNans=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Compare \"with optimisation\" results to \"reference\"\n",
    "\n",
    "(reference = analyses results computed in IX.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimised reference (analysis results with truncation params computed through optimisation)\n",
    "# 1. Clone results _without_ data.\n",
    "rsOptRef = results.copy(withData=False)\n",
    "\n",
    "# 2. Load it with reference data\n",
    "rsOptRef.fromExcel(f'tmp/mcds-anaftopt/valtests-mcds-analyser-afteropt{varOpt}-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort rows for each analysis optim param specs ... by left truncation distance first\n",
    "miSortCols = [('header (tail)', 'AbrevAnlys', 'Value'),\n",
    "              ('parameters', 'left truncation distance', 'Value'),\n",
    "              ('parameters', 'right truncation distance', 'Value'),\n",
    "              ('parameters', 'model fitting distance cut points', 'Value')]\n",
    "\n",
    "rsOptRes.sortRows(by=miSortCols)\n",
    "rsOptRef.sortRows(by=miSortCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple columns index (fr) + setup sorted analyses index\n",
    "miAnlysNumCol = 'NumAnlys'\n",
    "dfOptRes = rsOptRes.dfTransData('fr')\n",
    "dfOptRes[miAnlysNumCol] = [i for i in range(len(dfOptRes))]\n",
    "dfOptRef = rsOptRef.dfTransData('fr')\n",
    "dfOptRef[miAnlysNumCol] = [i for i in range(len(dfOptRef))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that order is \"compatible\" between reference and actual results\n",
    "miAnlysAbrevCol = 'AbrevAnlys'\n",
    "\n",
    "assert dfOptRes[miAnlysAbrevCol].to_list() == dfOptRef[miAnlysAbrevCol].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk for visual checks / comparison\n",
    "#dfOptRes.to_excel('tmp/opt-res-fr.xlsx')\n",
    "#dfOptRef.to_excel('tmp/opt-ref-fr.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a simple subset of analyses results ...\n",
    "indexCols = [miAnlysNumCol, miAnlysAbrevCol]\n",
    "subsetCols = ['AIC', 'PDetec', 'EDR/ESW', 'Densité']\n",
    "\n",
    "dfDiff = ads.ResultsSet.compareDataFrames(dfOptRes, dfOptRef, indexCols=indexCols, subsetCols=subsetCols, dropNans=True)\n",
    "\n",
    "dfDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some diff. stats\n",
    "dfDiffStats = pd.DataFrame(data=[dfDiff.min(), dfDiff.max(), dfDiff.replace(np.inf, 16).mean()],\n",
    "                           index=['min', 'max', 'mean'])\n",
    "dfDiffStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not too bad if less that 10% mean difference (100 / 10**1 = 10%) !\n",
    "assert dfDiffStats.loc['mean'].min() >= 1.0\n",
    "\n",
    "# And actually at most P % difference\n",
    "100 / 10**dfDiffStats.loc['mean'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to disk after \"merging\" ref and actual results, again for visual checks\n",
    "dfOptRef.insert(0, 'x', 'ref')\n",
    "dfOptRes.insert(0, 'x', 'res')\n",
    "\n",
    "dfOptComp = dfOptRef.append(dfOptRes, sort=False)\n",
    "\n",
    "dfOptComp.sort_values(by=['NumAnlys', 'x'], inplace=True)\n",
    "\n",
    "dfOptComp.to_excel('tmp/opt-comp.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Some history of computations difference stats with various 'maxIters' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep stats for history ... copy/paste results below ...\n",
    "print('**maxIters={} (N=?): max delta = {:.2f} %**'.format(defCoreMaxIters, 100 / 10**dfDiffStats.loc['mean'].min()))\n",
    "print()\n",
    "print(dfDiffStats.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maxIter=120 (N=3) : max delta = 6.1 %, 1.6 %, 1.7 %**\n",
    "\n",
    "|Exec1 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.2     |   0.9     |   0.6     |\n",
    "| max  | inf       |  5.1     | inf       |   6.5     |\n",
    "| mean |   2.37273 |  1.21364 |   2.15909 |   1.47273 |\n",
    "\n",
    "|Exec2 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.2     |   0.8     |   0.6     |\n",
    "| max  | inf       |  5       | inf       | inf       |\n",
    "| mean |   3.15455 |  1.79545 |   2.82273 |   2.47273 |\n",
    "\n",
    "|Exec3 |     AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|--------:|---------:|----------:|----------:|\n",
    "| min  | 1.1     |  0.3     |   0.6     |   0.4     |\n",
    "| max  | 6.6     |  4.9     |   5.2     |   4.9     |\n",
    "| mean | 2.57727 |  1.76818 |   2.21364 |   1.92273 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maxIter=250 (N=3) : max delta = 0.83 %, 3.4 %, 0.53 %**\n",
    "\n",
    "|Exec1 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.4     |   0.8     |   0.6     |\n",
    "| max  | inf       |  5.9     | inf       | inf       |\n",
    "| mean |   4.39545 |  2.08182 |   2.95455 |   2.68636 |\n",
    "\n",
    "|Exec2 |     AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|--------:|---------:|----------:|----------:|\n",
    "| min  | 1       |  0.4     |   0.5     |      0.3  |\n",
    "| max  | 6.7     |  5.4     |   5.7     |      5.5  |\n",
    "| mean | 2.18636 |  1.46818 |   1.82273 |      1.55 |\n",
    "\n",
    "|Exec3 |       AIC |    PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|----------:|----------:|----------:|\n",
    "| min  |   1       |   0.3     |   0.9     |   0.6     |\n",
    "| max  | inf       | inf       | inf       | inf       |\n",
    "| mean |   3.76818 |   2.27727 |   3.50909 |   3.24091 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maxIters=400 (N=4): max delta = 2.6 %, 2.9%, 1.9%, 1.8%**\n",
    "\n",
    "|Exec1 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.1     |   0.5     |   0.3     |\n",
    "| max  | inf       |  6.7     | inf       |   6.4     |\n",
    "| mean |   3.03182 |  1.57727 |   2.65455 |   1.89091 |\n",
    "\n",
    "|Exec2 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.3     |   0.9     |   0.6     |\n",
    "| max  | inf       |  4.3     |   5       |   4.7     |\n",
    "| mean |   2.79091 |  1.54091 |   2.08182 |   1.80909 |\n",
    "\n",
    "|Exec3 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.2     |   0.5     |   0.3     |\n",
    "| max  | inf       |  6.7     | inf       |  15.9     |\n",
    "| mean |   3.40455 |  1.71818 |   2.46364 |   2.24545 |\n",
    "\n",
    "|Exec4 |       AIC |   PDetec |   EDR/ESW |   Densité |\n",
    "|:-----|----------:|---------:|----------:|----------:|\n",
    "| min  |   1.1     |  0.2     |      0.8  |   0.6     |\n",
    "| max  |   6.7     |  4.9     |      5.2  |   4.9     |\n",
    "| mean |   2.66818 |  1.74091 |      2.45 |   2.18636 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XI. Truncation optimisation : Study on parameter variants\n",
    "\n",
    "Objective: How to choose key parameters ?\n",
    "* how many outliers ?\n",
    "* how many max iters ?\n",
    "* correlation with number of sightings ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data set, samples, transects, analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, run IX. 0, 1 and 2 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[IX. Truncation optimisation (short code / fast run)](#IX.-Truncation-optimisation-(short-code-%2B-fast-run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfOptimExplSpecs = ads.Analyser.explicitVariantSpecs('refin/ACDC2019-Naturalist-ExtraitSpecsOptanalyses.xlsx', \n",
    "                                                     ignore=['Params1_expl', 'Params2_expl'])\n",
    "\n",
    "dfOptimExplSpecs.drop(dfOptimExplSpecs[dfOptimExplSpecs[['TrGche', 'TrDrte', 'NbTrchMod', 'MultiOpt']]\n",
    "                                           .isnull().all(axis='columns')].index,\n",
    "                      inplace=True)\n",
    "\n",
    "dfOptimExplSpecs.drop(columns=['TrGche', 'TrDrte', 'NbTrchMod', 'MultiOpt'], inplace=True)\n",
    "\n",
    "nOptimExplSpecs = len(dfOptimExplSpecs)\n",
    "\n",
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parameter variants plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTimes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr2MaxPlan = ['chi2', 'ks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliersPctPlan = [2.5, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxItersPlan = [50, 100, 150, 200, 250, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3A. Or: Run optimisations according to the plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nParSets = len(expr2MaxPlan) * len(outliersPctPlan) * len(maxItersPlan)\n",
    "nOpt2Run = len(dfOptimExplSpecs) * nTimes * nParSets\n",
    "print(f'About to run {nOpt2Run} optimisations !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de dfOptimExplSpecs donnant les paramètres d'analyse / optimisation\n",
    "optimParamsSpecsCols  = ['FonctionClé', 'SérieAjust', 'CritChx', 'IntervConf',\n",
    "                         'TroncGche', 'TroncDrte', 'MethOutliers', 'NbTrModel', 'NbTrDiscr',\n",
    "                         'ExprOpt', 'ParExec', 'MoteurOpt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ldfResults = list()\n",
    "\n",
    "nParSetInd = 1\n",
    "for expr2Max in expr2MaxPlan:\n",
    "    \n",
    "    for olrsPct in outliersPctPlan:\n",
    "\n",
    "        for maxIters in maxItersPlan:\n",
    "\n",
    "            logger.info(f'Params set {nParSetInd}/{nParSets}: {expr2Max=}, {nTimes=}, {maxIters=}, {olrsPct=:.1f}')\n",
    "            \n",
    "            # Prepare optim. params.\n",
    "            dfMoreOptimCols = \\\n",
    "                pd.DataFrame([dict(CritChx='AIC', IntervConf=95,\n",
    "                                   TroncGche='auto', TroncDrte='auto',\n",
    "                                   MethOutliers=f'tucquant({olrsPct:.1f})',\n",
    "                                   NbTrModel='mult(2/3, 3/2)', NbTrDiscr=None,\n",
    "                                   ExprOpt=f'max({expr2Max})', ParExec=f'times({nTimes})',\n",
    "                                   MoteurOpt=f'zoopt({maxIters})')]*len(dfOptimExplSpecs))\n",
    "\n",
    "            dfOptVarExplSpecs = pd.concat([dfOptimExplSpecs.reset_index(drop=True), dfMoreOptimCols], axis='columns')\n",
    "\n",
    "            # Run optimisation.\n",
    "            zoptr = ads.MCDSZerothOrderTruncationOptimiser \\\n",
    "                            (dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea, \n",
    "                             transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                             sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                             abbrevCol=optAbbrevCol, abbrevBuilder=optimAbbrev,\n",
    "                             anlysIndCol=optIndCol, sampleIndCol=sampleNumCol,\n",
    "                             distanceUnit='Meter', areaUnit='Hectare',\n",
    "                             surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                             resultsHeadCols=dict(before=[optIndCol], sample=sampleSelCols, after=optimParamsSpecsCols),\n",
    "                             workDir='/tmp', logData=False,                 \n",
    "                             defCoreMaxIters=120)\n",
    "\n",
    "            results = zoptr.run(dfOptVarExplSpecs, threads=12)\n",
    "\n",
    "            zoptr.shutdown()\n",
    "\n",
    "            # Save results for this run\n",
    "            ldfResults.append(results.dfData)\n",
    "            \n",
    "            nParSet += 1\n",
    "        \n",
    "# Done : concat and save results.\n",
    "dfResults = pd.concat(ldfResults, ignore_index=True)\n",
    "\n",
    "resFileName = 'tmp/valtests-mcds-opter-res4stats.xlsx'\n",
    "dfResults.to_excel(resFileName, index=False)\n",
    "logger.info(f'Results saved to {resFileName}')\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3B. Or : Load results from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 'computed' not in dir():\n",
    "    computed = False\n",
    "if not computed:\n",
    "    \n",
    "    # Load results from file.\n",
    "    #resFileName = 'tmp/valtests-mcds-opter-res4stats-20200705.xlsx'\n",
    "    resFileName = 'tmp/valtests-mcds-opter-res4stats-20201103.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    dfResults = pd.read_excel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} results to process'.format(len(dfResults)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. First stats on optimisation results\n",
    "\n",
    "* raw stats : mean and std\n",
    "* first correlations : number of analyses / optimised criterium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfResults), dfResults.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optResCols = ['minDist', 'maxDist', 'fitDistCuts', 'chi2', 'ks', 'chi2*ks']\n",
    "#groupCols = [col for col in dfResults.columns if col not in optResCols]\n",
    "groupCols = ['Espèce', 'Passage', 'Adulte', 'Durée', 'FonctionClé', 'SérieAjust', 'MethOutliers', 'ExprOpt',\n",
    "             'MinDist', 'MaxDist', 'FitDistCuts', 'NFunEvals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Raw stats : mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats = dfResults.groupby(groupCols).agg(['mean', 'std'])\n",
    "dfStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resFileName = 'tmp/valtests-mcds-opter-stats.xlsx'\n",
    "dfStats.reset_index().to_excel(resFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Visual correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults['NFunEvalsR'] = dfResults.NFunEvals.apply(lambda v: int(50*np.ceil(v/50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for esp in dfResults['Espèce'].unique():\n",
    "    axes = dfResults[dfResults['Espèce'] == esp].plot.hexbin(y='NFunEvalsR', x='chi2', gridsize=(20, 6), figsize=(14, 3))\n",
    "    axes.set_title(f'{esp} : chi2 / NFunEvals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for esp in dfResults['Espèce'].unique():\n",
    "    axes = dfResults[dfResults['Espèce'] == esp].plot.hexbin(y='NFunEvalsR', x='ks', gridsize=(20, 6), figsize=(14, 3))\n",
    "    axes.set_title(f'{esp} : ks / NFunEvals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults['Outliers'] = dfResults.MethOutliers.apply(lambda s: float(s[len('tucquant('):-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = dfResults.plot.scatter(y='Outliers', x='chi2', figsize=(14, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = dfResults.plot.scatter(y='Outliers', x='ks', figsize=(14, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plyx.violin(dfResults, x='chi2', y='NFunEvalsR', facet_row='Outliers', color=\"Espèce\", orientation='h', height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plyx.violin(dfResults, x='ks', y='NFunEvalsR', facet_row='Outliers', color=\"Espèce\", orientation='h', height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Computed correlations\n",
    "\n",
    "(linéaires, de Pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonCorr(df, x, y):\n",
    "    \n",
    "    cv = np.cov(df[x].values, df[y].values)\n",
    "    \n",
    "    return pd.Series(dict(corr=cv[0, 1] / cv[0, 0] / cv[1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of analyses run / optimisation criterium\n",
    "groupCols = ['Espèce', 'Passage', 'Adulte', 'Durée', 'FonctionClé', 'SérieAjust', 'MethOutliers', 'ExprOpt']\n",
    "\n",
    "df = dfResults.loc[dfResults.ExprOpt == 'max(chi2)',\n",
    "                   groupCols + ['NFunEvals', 'chi2']].groupby(groupCols).apply(pearsonCorr, x='NFunEvals', y='chi2')\n",
    "df.rename(columns=dict(corr='NFun/Expr'), inplace=True)\n",
    "dfCorr = df.copy()\n",
    "\n",
    "df = dfResults.loc[dfResults.ExprOpt == 'max(ks)',\n",
    "                   groupCols + ['NFunEvals', 'ks']].groupby(groupCols).apply(pearsonCorr, x='NFunEvals', y='ks')\n",
    "df.rename(columns=dict(corr='NFun/Expr'), inplace=True)\n",
    "dfCorr = dfCorr.append(df)\n",
    "\n",
    "dfCorr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorr[dfCorr.index.get_level_values('ExprOpt') == 'max(ks)'].sort_values(by='NFun/Expr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCorr[dfCorr.index.get_level_values('ExprOpt') == 'max(chi2)'].sort_values(by='NFun/Expr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % of outliers excluded / optimisation criterium\n",
    "groupCols = ['Espèce', 'Passage', 'Adulte', 'Durée']\n",
    "\n",
    "df = dfResults.loc[dfResults.ExprOpt == 'max(chi2)',\n",
    "                   groupCols + ['Outliers', 'chi2']].groupby(groupCols).apply(pearsonCorr, x='Outliers', y='chi2')\n",
    "df.rename(columns=dict(corr='Outliers/Expr'), inplace=True)\n",
    "dfCorr = df.copy()\n",
    "\n",
    "df = dfResults.loc[dfResults.ExprOpt == 'max(ks)',\n",
    "                   groupCols + ['Outliers', 'ks']].groupby(groupCols).apply(pearsonCorr, x='Outliers', y='ks')\n",
    "df.rename(columns=dict(corr='Outliers/Expr'), inplace=True)\n",
    "dfCorr = dfCorr.append(df)\n",
    "\n",
    "dfCorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run analyses with optimised truncations\n",
    "\n",
    "(to get the actual numbers of sightings retained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Deduce analyses specs from optimisation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResults.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varIndCol = 'NumAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample and analysis params, and above all optimised truncation param. values from optimiser results.\n",
    "#optTgtCols = ['TrGche', 'TrDrte', 'NbTrchMod']\n",
    "optTgtCols = ['TroncGche', 'TroncDrte', 'NbTrModel']\n",
    "otherOptTgtCols = ['Outliers', 'NFunEvals']\n",
    "dfAnlysSpecs = dfResults[['Espèce', 'Passage', 'Adulte', 'Durée', 'FonctionClé', 'SérieAjust',\n",
    "                          'minDist', 'maxDist', 'fitDistCuts'] + optTgtCols + otherOptTgtCols].copy()\n",
    "\n",
    "# Add analysis abbreviation from truncation params optim. specs (not from optimised results).\n",
    "dfAnlysSpecs[anlysAbbrevCol] = dfAnlysSpecs.apply(analysisAbbrev, axis='columns')\n",
    "\n",
    "# No need for the truncation params optim. specs anymore\n",
    "dfAnlysSpecs.drop(columns=optTgtCols, inplace=True)\n",
    "\n",
    "# Rename optimised truncation param. columns for analysis\n",
    "dfAnlysSpecs.rename(columns=dict(minDist='TrGche', maxDist='TrDrte', fitDistCuts='NbTrchMod'), inplace=True)\n",
    "\n",
    "dfAnlysSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workDir = pl.Path('tmp/mcds-optstats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Or : Really run analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. MCDS Analyser object\n",
    "anlysParamCols = ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "\n",
    "anlysr = ads.MCDSAnalyser(dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea,\n",
    "                          transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                          sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                          abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                          distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                          surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                          resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                               after=anlysParamCols + [anlysAbbrevCol, 'Outliers', 'NFunEvals']),\n",
    "                          workDir=workDir,\n",
    "                          defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,\n",
    "                          defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,\n",
    "                          defMinDist=defMinDist, defMaxDist=defMaxDist,\n",
    "                          defFitDistCuts=defFitDistCuts, defDiscrDistCuts=defDiscrDistCuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ii. Check analysis explicit specs\n",
    "dfAnlysSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(dfExplParamSpecs=dfAnlysSpecs, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfAnlysSpecs) == len(dfResults)\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod'], str(userParamSpecCols)\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts'], str(intParamSpecCols)\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons, str(reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# iii. Run analyses\n",
    "\n",
    "# Analyses : 20mn for 8640 analyses with 12 threads on a Lenovo T490 (4-HT-core i5-8365U with PCI-e SSD)\n",
    "results = anlysr.run(dfAnlysSpecs, threads=12)\n",
    "\n",
    "computed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iiii. Save results for later reload or examination\n",
    "results.toExcel(workDir / 'valtests-mcds-analyser-afteropt-results.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.toExcel(workDir / 'valtests-mcds-analyser-afteropt-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Or : Load analyses from a previous run\n",
    "\n",
    "(already run and saved above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not computed:\n",
    "    \n",
    "    # An analyser object knowns how to build an empty results object ...\n",
    "    anlysr = ads.MCDSAnalyser(dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea,\n",
    "                              resultsHeadCols=dict(before=[varIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                   after=[anlysAbbrevCol]),\n",
    "                              transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                              sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                              abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleNumCol,\n",
    "                              distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                              surveyType=surveyType, distanceType=distanceType, clustering=clustering)\n",
    "    \n",
    "    results = anlysr.setupResults()\n",
    "    \n",
    "    anlysr.shutdown()\n",
    "    \n",
    "    # Load results from file.\n",
    "    resFileName = workDir / 'valtests-mcds-analyser-afteropt-results.xlsx'\n",
    "    print('Loading results from {} ...'.format(resFileName))\n",
    "\n",
    "    results.fromExcel(resFileName)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    print('Just computed, not reloading ...')\n",
    "    \n",
    "print('... {} analyses to study'.format(len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnRes = results.dfTransData('fr')\n",
    "dfAnRes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Other stats on analysis results\n",
    "\n",
    "Through NObs mainly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAnRes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for esp in dfAnRes['Espèce'].unique():\n",
    "    axes = dfAnRes[dfAnRes['Espèce'] == esp].plot.hexbin(y='NObs', x='Chi2 P', gridsize=(20, 6), figsize=(14, 3))\n",
    "    axes.set_title(f'{esp} : chi2 / NObs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for esp in dfAnRes['Espèce'].unique():\n",
    "    axes = dfAnRes[dfAnRes['Espèce'] == esp].plot.hexbin(y='NObs', x='KS P', gridsize=(20, 6), figsize=(14, 3))\n",
    "    axes.set_title(f'{esp} : KS / NObs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnRes['NFunEvalsR'] = dfAnRes.NFunEvals.apply(lambda v: int(50*np.ceil(v/50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plyx.scatter(dfAnRes[['Espèce', 'Chi2 P', 'NObs', 'Outliers', 'NFunEvalsR']].dropna(subset=['NObs']),\n",
    "             x='Chi2 P', y='NObs', facet_col='Outliers', facet_row='NFunEvalsR', color='Espèce', height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plyx.scatter(dfAnRes[['Espèce', 'KS P', 'NObs', 'Outliers', 'NFunEvalsR']].dropna(subset=['NObs']),\n",
    "             x='KS P', y='NObs', facet_col='Outliers', facet_row='NFunEvalsR', color='Espèce', height=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plyx.scatter(dfAnRes[['Espèce', 'Chi2 P', 'NObs', 'Outliers', 'NFunEvals']].dropna(subset=['NObs']),\n",
    "            x='NObs', y='Chi2 P', facet_col='Outliers', facet_row='Espèce', color='NFunEvals', height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plyx.scatter(dfAnRes[['Espèce', 'KS P', 'NObs', 'Outliers', 'NFunEvals']].dropna(subset=['NObs']),\n",
    "             x='NObs', y='KS P', facet_col='Outliers', facet_row='Espèce', color='NFunEvals', height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResultsSet.append\n",
    "\n",
    "Updated version thanks to pd.DataFrame.append(pd.Series) study above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append(dfData, sdfResult, sCustomHead):\n",
    "\n",
    "    if sCustomHead is not None:\n",
    "        if isinstance(sdfResult, pd.Series):\n",
    "            sdfResult = sCustomHead.append(sdfResult)\n",
    "        else: # DataFrame\n",
    "            dfCustomHead = pd.DataFrame([sCustomHead]*len(sdfResult)).reset_index(drop=True)\n",
    "            sdfResult = pd.concat([dfCustomHead, sdfResult], axis='columns')\n",
    "\n",
    "    # Normal append if _dfData not empty ; otherwise initialise _dfData in a way\n",
    "    # that keeps the original types of sdfResult / \n",
    "    if dfData.columns.empty:\n",
    "        if isinstance(sdfResult, pd.Series):\n",
    "            dfData = pd.DataFrame([sdfResult])\n",
    "        else: # DataFrame\n",
    "            dfData = sdfResult\n",
    "    else:\n",
    "        dfData = dfData.append(sdfResult, ignore_index=True)\n",
    "\n",
    "    return dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Initialise DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not empty, mono-index columns\n",
    "df = pd.DataFrame([dict(a=1, b=2.5, c='x', x='a', y=1, z=1.78),\n",
    "                   dict(a=2, b=4.5, c='y', x='b', y=2, z=5.88889)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Not empty, multi-index columns\n",
    "df = pd.DataFrame([{('a', 'z'): 1, ('b', 'y'): 2.5, ('c', 'x'): 'x', ('x', 'w'): 'a', ('y', 'v'): 1, ('z', 'u'): 1.78},\n",
    "                   {('a', 'z'): 2, ('b', 'y'): 4.5, ('c', 'x'): 'y', ('x', 'w'): 'b', ('y', 'v'): 2, ('z', 'u'): 5.88889}])\n",
    "df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Initialise Series / DataFrame to append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mono-index\n",
    "sh = pd.Series(dict(a=3, b=5.978, c='w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.Series(dict(x='c', y=4, z=9.567))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.DataFrame([dict(x='d', y=9, z=12.9),\n",
    "                   dict(x='e', y=8, z=7.778)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-index\n",
    "sh = pd.Series({('a', 'z'): 3, ('b', 'y'): 5.978, ('c', 'x'): 'w'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.Series({('x', 'w'): 'c', ('y', 'v'): 4, ('z', 'u'): 9.567})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = pd.DataFrame([{('x', 'w'): 'd', ('y', 'v'): 9, ('z', 'u'): 12.9},\n",
    "                   {('x', 'w'): 'e', ('y', 'v'): 8, ('z', 'u'): 7.778}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. append Series /DataFrame to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = append(df, sr, sh)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. See what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sh.a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
