{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Auto table of contents -->\n",
    "<h1 class='tocIgnore'>AutoDS : Mise au point et tests unitaires</h1>\n",
    "<p>(module <b>autods</b> d'interface python à MCDS.exe)</p>\n",
    "<div style=\"overflow-y: auto\">\n",
    "  <h2 class='tocIgnore'>Table des matières</h2>\n",
    "  <div id=\"toc\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib as pl\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import OrderedDict as odict, namedtuple as ntuple\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly as ply\n",
    "import plotly.graph_objs as plygo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Warnings as Exception\n",
    "#import warnings\n",
    "#warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests unitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Détection de Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autods as ads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classe DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excel source (path as simple string)\n",
    "ds = ads.SampleDataSet(source='refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                       decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "ds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV source with ',' as decimal point (path as pl.Path)\n",
    "ds = ads.SampleDataSet(source=pl.Path('refin/ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt'),\n",
    "                       decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])\n",
    "\n",
    "assert not any(ds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in ds.decimalFields), \\\n",
    "       'Error: Some strings found in declared decimal fields ... any decimal format issue ?'\n",
    "\n",
    "ds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV source with '.' as decimal point\n",
    "ds = ads.SampleDataSet(source=pl.Path('refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt'),\n",
    "                       decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])\n",
    "\n",
    "assert not any(ds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in ds.decimalFields), \\\n",
    "       'Error: Some strings found in declared decimal fields ... any decimal format issue ?'\n",
    "\n",
    "ds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame source.\n",
    "dfData = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],\n",
    "                      data=[('2019-05-13', 3.5, 'TURMER', 23, 2,   83),\n",
    "                            ('2019-05-15', np.nan, 'TURMER', 23, 2,   27.355),\n",
    "                            ('2019-05-13', 0, 'ALAARV', 29, 2,   56.85),\n",
    "                            ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3,  7.2),\n",
    "                            ('2019-06-01', 2, 'PHICOL', 12, 1,  np.nan),\n",
    "                            ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),\n",
    "                           ])\n",
    "dfData['Region'] = 'ACDC'\n",
    "dfData['Surface'] = '2400'\n",
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])\n",
    "ds.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classes XXEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Instanciation et chargement des spécifs sur les stats en sortie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eng = ads.MCDSEngine(workDir='tmp/test out') # Simple string path\n",
    "    print('Error: Should have raised an AssertionError !')\n",
    "except AssertionError as exc:\n",
    "    print('Good forbidden chars detection:', exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'test out')) # pl.Path path\n",
    "    print('Error: Should have raised an AssertionError !')\n",
    "except AssertionError as exc:\n",
    "    print('Good forbidden chars detection:', exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = eng.setupRunFolder(runPrefix='uni') # Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Génération fichier de données en entrée de MCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataFileName = eng.buildDataFile(dataSet=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Génération fichier de \"commandes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmdFileName = eng.buildCmdFile(estimKeyFn='HNORMAL', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### d. Execution en mode \"debug\"\n",
    "\n",
    "(génération des fichiers cmd et data, mais pas d'appel à l'exécutable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runCode, runTime, runDir = eng.run(ds, realRun=False, runPrefix='int',\n",
    "                                   estimKeyFn='UNIFORM', estimAdjustFn='POLY',\n",
    "                                   estimCriterion='AIC', cvInterval=95)\n",
    "assert runCode == 0, 'Should have NOT run (run code = 0)'\n",
    "dict(runCode=runCode, runDir=runDir, runTime=runTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### e. Exécution réelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runCode, runTime, runDir = eng.run(ds, realRun=True, runPrefix='int',\n",
    "                                   estimKeyFn='UNIFORM', estimAdjustFn='POLY',\n",
    "                                   estimCriterion='AIC', cvInterval=95)\n",
    "assert runCode == 2, 'Should have run with warnings (run code = 2)'\n",
    "dict(runCode=runCode, runDir=runDir, runTime=runTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Génération fichier de données en entrée pour Distance\n",
    "\n",
    "(mode 'point transect' uniquement pour le moment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Path(eng.workDir, 'distance-in').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDataFileName = \\\n",
    "    eng.buildDistanceDataFile(ds, tgtFilePathName=os.path.join(eng.workDir, 'distance-in', 'import-data-noextra.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distDataFileName = \\\n",
    "    eng.buildDistanceDataFile(ds, tgtFilePathName=pl.Path(eng.workDir, 'distance-in', 'import-data-withextra.txt'),\n",
    "                              withExtraFields=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Classes XXResultsSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autods.data import ResultsSet\n",
    "\n",
    "# A specialized results set for the tests = with extra. post-computed columns : Delta AIC\n",
    "class TestResultsSet(ResultsSet):\n",
    "    \n",
    "    def __init__(self, miCustomCols=None, dfCustomColTrans=None,\n",
    "                       dComputedCols=None, dfComputedColTrans=None):\n",
    "        \n",
    "        # Initialise base.\n",
    "        super().__init__(ads.MCDSAnalysis, miCustomCols, dfCustomColTrans, dComputedCols, dfComputedColTrans)\n",
    "        \n",
    "    # Post-computations.\n",
    "    def postComputeColumns(self):\n",
    "        \n",
    "        # Compute Delta AIC (AIC - min(group)) per { species, sample, precision, duration } group.\n",
    "        # a. Minimum AIC per group\n",
    "        aicColInd = ('detection probability', 'AIC value', 'Value')\n",
    "        aicGroupColInds = [('sample', 'species', 'Value'), ('sample', 'periods', 'Value'),\n",
    "                           ('sample', 'duration', 'Value'), ('variant', 'precision', 'Value')]\n",
    "        df2Join = self._dfData.groupby(aicGroupColInds)[[aicColInd]].min()\n",
    "        \n",
    "        # b. Rename computed columns to target\n",
    "        deltaAicColInd = ('detection probability', 'Delta AIC', 'Value')\n",
    "        df2Join.columns = pd.MultiIndex.from_tuples([deltaAicColInd])\n",
    "        \n",
    "        # c. Join the column to the target data-frame\n",
    "        self._dfData = self._dfData.join(df2Join, on=aicGroupColInds)\n",
    "        \n",
    "        # d. Compute delta-AIC in-place\n",
    "        self._dfData[deltaAicColInd] = self._dfData[aicColInd] - self._dfData[deltaAicColInd]\n",
    "\n",
    "# Results object construction\n",
    "miCustCols = pd.MultiIndex.from_tuples([('id', 'index', 'Value'),\n",
    "                                        ('sample', 'species', 'Value'),\n",
    "                                        ('sample', 'periods', 'Value'),\n",
    "                                        ('sample', 'duration', 'Value'),\n",
    "                                        ('variant', 'precision', 'Value')])\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=['index', 'species', 'periods', 'duration', 'precision'],\n",
    "                           fr=['numéro', 'espèce', 'périodes', 'durée', 'précision']))\n",
    "\n",
    "dCompCols = { ('detection probability', 'Delta AIC', 'Value'): 18 } # Right before AIC\n",
    "dfCompColTrans = \\\n",
    "    pd.DataFrame(index=dCompCols.keys(),\n",
    "                 data=dict(en=['Delta AIC'], fr=['Delta AIC']))\n",
    "\n",
    "rs = TestResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                    dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rs.dfData.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sHead = pd.Series(index=miCustCols, data=list(range(len(miCustCols))))\n",
    "miResCols = ads.MCDSAnalysis.MIRunColumns.append(ads.MCDSEngine.statModCols())\n",
    "sResult = pd.Series(index=miResCols, data=list(range(len(miResCols))))\n",
    "rs.append(sResult, sCustomHead=sHead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw = rs.dfData\n",
    "dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrans = rs.dfTransData('fr')\n",
    "dfTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfRaw.columns) == len(dfTrans.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRaw.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AutoDS data tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObs = pd.read_csv('refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt', sep='\\t', decimal=',')\n",
    "dfObs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countCols =  ['nMalAd10', 'nAutAd10', 'nMalAd5', 'nAutAd5']\n",
    "sCounts = dfObs[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfObs), sCounts.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObs) == 724\n",
    "assert not any(sCounts - pd.Series({'nMalAd10': 613, 'nAutAd10': 192, 'nMalAd5': 326, 'nAutAd5': 102}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. separateMultiCategoricalCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsMonoCat = ads.separateMultiCategoricalCounts(dfObs, countCols)\n",
    "len(dfObsMonoCat), dfObsMonoCat[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObsMonoCat) == 1125\n",
    "assert not any(dfObsMonoCat[countCols].sum() - sCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfObsMonoCat[countCols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsMonoCat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. individualiseMonoCategoricalCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndiv = ads.individualiseMonoCategoricalCounts(dfObsMonoCat, countCols)\n",
    "len(dfObsIndiv), dfObsIndiv[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObsIndiv) == 1233\n",
    "assert not any(dfObsIndiv[countCols].sum() - sCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Categorise sightings\n",
    "\n",
    "Needed for adding absence data below\n",
    "\n",
    "(no more counts - by the way, all 0 or 1 - => only catgories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should not see any sightings with all null counts\n",
    "assert dfObsIndiv[~dfObsIndiv[countCols].any(axis='columns')].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv['Adulte'] = \\\n",
    "  dfObsIndiv[countCols].apply(lambda sNb: 'm' if 'Mal' in sNb[sNb > 0].index[0] else 'a', axis='columns')\n",
    "dfObsIndiv['Duree'] = \\\n",
    "  dfObsIndiv[countCols].apply(lambda sNb: '5' if '5' in sNb[sNb > 0].index[0] else '10', axis='columns')\n",
    "\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.drop(columns=countCols, inplace=True)\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. addTransectEffort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = ads.addTransectEffort(dfObsIndiv, transectCol='Point', passesCol='Passage')\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(dfObsIndiv[dfObsIndiv.Effort != 2].Point.unique() == [42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juste pour voir ...\n",
    "#dfObsIndiv.to_excel('AutoDS/tmp/tools-unitests-obs-indiv.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Extract transect info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectIdCol = 'Point'\n",
    "transectCols = [transectIdCol, 'Effort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransects = dfObsIndiv[transectCols].drop_duplicates(subset=transectIdCol)\n",
    "dfTransects.set_index(transectIdCol, inplace=True)\n",
    "dfTransects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfTransects) == 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Add abscence sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample columns\n",
    "sampleCols = ['Passage', 'Adulte', 'Duree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1 random sample\n",
    "espece = 'Fauvette à tête noire'\n",
    "passage = 'a'\n",
    "adulte = 'm'\n",
    "duree = '10'\n",
    "dfObsIndivSmpl = dfObsIndiv[(dfObsIndiv.Passage == passage) & (dfObsIndiv.Adulte == adulte) & (dfObsIndiv.Duree == duree)\n",
    "                            & (dfObsIndiv.Espece == espece)]\n",
    "\n",
    "assert len(dfObsIndivSmpl) == 36 and dfObsIndivSmpl[transectIdCol].nunique() == 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, sampleCols, dfTransects)\n",
    "len(dfObsIndivAbscSmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for no change in sample columns\n",
    "assert list(dfObsIndivAbscSmpl.columns) == list(dfObsIndivSmpl.columns)\n",
    "\n",
    "# Check for number of added rows\n",
    "assert len(dfObsIndivAbscSmpl) == 39 # 36 sightings + 3 missings transects\n",
    "\n",
    "# Check for final number of transects\n",
    "assert dfObsIndivAbscSmpl[dfTransects.index.name].nunique() == 21\n",
    "\n",
    "# Check for no change in sample identification\n",
    "assert list(dfObsIndivAbscSmpl.Espece.unique()) == [espece, None] # Noe for absence sightings !\n",
    "assert list(dfObsIndivAbscSmpl.Passage.unique()) == [passage]\n",
    "assert list(dfObsIndivAbscSmpl.Adulte.unique()) == [adulte]\n",
    "assert list(dfObsIndivAbscSmpl.Duree.unique()) == [duree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfObsIndivAbscSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Performance test\n",
    "print('Espece      Passage  Adulte Duree NbDonnees')\n",
    "\n",
    "for espece in ['Fauvette à tête noire', 'Alouette des champs', 'Fauvette grisette', 'Pouillot véloce']: \n",
    "    \n",
    "    for passage in ['a', 'b', 'a+b']: \n",
    "\n",
    "        for adulte in ['m', 'a', 'm+a']:\n",
    "\n",
    "            for duree in ['5', '10']:\n",
    "\n",
    "                passages = passage.split('+')\n",
    "                adultes = adulte.split('+')\n",
    "                dfObsIndivSmpl = dfObsIndiv[dfObsIndiv.Passage.isin(passages) & dfObsIndiv.Adulte.isin(adultes) \\\n",
    "                                            & (dfObsIndiv.Duree == duree) & (dfObsIndiv.Espece == espece)]\n",
    "\n",
    "                try:\n",
    "                    print(espece, passage, adulte, duree, ':', len(dfObsIndivSmpl), '=> ', end='')\n",
    "                    dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, sampleCols, dfTransects)\n",
    "                    print(len(dfObsIndivAbscSmpl))\n",
    "                except Exception as e:\n",
    "                    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Generate implicit variant combination table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'individus par espèce, pour voir quelles espèces on va analyser\n",
    "dfIndivCounts = dfObsIndiv.loc[dfObsIndiv.Adulte == 'm', ['Espece', 'Adulte']].groupby('Espece').count()\n",
    "\n",
    "dfIndivCounts.rename(columns=dict(Adulte='Males'), inplace=True)\n",
    "dfIndivCounts.sort_values(by='Males', ascending=False, inplace=True)\n",
    "\n",
    "dfIndivCounts[dfIndivCounts.Males >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMaxMal10 = 30\n",
    "varEspeces = list(dfIndivCounts[dfIndivCounts.Males >= nMaxMal10].index) # 1 variante par espèce\n",
    "\n",
    "varPassages = ['a+b'] # Passage a ou b => 1 seule variante\n",
    "varAdultes = ['m', 'm+a'] # Les mâles, et ensuite les mâles et autres adultes (=> 2 variantes)\n",
    "varDurees = ['5', '10'] # 5 1ères mn, ou toutes les 10 => 2 variantes\n",
    "\n",
    "dfImplSampSpecs = ads.implicitPartialVariantSpecs(dict(Especes=varEspeces, Passages=varPassages,\n",
    "                                                       Adultes=varAdultes, Durees=varDurees))\n",
    "dfImplSampSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Explicit variant combination generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExplSampSpecs = ads.explicitPartialVariantSpecs(dfImplSampSpecs)\n",
    "dfExplSampSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Final explicitation of all variants\n",
    "\n",
    "from user specs (implicit and explict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddfUserVariantSpecs = pd.read_excel('refin/ACDC2019-Naturalist-SpecsAnalyses.xlsx', sheet_name=None)\n",
    "print('sheets:', ', '.join(oddfUserVariantSpecs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinalExplSpecs = ads.explicitVariantSpecs(oddfUserVariantSpecs)\n",
    "dfFinalExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to see by eye\n",
    "dfFinalExplSpecs.to_excel('tmp/tools-unitests-final-expl-specs.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational checks\n",
    "nEch1Vars = 1\n",
    "df = oddfUserVariantSpecs['Echant1_impl']\n",
    "for col in df.columns:\n",
    "    nEch1Vars *= len(df[col].dropna())\n",
    "\n",
    "    \n",
    "nEch2Vars = 1\n",
    "df = oddfUserVariantSpecs['Echant2_impl']\n",
    "for col in df.columns:\n",
    "    nEch2Vars *= len(df[col].dropna())\n",
    "    \n",
    "nModVars = 1\n",
    "df = oddfUserVariantSpecs['Modl_impl']\n",
    "for col in df.columns:\n",
    "    nModVars *= len(df[col].dropna())\n",
    "\n",
    "nEch1ParWithVars = \\\n",
    "  len(oddfUserVariantSpecs['Params1_expl'].drop_duplicates(subset=oddfUserVariantSpecs['Echant1_impl'].columns))\n",
    "\n",
    "nEch1Pars = len(oddfUserVariantSpecs['Params1_expl'])\n",
    "\n",
    "nEch2ParWithVars = \\\n",
    "  len(oddfUserVariantSpecs['Params2_expl'].drop_duplicates(subset=oddfUserVariantSpecs['Echant2_impl'].columns))\n",
    "\n",
    "nEch2Pars = len(oddfUserVariantSpecs['Params2_expl'])\n",
    "\n",
    "nExpdVars = nModVars * (nEch1Pars + nEch1Vars - nEch1ParWithVars + nEch2Pars + nEch2Vars - nEch2ParWithVars)\n",
    "assert len(dfFinalExplSpecs) == nExpdVars\n",
    "\n",
    "nModVars, nEch1Pars, nEch1Vars, nEch1ParWithVars, nEch2Pars, nEch2Vars, nEch2ParWithVars, nExpdVars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests d'intégration module autods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MCDSEngine : Génération de fichiers d'entrée pour Distance\n",
    "\n",
    "* via un jeu de fichiers d'entrée bruts Excel, et leur export de référence, éprouvé dans Distance,\n",
    "* et comparaison du produit de XXEngine.buildDistanceDataFile à cette référence.\n",
    "\n",
    "Nécessite 2.g. ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDistCases = pd.DataFrame([dict(inFileName='ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                                 decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'],\n",
    "                                 refOutFileName='ACDC2019-Papyrus-ALAARV-saisie-5-cols.txt', withExtraFields=False),\n",
    "                            dict(inFileName='ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                                 decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'],\n",
    "                                 refOutFileName='ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.txt', withExtraFields=True)])\n",
    "dfDistCases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eng = ads.MCDSEngine(workDir=pl.Path('tmp/mcds-out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fails = 0\n",
    "for ind, sCase in dfDistCases.iterrows():\n",
    "    \n",
    "    print('#', ind, ':', sCase.inFileName)\n",
    "\n",
    "    # Create data set\n",
    "    ds = ads.SampleDataSet(source=pl.Path('refin', sCase.inFileName),\n",
    "                           decimalFields=sCase.decimalFields)\n",
    "    \n",
    "    # Build distance import data file\n",
    "    ofn = pl.Path(eng.workDir, 'distance-in', sCase.refOutFileName)\n",
    "    ofn = eng.buildDistanceDataFile(dataSet=ds, tgtFilePathName=ofn, withExtraFields=sCase.withExtraFields)\n",
    "    \n",
    "    # Compare generated file to reference\n",
    "    rfn = pl.Path('refout', sCase.refOutFileName)\n",
    "    with open(ofn, 'r') as fOut, open(rfn, 'r') as fRef:\n",
    "        if fOut.read() == fRef.read():\n",
    "            print('Success : Conform to reference.')\n",
    "        else:\n",
    "            print('Error: Generated file differs from reference', rfn)\n",
    "            fails += 1\n",
    "            \n",
    "    print()\n",
    "    \n",
    "print('All test cases succeeded !' if fails == 0 else 'Error: {} test case(s) failed.'.format(fails))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCDSEngine : Exécution avec de vraies données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ads.SampleDataSet(source=pl.Path('refin', 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx'),\n",
    "                       decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "\n",
    "eng = ads.MCDSEngine(workDir='tmp/mcds-out')\n",
    "\n",
    "runCode, runTime, runDir = eng.run(ds, realRun=True, runPrefix='int',\n",
    "                                   estimKeyFn='UNIFORM', estimAdjustFn='POLY',\n",
    "                                   estimCriterion='AIC', cvInterval=95)\n",
    "assert runCode == 2, 'Should have run with warnings (run code = 2)'\n",
    "dict(runCode=runCode, runDir=runDir, runTime=runTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usefull for later plot file decoding developpement\n",
    "realRunWorkDir = runDir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mise au point du code du module autods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection de Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance software detection params.\n",
    "DistanceSuppVers = [7, 6] # Lastest first.\n",
    "DistancePossInstPaths = [pl.Path().resolve(), pl.Path('C:\\\\Program files (x86)'), pl.Path('C:\\\\Program files')]\n",
    "\n",
    "# Find given executable installation dir.\n",
    "def findExecutable(exeFileName):\n",
    "\n",
    "    exeFilePathName = None\n",
    "    print('Looking for {} ...'.format(exeFileName))\n",
    "    for path in DistancePossInstPaths:\n",
    "        for ver in DistanceSuppVers:\n",
    "            exeFileDir = path / 'Distance {}'.format(ver)\n",
    "            print(' - checking {} : '.format(exeFileDir), end='')\n",
    "            exeFN = exeFileDir / exeFileName\n",
    "            if not exeFN.exists():\n",
    "                print('no.')\n",
    "            else:\n",
    "                print('yes !')\n",
    "                exeFilePathName = exeFN\n",
    "                break\n",
    "        if exeFilePathName:\n",
    "            break\n",
    "\n",
    "    if exeFilePathName:\n",
    "        print('{} found in {}'.format(exeFileName, exeFileDir))\n",
    "    else:\n",
    "        raise Exception('Could not find {} ; please install Distance software (V6 or later)'.format(exeFileName))\n",
    "\n",
    "    return exeFilePathName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findExecutable('MCDS.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results reports styling\n",
    "\n",
    "(to stress interesting and/or important things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrSynRes = results.dfTransData('fr', subset=synthCols)\n",
    "dfTrSynRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cChrGray = '#869074'\n",
    "cBckGreen, cBckGray = '#e0ef8c', '#dae3cb'\n",
    "cSclGreen, cSclOrange, cSclRed = '#cbef8c', '#f9da56', '#fe835a' #'#f25e2d'\n",
    "scaledColors = [cSclGreen, cSclOrange, cSclRed]\n",
    "scaledColorsRvd = list(reversed(scaledColors))\n",
    "\n",
    "dExCodeColors = dict(zip([1, 2, 3], scaledColors))\n",
    "def colorExecCodes(sCodes):\n",
    "    return ['background-color: ' + dExCodeColors.get(c, dExCodeColors[3]) for c in sCodes]\n",
    "\n",
    "def scaledColorV(v, thresholds, colors): # len(thresholds) == len(colors) - 1\n",
    "    if pd.isnull(v):\n",
    "        return cBckGray\n",
    "    for ind, thresh in enumerate(thresholds):\n",
    "        if v > thresh:\n",
    "            return colors[ind]\n",
    "    return colors[-1]\n",
    "def scaledColorS(sValues, thresholds, colors):\n",
    "    return ['background-color: ' + scaledColorV(v, thresholds, colors) for v in sValues]\n",
    "\n",
    "densCVThresholds = [0.4, 0.1]\n",
    "\n",
    "dfs = dfTrSynRes \\\n",
    "        .sort_values(by=['Espèce', 'Echantillon', 'Précision', 'Durée', 'Delta AIC']) \\\n",
    "        .style \\\n",
    "        .set_precision(3) \\\n",
    "        .set_properties(subset=pd.IndexSlice[dfTrSynRes[dfTrSynRes['Delta AIC'] == 0].index, :],\n",
    "                        **{'background-color': cBckGreen}) \\\n",
    "        .apply(colorExecCodes, subset=['CodEx'], axis='columns') \\\n",
    "        .apply(scaledColorS, subset=['CoefVar Densité'], axis='columns',\n",
    "               thresholds=densCVThresholds, colors=scaledColors) \\\n",
    "        .set_properties(subset=pd.IndexSlice[dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index, :],\n",
    "                         **{'color': cChrGray}) \\\n",
    "        .where(pd.isnull, 'color: transparent')\n",
    "\n",
    "    #.format(lambda v: v if not pd.isnull(v) else '') # Détruit une partie des arrondis, auugmente la précision ???\n",
    "\n",
    "    #.set_precision(3) # Not really usable, as only for the whole frame\n",
    "\n",
    "    #.apply(lambda s: ['color: grey']*len(s), subset=pd.IndexSlice[dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index, :],\n",
    "    #       axis='index') # OK\n",
    "    \n",
    "    #.apply(lambda s: ['color: grey']*len(s), subset=dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index,\n",
    "    #       axis='index') # KO\n",
    "    \n",
    "dfs.to_excel('tmp/styled-results.xlsx')\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode MCDS plots file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcFileName = pl.Path(realRunWorkDir, 'plots.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(srcFileName, 'r').readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itLines = iter(lines)\n",
    "chapters = list()\n",
    "for title in itLines:\n",
    "    #title = next(itLines)\n",
    "    subTitle = next(itLines)\n",
    "    xLabel = next(itLines)\n",
    "    yLabel = next(itLines)\n",
    "    xMin, xMax, yMin, yMax = [float(s) for s in next(itLines).split()]\n",
    "    nDataRows = int(next(itLines))\n",
    "    dataRows = list()\n",
    "    for l in range(nDataRows):\n",
    "        dataRows.append([float(s) for s in next(itLines).split()])\n",
    "    chapters.append(dict(title=title, subTitle=subTitle, dataRows=dataRows, #nDataRows=nDataRows,\n",
    "                         xLabel=xLabel, yLabel=yLabel, xMin=xMin, xMax=xMax, yMin=yMin, yMax=yMax))\n",
    "len(chapters), chapters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QQ-plot\n",
    "chapter = chapters[0]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(chapter['dataRows'])\n",
    "dfQqData = pd.DataFrame(data=chapter['dataRows'], columns=['If the fit was perfect ...', 'Real observations'],\n",
    "                        index=np.linspace(0.5/n, 1.0-0.5/n, n))\n",
    "dfQqData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 : OK\n",
    "#fig = plt.figure(figsize=(16, 6))\n",
    "#axes = fig.subplots()\n",
    "#_ = dfQqData.plot(ax=axes, color=['blue', 'red'], grid=True,\n",
    "#                  xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "\n",
    "# Option 2 : OK\n",
    "axes = dfQqData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True,\n",
    "                     \n",
    "                     xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "fig = axes.figure\n",
    "\n",
    "axes.legend(['If the fit was perfect ...', 'Real observations'], fontsize=12)\n",
    "axes.set_facecolor('#f9fbf3')\n",
    "axes.figure.patch.set_facecolor('#f9fbf3')\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes.figure.savefig('tmp/mlb-qqplot.jpg', box_inches='tight')\n",
    "axes.figure.savefig('tmp/mlb-qqplot.png', box_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfQqData.index, y=dfQqData['If the fit was perfect ...'],\n",
    "                            name='If the fit was perfect ...', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfQqData.index, y=dfQqData['Real observations'],\n",
    "                            name='Real observations', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(x=0.09, y=0.90, bordercolor='black', borderwidth=1),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow ... VERY slooooooooow !\n",
    "fig.write_image(\"tmp/ply-qqplot.svg\")\n",
    "fig.write_image(\"tmp/ply-qqplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection probability\n",
    "chapter = chapters[1]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDetProbData = pd.DataFrame(data=chapter['dataRows'], \n",
    "                             columns=[chapter['xLabel'], chapter['yLabel'] + ' (sampled)', chapter['yLabel'] + ' (fitted)'])\n",
    "dfDetProbData.set_index(chapter['xLabel'], inplace=True)\n",
    "dfDetProbData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "axes = dfDetProbData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True,\n",
    "                          xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.legend(dfDetProbData.columns, fontsize=12)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfDetProbData.index, y=dfDetProbData[chapter['yLabel'] + ' (sampled)'],\n",
    "                            name=chapter['yLabel'] + ' (sampled)', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfDetProbData.index, y=dfDetProbData[chapter['yLabel'] + ' (fitted)'],\n",
    "                            name=chapter['yLabel'] + ' (fitted)', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(x=0.65, y=0.85*chapter['yMax'], bordercolor='black', borderwidth=1),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Detection probability\n",
    "chapter = chapters[2]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProdDensData = pd.DataFrame(data=chapter['dataRows'], \n",
    "                              columns=[chapter['xLabel'], chapter['yLabel'] + ' (sampled)', chapter['yLabel'] + ' (fitted)'])\n",
    "dfProdDensData.set_index(chapter['xLabel'], inplace=True)\n",
    "dfProdDensData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = dfProdDensData.plot(figsize=(16, 6), color=['blue', 'red'],\n",
    "                           xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.legend(dfProdDensData.columns, fontsize=12)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfProdDensData.index, y=dfProdDensData[chapter['yLabel'] + ' (sampled)'],\n",
    "                            name=chapter['yLabel'] + ' (sampled)', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfProdDensData.index, y=dfProdDensData[chapter['yLabel'] + ' (fitted)'],\n",
    "                            name=chapter['yLabel'] + ' (fitted)', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(xanchor='right', yanchor='top', bordercolor='black', borderwidth=1),\n",
    "                  #margin=plygo.layout.Margin(l=40, r=40, b=40, t=40, pad=0),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode stats (actual results) from MCDS work folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = implib.reload(ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results set to store results into.\n",
    "miCustCols = pd.MultiIndex.from_tuples([('id', 'ExecCase', 'Value')])\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols, data=dict(en=['ExecCase'], fr=['CasExec']))\n",
    "\n",
    "results = ads.MCDSResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis engine\n",
    "mcds = ads.MCDSEngine(workDir='refout/dist-order-sens-min',\n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process folders in engine work folder.\n",
    "for folder in os.listdir(mcds.workDir):\n",
    "    \n",
    "    # Skip folders that are not MCDS run ones.\n",
    "    folderPath = os.path.join(mcds.workDir, folder)\n",
    "    if not os.path.isdir(folderPath):\n",
    "        continue\n",
    "    if os.path.splitext(folder)[1] or 'stats.txt' not in os.listdir(folderPath):\n",
    "        print(f'Skipping {folderPath}, not an MCDS.exe run folder with a stats.txt file')\n",
    "        continue\n",
    "        \n",
    "    # Tell the engine were it has run (even it does not rember it ;-)\n",
    "    _ = mcds.setupRunFolder(forceSubFolder=folder)\n",
    "    \n",
    "    # Decode results.\n",
    "    sRes = mcds.decodeStats()\n",
    "    print()\n",
    "    \n",
    "    # Store them for later.\n",
    "    sHead = pd.Series(data=[folder], index=miCustCols)\n",
    "    results.append(sRes, sCustomHead=sHead)\n",
    "\n",
    "# Tadaaaaaaa !\n",
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('en').to_excel(pl.Path('tmp', 'dist-order-sens-auto-results.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitary tests for reference / actual results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [np.nan, -np.inf, -1.0e12, -1.0e5, -1.0-1e-5, -1.0, -1.0+1e-5, -1.0e-8, 0.0, 1.0e-8, 1.0, 1.0e5, 1.0e12, np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual / reference closeness measure : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)\n",
    "# = Compute the order of magnitude that separate the difference from the absolute max. of the two values\n",
    "# The greater it is, the lower the relative difference\n",
    "#    Ex: 3 = 10**3 ratio between difference absolue max. of the two,\n",
    "#        +inf = NO difference at all,\n",
    "#        0 = bad, one of the two is 0, and the other not,\n",
    "# See unitary test below.\n",
    "def closeness(sRefAct):\n",
    "    \n",
    "    x, y = sRefAct.to_list()\n",
    "    \n",
    "    # Special cases with 1 NaN, or 1 or more inf => all different\n",
    "    if np.isnan(x):\n",
    "        if not np.isnan(y):\n",
    "            return 0 # All different\n",
    "    elif np.isnan(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    if np.isinf(x) or np.isinf(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    # Normal case\n",
    "    c = abs(x - y)\n",
    "    if not np.isnan(c) and c != 0:\n",
    "        c = c / max(abs(x), abs(y))\n",
    "    \n",
    "    return np.inf if c == 0 else round(-np.log10(c), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aClose = np.ndarray(shape=(len(values), len(values)))\n",
    "for r in range(len(values)):\n",
    "    for c in range(len(values)):\n",
    "        try:\n",
    "            aClose[r, c] = closeness(pd.Series([values[r], values[c]]))\n",
    "        except Exception as exc:\n",
    "            print(exc, r, c, values[r], values[c])\n",
    "pd.DataFrame(data=aClose, index=values, columns=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximité infinie sur la diagonale (sauf pour nan et +/-inf)\n",
    "assert all(np.isnan(values[i]) or np.isinf(values[i]) or np.isinf(aClose[i, i]) for i in range(len(values))), \\\n",
    "       'Error: Inequality on the diagonal'\n",
    "\n",
    "# Pas de proximité infinie ailleurs\n",
    "assert all(r == c or not np.isinf(aClose[r, c]) for r in range(len(values)) for c in range(len(values))), \\\n",
    "       'Error: No equality should be found outside the diagonal'\n",
    "\n",
    "# Bonne proximité uniquement autour de -1\n",
    "whereClose = [i for i in range(len(values)) if abs(values[i] + 1) <= 1.0e-5]\n",
    "assert all(aClose[r, c] > 4 for r in whereClose for c in whereClose), 'Error: Unexpectedly bad closeness around -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ancienne méthode qui ne marche pas.\n",
    "# Comparaison actual / reference : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)\n",
    "# => Plus c'est grand, plus petite est la différence relative entre les 2\n",
    "#    Ex: 3 = facteur 10**3 entre différence et valeurs absolues ; +inf = AUCUNE différence\n",
    "#        0 = pas bon, l'un des 2 est nul n'autre pas du tout\n",
    "# Cf. tests unitaires plus bas.\n",
    "#dfRelDif = pd.DataFrame(index=dfRefRes4c.index)\n",
    "#for col in dfRefRes4c.columns:\n",
    "#    dfRelDif['NormalCases'] = ~((dfActRes4c[col].isnull() & dfRefRes4c.notnull()) \\\n",
    "#                                | (dfActRes4c[col].notnull() & dfRefRes4c.isnull()) \\\n",
    "#                                | dfActRes4c[col].notnull() | dfRefRes4c.isnull())\n",
    "#    dfRelDif[col] = abs(dfActRes4c[col] - dfRefRes4c[col])\n",
    "#    dfRelDif[col].where(dfRelDif[col].isnull() | dfRelDif[col] == 0,\n",
    "#                        dfRelDif[col] / pd.DataFrame(dict(act=dfActRes4c[col], ref=dfRefRes4c[col])).abs().max(axis='columns'),\n",
    "#                        inplace=True)\n",
    "#    dfRelDif[col].where(dfRelDif['NormalCases'], 1, inplace=True) # Force special case to \"all different\"\n",
    "#    dfRelDif.drop(columns=['NormalCases'], inplace=True)\n",
    "#    dfRelDif[col] = np.round(-np.log10(dfRelDif[col]), 1)\n",
    "#    \n",
    "#dfRelDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stats columns translation file\n",
    "\n",
    "(from documentation stats & modules specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgtTransFileName = 'tmp/stat-mod-trans.auto.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(object):\n",
    "    \n",
    "    def __init__(self, dTrans, lang='en'):\n",
    "        assert 'en' in dTrans, 'At least \"en\" translation must be defined'\n",
    "        self.dTrans = dTrans\n",
    "        self.setLang(lang)\n",
    "        \n",
    "    def setLang(self, lang):\n",
    "        self.lang = lang.lower()\n",
    "        assert self.lang in ['en', 'fr'], 'No support for \"{}\" language'.format(lang)\n",
    "        \n",
    "    def __call__(self, s):\n",
    "        return self.dTrans.get(self.lang, self.dTrans['en']).get(s, self.dTrans['en'].get(s, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFigureTrans = \\\n",
    "    dict(en=dict(Value='', Cv='CoefVar', Lcl='Min', Ucl='Max', Df='DoF'),\n",
    "         fr=dict(Value='', Cv='CoefVar', Lcl='Min', Ucl='Max', Df='DegLib'))\n",
    "\n",
    "figtr = Translator(DFigureTrans, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DStatisticTrans = \\\n",
    "    dict(en={ 'number of observations (n)': 'NObs',\n",
    "              'number of samples (k)': 'NSamp',\n",
    "              'effort (L or K or T)': 'Effort',\n",
    "              'encounter rate (n/L or n/K or n/T)': 'EncRate',\n",
    "              'left truncation distance': 'LeftTruncDist',\n",
    "              'right truncation distance (w)': 'RightTruncDist',\n",
    "              'total number of parameters (m)': 'TotNumPars',\n",
    "              'AIC value': 'AIC',\n",
    "              'chi-square test probability (distance set 1)': 'Chi2 P 1',\n",
    "              'chi-square test probability (distance set 2)': 'Chi2 P 2',\n",
    "              'chi-square test probability (distance set 3)': 'Chi2 P 3',\n",
    "              'f(0) or h(0)': 'f/h(0)',\n",
    "              'probability of detection (Pw)': 'PDetec',\n",
    "              'effective strip width (ESW) or effective detection radius (EDR)': 'EDR/ESW',\n",
    "              'AICc': 'AICc',\n",
    "              'BIC': 'BIC',\n",
    "              'Log likelihood': 'LogLhood',\n",
    "              'Kolmogorov-Smirnov test probability': 'KS P',\n",
    "              'Cramér-von Mises (uniform weighting) test probability': 'CvM Uw P',\n",
    "              'Cramér-von Mises (cosine weighting) test probability': 'CvM Cw P',\n",
    "              'key function type': 'KeyFn',\n",
    "              'adjustment series type': 'AdjSer',\n",
    "              'number of key function parameters (NKP)': 'NumKFnPars',\n",
    "              'number of adjustment term parameters (NAP)': 'NumASerPars',\n",
    "              'number of covariate parameters (NCP)': 'NumCovars',\n",
    "              'estimated value of A(1) adjustment term parameter': 'EstA(1)',\n",
    "              'estimated value of A(2) adjustment term parameter': 'EstA(2)',\n",
    "              'estimated value of A(3) adjustment term parameter': 'EstA(3)',\n",
    "              'estimated value of A(4) adjustment term parameter': 'EstA(4)',\n",
    "              'estimated value of A(5) adjustment term parameter': 'EstA(5)',\n",
    "              'estimated value of A(6) adjustment term parameter': 'EstA(6)',\n",
    "              'estimated value of A(7) adjustment term parameter': 'EstA(7)',\n",
    "              'estimated value of A(8) adjustment term parameter': 'EstA(8)',\n",
    "              'estimated value of A(9) adjustment term parameter': 'EstA(9)',\n",
    "              'estimated value of A(10) adjustment term parameter': 'EstA(10)',\n",
    "              'average cluster size': 'AvgClustSz',\n",
    "              'size-bias regression correlation (r)': 'SzBias RegCorr',\n",
    "              'p-value for correlation significance (r-p)': 'CorSignPVal',\n",
    "              'estimate of expected cluster size corrected for size bias': 'EstExpFixedCluSz',\n",
    "              'density of clusters (or animal density if non-clustered)': 'DensClu',\n",
    "              'density of animals': 'Density',\n",
    "              'number of animals, if survey area is specified': 'Number',\n",
    "              'bootstrap density of clusters': 'BootsDensClu',\n",
    "              'bootstrap density of animals': 'BootDensity',\n",
    "              'bootstrap number of animals': 'BootNumber' },\n",
    "         fr={ 'number of samples (k)': 'NEchant',\n",
    "              'encounter rate (n/L or n/K or n/T)': 'TxContact',\n",
    "              'left truncation distance': 'DistTroncGche',\n",
    "              'right truncation distance (w)': 'DistTroncDte',\n",
    "              'total number of parameters (m)': 'NbTotPars',\n",
    "              'Log likelihood': 'LogProba',\n",
    "              'key function type': 'FnClé',\n",
    "              'adjustment series type': 'SérAjust',\n",
    "              'number of key function parameters (NKP)': 'NbParsFnClé',\n",
    "              'number of adjustment term parameters (NAP)': 'NbParsSérAjust',\n",
    "              'number of covariate parameters (NCP)': 'NbCovars',\n",
    "              'average cluster size': 'TailMoyClust',\n",
    "              'size-bias regression correlation (r)': 'CorrReg BiaisTail',\n",
    "              'p-value for correlation significance (r-p)': 'PVal SignifCorr',\n",
    "              'estimate of expected cluster size corrected for size bias': 'TailCorrCluAttEst',\n",
    "              'density of animals': 'Densité',\n",
    "              'number of animals, if survey area is specified': 'Nombre',\n",
    "              'bootstrap density of clusters': 'BootsDensClu',\n",
    "              'bootstrap density of animals': 'DensitéBoot',\n",
    "              'bootstrap number of animals': 'NombreBoot' })\n",
    "\n",
    "statr = Translator(DStatisticTrans, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans = ads.MCDSEngine.statModCols().to_frame()\n",
    "dfStatModTrans.reset_index(drop=True, inplace=True)\n",
    "dfStatModTrans.rename(columns={ 0: 'Module', 1: 'Statistic', 2: 'Figure' }, inplace=True)\n",
    "for lang in ['en', 'fr']:\n",
    "    figtr.setLang(lang)\n",
    "    statr.setLang(lang)\n",
    "    dfStatModTrans[lang] = \\\n",
    "        dfStatModTrans.apply(lambda sRow: '{} {}'.format(figtr(sRow.Figure), statr(sRow.Statistic)).strip(),\n",
    "                             axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans.to_csv(tgtTransFileName, sep='\\t', index=False)\n",
    "tgtTransFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=ads.MCDSAnalysis.MIRunColumns,\n",
    "             data=dict(en=['ModKeyFn', 'ModAdjSer', 'ModChcCrit', 'ConfInterv', 'LeftTrunc', 'RightTrunc',\n",
    "                           'FitDistCuts', 'DiscrDistCuts', 'RunCode', 'RunTime', 'RunFolder'],\n",
    "                       fr=['FnCléMod', 'SérAjustMod', 'CritChxMod', 'IntervConf', 'TroncGauche', 'TroncDroite',\n",
    "                           'TranchDistFit', 'TranchDistDiscr', 'CodeExec', 'HeureExec', 'DossierExec']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfStatModTransExt = pd.read_csv(tgtTransFileName, sep='\\t')\n",
    "dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "dTrans = dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])[lang].to_dict()\n",
    "results.dfData.columns = [dTrans.get(col, col) for col in results.dfData.columns]\n",
    "results.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])[lang].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case class\n",
    "\n",
    "(no use actually : pd.DataFrame already does the job !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super-class for test cases\n",
    "class TestCase(object):\n",
    "    def __init__(self, **attrs):\n",
    "        if not hasattr(self.__class__, 'AttributeNames'):\n",
    "            self.__class__.AttributeNames = set(attrs.keys())\n",
    "        else:\n",
    "            assert set(attrs.keys()) == self.AttributeNames, \\\n",
    "                   'Some attribute name not in frozen set {{{}}}'.format(','.join(self.AttributeNames))\n",
    "        for attrName, AttrValue in attrs.items():\n",
    "            setattr(self, attrName, AttrValue)\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.__class__.__name__, ','.join('{}:{}'.format(k, v) for k, v in self.__dict__.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this super-class.\n",
    "class TCTest(TestCase):\n",
    "    pass\n",
    "\n",
    "tstTestCases = list()\n",
    "tstTestCases.append(TCTest(x=1, y='a')) # Define attributes\n",
    "tstTestCases.append(TCTest(x=2, y='b')) # Check attributes\n",
    "try:\n",
    "    tstTestCases.append(TCTest(x=2, z=None)) # Refuse new attributes\n",
    "    assert False, 'Error: New attributes should be refused'\n",
    "except AssertionError as exc:\n",
    "    print('Good refuse of new attributes:', exc)\n",
    "    \n",
    "[str(tc) for tc in tstTestCases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise au point décodage sorties de MCDS : fichier de stats\n",
    "\n",
    "TODO: Add french translation of variables / parameters names and descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nom et description des colonnes du tableau de stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-row-specs.txt'\n",
    "\n",
    "fStatRowSpecs = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statRowSpecLines = [line.rstrip('\\n') for line in fStatRowSpecs.readlines() if not line.startswith('#')]\n",
    "statRowSpecs =  [(statRowSpecLines[i].strip(), statRowSpecLines[i+1].strip()) \\\n",
    "                 for i in range(0, len(statRowSpecLines)-2, 3)]\n",
    "dfStatRowSpecs = pd.DataFrame(columns=['Name', 'Description'], data=statRowSpecs).set_index('Name')\n",
    "\n",
    "dfStatRowSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRowSpecs.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numéro et description des modules et statistiques associées\n",
    "\n",
    "(colonnes Module et Statistic du tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-mod-specs.txt'\n",
    "\n",
    "fStatModSpecs = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMaxAdjParams = 10\n",
    "\n",
    "statModSpecLines = [line.rstrip('\\n') for line in fStatModSpecs.readlines() if not line.startswith('#')]\n",
    "reModSpecNumName = re.compile('(.+) – (.+)')\n",
    "statModSpecs = list()\n",
    "moModule = None\n",
    "for line in statModSpecLines:\n",
    "    if not line:\n",
    "        continue\n",
    "    if moModule is None:\n",
    "        moModule = reModSpecNumName.match(line.strip())\n",
    "        continue\n",
    "    if line == ' ':\n",
    "        moModule = None\n",
    "        continue\n",
    "    moStatistic = reModSpecNumName.match(line.strip())\n",
    "    modNum, modDesc, statNum, statDescNotes = \\\n",
    "        moModule.group(1), moModule.group(2), moStatistic.group(1), moStatistic.group(2)\n",
    "    for i in range(len(statDescNotes)-1, -1, -1):\n",
    "        if not re.match('[\\d ,]', statDescNotes[i]):\n",
    "            statDesc = statDescNotes[:i+1]\n",
    "            statNotes = statDescNotes[i+1:].replace(' ', '')\n",
    "            break\n",
    "    modNum = int(modNum)\n",
    "    if statNum.startswith('101 '):\n",
    "        for num in range(nMaxAdjParams): # Assume no more than that ... a bit hacky !\n",
    "            statModSpecs.append((modNum, modDesc, 101+num, # Make statDesc unique for later indexing\n",
    "                                 statDesc.replace('each', 'A({})'.format(num+1)), statNotes))\n",
    "    else:\n",
    "        statNum = int(statNum)\n",
    "        if modNum == 2 and statNum == 3: # Actually, there are 0 or 3 of these ...\n",
    "            for num in range(3):\n",
    "                statModSpecs.append((modNum, modDesc, num+201,\n",
    "                                     # Change statNum & Make statDesc unique for later indexing\n",
    "                                     statDesc+' (distance set {})'.format(num+1), statNotes))\n",
    "        else:\n",
    "            statModSpecs.append((modNum, modDesc, statNum, statDesc, statNotes))\n",
    "dfStatModSpecs = pd.DataFrame(columns=['modNum', 'modDesc', 'statNum', 'statDesc', 'statNotes'],\n",
    "                              data=statModSpecs).set_index(['modNum', 'statNum'])\n",
    "\n",
    "dfStatModSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "dfStatModSpecs.modDesc.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Notes sur les statistiques des modules\n",
    "\n",
    "(infos supplémentaire indiquant comment utiliser ou pas les 5 dernières colonnes Value, Cv, Lcl, Ucl, Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-mod-notes.txt'\n",
    "\n",
    "fStatModNotes = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statModNoteLines = [line.rstrip('\\n') for line in fStatModNotes.readlines() if not line.startswith('#')]\n",
    "statModNotes =  [(int(line[:2]), line[2:].strip()) for line in statModNoteLines if line]\n",
    "\n",
    "dfStatModNotes = pd.DataFrame(data=statModNotes, columns=['Note', 'Text']).set_index('Note')\n",
    "\n",
    "dfStatModNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lecture du tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = mcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.statsFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRows = pd.read_csv(eng.statsFileName, sep=' +', engine='python', names=dfStatRowSpecs.index)\n",
    "dfStatRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Décodage du tableau\n",
    "\n",
    "Attention: On suppose 1 seule strate '0' (Stratum), 1 seul échantillon '0' (Sample) et 1 seul estimateur '1' (Estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Suppression des colonnes Stratum, Sample et Estimator\n",
    "\n",
    "(puisqu'on se limite ici aux cas où il n'y a qu'1 de chaque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRows.drop(columns=['Stratum', 'Sample', 'Estimator'], inplace=True)\n",
    "dfStatRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Nettoyage des données sans objets\n",
    "\n",
    "(selon les notes descriptives des statistiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Empilage des \"chiffres\" (Figures) Value, Cv, Lcl, Ucl, Df pour chaque statistique / module\n",
    "dfStats = dfStatRows.set_index(['Module', 'Statistic'], append=True).stack() \\\n",
    "                    .reset_index().rename(columns={'level_0': 'id', 'level_3': 'Figure', 0: 'Value'})\n",
    "dfStats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fix multiple Module=2 & Statistic=3 rows (before joining with self.DfStatModSpecs)\n",
    "newStatNum = 200\n",
    "for lbl, sRow in dfStats[(dfStats.Module == 2) & (dfStats.Statistic == 3)].iterrows():\n",
    "    if dfStats.loc[lbl, 'Figure'] == 'Value':\n",
    "        newStatNum += 1\n",
    "    dfStats.loc[lbl, 'Statistic'] = newStatNum\n",
    "dfStats[(dfStats.Module == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des colonnes de description/nommage des modules et statistiques\n",
    "dfStats = dfStats.join(dfStatModSpecs, on=['Module', 'Statistic'])\n",
    "dfStats.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfStats[(dfStats.Module == 2) & (dfStats.Statistic > 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que les chiffres sans objet le sont vraiment (tous à 0.0 ?)\n",
    "# Attention: Il doit y avoir un bug dans MCDS avec Module 2 / Statistic 10x : certains Cv ne sont pas nuls ...\n",
    "sKeepOnlyValueFig = ~dfStats.statNotes.str.contains('1')\n",
    "sFigs2Drop = (dfStats.Figure != 'Value') & sKeepOnlyValueFig\n",
    "assert ~dfStats[sFigs2Drop & ((dfStats.Module != 2) | (dfStats.Statistic < 100))].Value.any(), \\\n",
    "       'Attention: Des chiffres supposés \"sans objet\" on des valeurs non nulles !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nde vérif. visuelle\n",
    "dfStats[sFigs2Drop & dfStats.Value != 0].sort_values(by='Value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes / chiffres sans objet.\n",
    "dfStats.drop(dfStats[sFigs2Drop].index, inplace=True)\n",
    "dfStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats = dfStats.reindex(columns=['modDesc', 'statDesc', 'Figure', 'Value'])\n",
    "dfStats.set_index(['modDesc', 'statDesc', 'Figure'], inplace=True)\n",
    "dfStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.T.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance cut specs for MCDS\n",
    "\n",
    "* Mise au point\n",
    "* tests unitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceCutSpecs(minDist, maxDist, distCuts):\n",
    "    \n",
    "    distCutSpecs = ''\n",
    "        \n",
    "    #distCuts = params['distCuts']\n",
    "    if distCuts is not None:\n",
    "\n",
    "        if isinstance(distCuts, list) and minDist is not None and maxDist is not None:\n",
    "            distCutSpecs += ' /Int=' + ','.join(str(d) for d in [minDist] + distCuts + [maxDist])\n",
    "        elif isinstance(distCuts, int):\n",
    "            distCutSpecs += ' /NClass=' + str(distCuts)\n",
    "\n",
    "    if not isinstance(distCuts, list): # None or int\n",
    "\n",
    "        #minDist = params['minDist']\n",
    "        if minDist is not None:\n",
    "            distCutSpecs += ' /Left=' + str(minDist)\n",
    "\n",
    "        #maxDist = params['maxDist']\n",
    "        if maxDist is not None:\n",
    "            distCutSpecs += ' /Width=' + str(maxDist)\n",
    "            \n",
    "    return distCutSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=None) == ''\n",
    "\n",
    "assert distanceCutSpecs(minDist=5, maxDist=None, distCuts=None) == ' /Left=5'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=100, distCuts=None) == ' /Width=100'\n",
    "assert distanceCutSpecs(minDist=25.2, maxDist=100.5, distCuts=None) == ' /Left=25.2 /Width=100.5'\n",
    "\n",
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=3) == ' /NClass=3'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=300, distCuts=8) == ' /NClass=8 /Width=300'\n",
    "assert distanceCutSpecs(minDist=20, maxDist=None, distCuts=8) == ' /NClass=8 /Left=20'\n",
    "assert distanceCutSpecs(minDist=20, maxDist=300, distCuts=8) == ' /NClass=8 /Left=20 /Width=300'\n",
    "\n",
    "assert distanceCutSpecs(minDist=20, maxDist=300, distCuts=[100, 200, 230, 290]) == ' /Int=20,100,200,230,290,300'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined\n",
    "assert distanceCutSpecs(minDist=0, maxDist=None, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined\n",
    "assert distanceCutSpecs(minDist=None, maxDist=4, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data tools development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addAbsenceSightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInSightings = dfObsIndiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectCol = 'Point'\n",
    "taxonCol = 'Espece'\n",
    "sampleCols = ['Passage', 'Adulte', 'Duree']\n",
    "\n",
    "# The set of expected taxa ... of which we'll look for abscence on every location\n",
    "expectedTaxa = list(dfObsIndiv[taxonCol].unique())\n",
    "', '.join(expectedTaxa), len(expectedTaxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"abscence\" sightings to field data collected on transects for a given sample\n",
    "#def addAbsenceSightings(dfInSightings, transectCol, taxonCol, expectedTaxa, sampleCols):\n",
    "    \n",
    "def absenceSightings(taxonCol, taxon, dAbscSightTmpl):\n",
    "    dAbscSight = dAbscSightTmpl.copy()\n",
    "    dAbscSight[taxonCol] = taxon\n",
    "    return dAbscSight\n",
    "\n",
    "assert not dfInSightings.empty, 'Error : Empty sightings data to complete !'\n",
    "\n",
    "ldfAbscSightings = list()\n",
    "\n",
    "# Use 1st sightings of the sample to build the absence sightings prototype\n",
    "# (all null columns except for the sample identification ones)\n",
    "dAbscSightTmpl = dfInSightings.iloc[0].to_dict()\n",
    "dAbscSightTmpl.update({ k: None for k in dAbscSightTmpl.keys() if k not in sampleCols })\n",
    "\n",
    "# For each transect\n",
    "for transect in dfInSightings[transectCol].unique():\n",
    "\n",
    "    # Update absence sightings template with transect id\n",
    "    dAbscSightTmpl.update({ transectCol: transect })\n",
    "    \n",
    "    # Generate the absence sightings from it : 1 per lacking taxon\n",
    "    lackingTaxa = set(expectedTaxa) - set(dfInSightings.loc[dfInSightings[transectCol] == transect, taxonCol].unique())\n",
    "    dfAbscSights = pd.DataFrame([absenceSightings(taxonCol, txn, dAbscSightTmpl) for txn in lackingTaxa])\n",
    "    \n",
    "    ldfAbscSightings.append(dfAbscSights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all data frames into one.\n",
    "dfOutSightings = pd.concat([dfInSightings] + ldfAbscSightings)\n",
    "\n",
    "# Reset index (for unique labels).\n",
    "dfOutSightings.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfOutSightings), len(dfInSightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance troncations : auto-generation of variants\n",
    "\n",
    "(at least a try ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for playing : Individualised ones ...\n",
    "\n",
    "(output from \"4. AutoDS data tools\" above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.groupby('Espece').size().sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc = dfObsIndiv[dfObsIndiv.Espece == 'Merle noir'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogramme uniforme\n",
    "_ = dfObsIndSpc.distMem.hist(figsize=(16, 3), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.distributions.empirical_distribution as sted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecdf = sted.ECDF(dfObsIndSpc.distMem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sEcdf = pd.Series({ x : ecdf(x) for x in dfObsIndSpc.distMem.unique() }).sort_index()\n",
    "_ = sEcdf.plot(figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles : 2.5, 5 et 10%, left and right sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqLims = np.array([0.025, 0.05, 0.1, 0.9, 0.95, 0.975])\n",
    "aqLims * len(dfObsIndSpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(a=dfObsIndSpc.distMem, q=aqLims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc[dfObsIndSpc.distMem <= 11.61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force combination algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lParams = list() # of dict(ltr=<left trunc dist or None>, rtr=<right trunc dist or None>, nc=<nb of cuts>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqtlLims = np.array([1.25, 2.5, 3.75, 5, 7.5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sLeftTruncs = pd.Series(index=aqtlLims, data=np.percentile(a=dfObsIndSpc.distMem, q=aqtlLims))\n",
    "for leftPct, leftTrunc in sLeftTruncs.items():\n",
    "    nRetSights = len(dfObsIndSpc[dfObsIndSpc.distMem >= leftTrunc])\n",
    "    sqrNRetSights = math.sqrt(nRetSights)\n",
    "    for nCuts in [2*sqrNRetSights/3, sqrNRetSights, 3*sqrNRetSights/2]:\n",
    "        lParams.append(dict(ltr=leftTrunc, rtr=None, nc=round(nCuts), nrs=nRetSights, pct=100-leftPct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sRightTruncs = pd.Series(index=100-aqtlLims, data=np.percentile(a=dfObsIndSpc.distMem, q=100-aqtlLims)).sort_index()\n",
    "for rightPct, rightTrunc in sRightTruncs.items():\n",
    "    nRetSights = len(dfObsIndSpc[dfObsIndSpc.distMem <= rightTrunc])\n",
    "    sqrNRetSights = math.sqrt(nRetSights)\n",
    "    for nCuts in [2*sqrNRetSights/3, sqrNRetSights, 3*sqrNRetSights/2]:\n",
    "        lParams.append(dict(ltr=None, rtr=rightTrunc, nc=round(nCuts), nrs=nRetSights, pct=rightPct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... etc ... but, why not use an optimisation engine then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mcds = ads.MCDSEngine(workDir=os.path.join('ACDC', '2019-nat-opt'),\n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDecFields = ['Effort', 'Distance']\n",
    "\n",
    "dAreaInfo = dict(Zone='ACDC', Surface=2400) # ha\n",
    "dfObsIndSpc = ads.addSurveyAreaInfo(dfObsIndSpc, dAreaInfo=dAreaInfo)\n",
    "\n",
    "dfObsIndSpc.rename(columns=dict(distMem='Distance'), inplace=True)\n",
    "dfObsIndSpc.sort_values(by='Point', inplace=True)\n",
    "\n",
    "dataSet = ads.DataSet(dfObsIndSpc, decimalFields=sampleDecFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSamp = dfObsIndSpc.iloc[0]\n",
    "abrvSpe = ''.join(word[:4].title() for word in sSamp['Espece'].split(' '))\n",
    "sampAbbrev = '{}-{}-{}-{}'.format(abrvSpe, sSamp.Passage.replace('+', ''),\n",
    "                                  sSamp.Adulte.replace('+', ''), sSamp['Duree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPreEstimCrit = 'AICC'\n",
    "KPreCVInterval = 95\n",
    "\n",
    "def dsAnalyser3(aParams, mcdsEngine, dataSet, sampAbbrev, keyFn, adjSer):\n",
    "\n",
    "    minDist = aParams[0]\n",
    "    maxDist = aParams[1]\n",
    "    fitDistCuts = round(aParams[2])\n",
    "    print(minDist, maxDist, fitDistCuts)\n",
    "    \n",
    "    modAbbrev = keyFn[:3].lower() + '-' + adjSer[:3].lower()\n",
    "\n",
    "    analysis = ads.MCDSAnalysis(engine=mcdsEngine, dataSet=dataSet,\n",
    "                                name=sampAbbrev + '-' + modAbbrev, logData=False,\n",
    "                                estimKeyFn=keyFn, estimAdjustFn=adjSer,\n",
    "                                estimCriterion=KPreEstimCrit, cvInterval=KPreCVInterval,\n",
    "                                minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts)\n",
    "    \n",
    "    sResult = analysis.run()\n",
    "\n",
    "    aic = sResult[('detection probability', 'AIC value', 'Value')]\n",
    "    \n",
    "    return aic\n",
    "\n",
    "def dsAnalyser2(aParams, mcdsEngine, dataSet, sampAbbrev, keyFn, adjSer, fitDistCuts):\n",
    "\n",
    "    minDist = aParams[0]\n",
    "    maxDist = aParams[1]\n",
    "    print(minDist, maxDist)\n",
    "    \n",
    "    modAbbrev = keyFn[:3].lower() + '-' + adjSer[:3].lower()\n",
    "\n",
    "    analysis = ads.MCDSAnalysis(engine=mcdsEngine, dataSet=dataSet,\n",
    "                                name=sampAbbrev + '-' + modAbbrev, logData=False,\n",
    "                                estimKeyFn=keyFn, estimAdjustFn=adjSer,\n",
    "                                estimCriterion=KPreEstimCrit, cvInterval=KPreCVInterval,\n",
    "                                minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts)\n",
    "    \n",
    "    sResult = analysis.run()\n",
    "    #print(sResult.to_dict())\n",
    "\n",
    "    #return sResult[('detection probability', 'AIC value', 'Value')]\n",
    "    return sResult[('detection probability', 'AICc', 'Value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjSer = 'COSINE'\n",
    "keyFn = 'HNORMAL'\n",
    "#keyFn = 'HAZARD'\n",
    "#keyFn = 'UNIFORM'\n",
    "#keyFn = 'NEXPON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juste une analyse pour tester la fonction à minimiser (AIC)\n",
    "#              minDist, maxDist, fitDistCuts\n",
    "aParams = np.array([0, 250, round(math.sqrt(len(dataSet.dfData)))])\n",
    "dsAnalyser3(aParams, mcds, dataSet, sampAbbrev, keyFn, adjSer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et maintenant, on lance l'optimisation.\n",
    "#              minDist, maxDist, fitDistCuts\n",
    "#maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[20, 80])\n",
    "#maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[40, 60])\n",
    "maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[49, 51])\n",
    "sqrNRetSights = math.sqrt(len(dfObsIndSpc))\n",
    "minFitDistCuts, maxFitDistCuts = round(2*sqrNSights/3), round(3*sqrNRetSights/2)\n",
    "paramBounds = [(0, maxMinDist), (minMaxDist, dfObsIndSpc.Distance.max()), (minFitDistCuts, maxFitDistCuts)]\n",
    "paramBounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitDistCuts = 12\n",
    "dOptRes['shgo'] = optimize.shgo(func=dsAnalyser2, bounds=paramBounds[:2], iters=2,\n",
    "                                args=(mcds, dataSet, sampAbbrev, keyFn, adjSer, fitDistCuts))\n",
    "dOptRes['shgo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['shgo'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dOptRes['shgo'] = optimize.shgo(func=dsAnalyser3, bounds=paramBounds, iters=2,\n",
    "                                args=(mcds, dataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['shgo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['da'] = optimize.dual_annealing(func=dsAnalyser3, bounds=paramBounds,\n",
    "                                        args=(mcds, dataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['da']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['de'] = optimize.differential_evolution(func=dsAnalyser3, bounds=paramBounds,\n",
    "                                                args=(mcds, dataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['bh'] = optimize.basinhopping(func=dsAnalyser3, x0=[(mx+mn)/2 for mx, mn in paramBounds], stepsize=2,\n",
    "                                      minimizer_kwargs=dict(args=(mcds, dataSet, sampAbbrev, keyFn, adjSer)))\n",
    "dOptRes['bh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tuple from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a=1, b=[3, 2], c='xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = ntuple('NT', d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = NT(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending series to series ... index order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s.append(pd.Series(index=[('A', 'b'), ('A', 'a'), ('B', 'c')], data=[1, 2, 3], name=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending series to DataFrame ... columns order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "#df = df.append(s, ignore_index=False) # => df.columns pas MultiIndex !\n",
    "df = df.append([s], ignore_index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('A', 'c'), ('B', 'b'), ('B', 'a')], data=[4, 5, 6], name=1)  # Mêmes colonnes : append ne retrie pas\n",
    "#s = pd.Series(index=[('A', 'a'), ('A', 'b'), ('B', 'c')], data=[4, 5, 6], name=1)  # Nouvelle colonne : append retrie\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('A', 'a'), ('B', 'c')], data=[7, 8])\n",
    "df = df.append(s, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(index=[], data=[])\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('C', 'd')], data=[9])\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('d',)], data=[10])\n",
    "df = df.append(s, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "df = pd.concat([df, s], axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('B', 'b'), ('B', 'a'), ('A', 'c')], data=[4, 5, 6], name=1) # Mêmes colonnes : concat ne retrie pas\n",
    "#s = pd.Series(index=[('A', 'a'), ('A', 'b'), ('B', 'c')], data=[4, 5, 6], name=1) # Nouvelle colonne : concat retrie\n",
    "df = pd.concat([df, s], axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Restore desired columns\n",
    "\n",
    "* desired order,\n",
    "* desired list of columns : new ones, and / or ignored ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new A/b, D/a and remove B/c and C/d\n",
    "i = pd.MultiIndex.from_tuples([('A', 'c'), ('A', 'b'), ('A', 'a'), ('B', 'b'), ('B', 'a'), ('D', 'a')])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep added columns (with no data inside)\n",
    "df2 = df.reindex(i, axis='columns')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove added columns (with no data inside)\n",
    "df2 .dropna(how='all', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code attic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-categorical sightings set into an equivalent mono-categorical sightings set,\n",
    "# that is where no sightings has more that one category with positive count (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 categorical count columns nMales and nFemales\n",
    "#     * in the input set, you may have 1 sightings with nMales = 5 and nFemales = 2\n",
    "#     * in the output set, this sightings have been separated in 2 distinct ones (all other properties left untouched) :\n",
    "#       the 1st with nMales = 5 and nFemales = 0, the 2nd with nMales = 0 and nFemales = 2.\n",
    "\n",
    "# A slower version or ads.separateMultiCategoricalCounts :\n",
    "#  from 9.5s to 0.1s with countColumns = [nMalAd1,nAutAd10,nJuv10,nDetTot10,nMalAd5,nAutAd5,nJuv5,nDetTot5,nTotAd5,nTotAd10]\n",
    "#  on the \"ACDC 2019 Naturalist\" multi-categorical data set (~4000 rows)\n",
    "def separateMultiCategoricalCounts_slow_version(dfInSightings, countColumns):\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "        \n",
    "        # [a little optimisation ?] If this is already a mono-categorical sightings, simply append it as is.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        sCounts = sCounts[sCounts > 0]\n",
    "        if len(sCounts) == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # If it is a multi-categorical sightings, we need to split it down.\n",
    "        for col in sCounts.index:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[countColumns] = 0\n",
    "            sOutSight[col] = sInSight[col]\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=np.arange(len(outSightings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsMonoCat_slow = separateMultiCategoricalCounts_slow_version(dfObs, countCols)\n",
    "len(dfObsMonoCat_slow), dfObsMonoCat_slow[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-individual mono-categorical sightings set into an equivalent mono-individual mono-categorical sightings set\n",
    "# that is where no sightings has more that one individual per category (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 mono-categorical count columns nMales and nFemales\n",
    "#     In input set, you may have 1 sightings with nMales = 3 and nFemales = 0 (but none with nMales and nFemales > 0)\n",
    "#     In out set, no : this sightings have been separated in 3 distinct ones (all other properties left untouched) :\n",
    "#                      all with nMales = 1 and nFemales = 0.\n",
    "\n",
    "# A slower version or ads.individualiseMonoCategoricalCounts :\n",
    "#  from 15.5s to 0.06s with countColumns = [nMalAd1,nAutAd10,nJuv10,nDetTot10,nMalAd5,nAutAd5,nJuv5,nDetTot5,nTotAd5,nTotAd10]\n",
    "#  on the \"ACDC 2019 Naturalist\" mono-categorical data set (~20000 rows)\n",
    "def individualiseMonoCategoricalCounts_slow(dfInSightings, countColumns):\n",
    "\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "\n",
    "        # [a little check] Multi-categorical sightings not supported here.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        sCounts = sCounts[sCounts > 0]\n",
    "        assert len(sCounts) == 1, 'Error: Multi-categorical sightings not supported ' + str(lbl, sInSight)\n",
    "        \n",
    "        # Get the positive count column and its value\n",
    "        posCol = sCounts.index[0]\n",
    "        count = sCounts[posCol]\n",
    "\n",
    "        # [a little optimisation ?] If this is a mono-individual sightings, simply append it as is.\n",
    "        if count == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # If it is a multi-individual sightings, we need to split it down.\n",
    "        while count > 0:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[posCol] = 1\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "            count -= 1\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=np.arange(len(outSightings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndiv_slow = individualiseMonoCategoricalCounts_slow(dfObsMonoCat_slow, countCols)\n",
    "len(dfObsIndiv_slow), dfObsIndiv_slow[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-individual multi-categorical sightings set into an equivalent mono-individual multi-categorical sightings set\n",
    "# that is where no sightings has more that one individual per category (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 categorical count columns nMales and nFemales\n",
    "#     In input set, you may have 1 sightings with nMales = 5 and nFemales = 2\n",
    "#     In out set, no : this sightings have been separated in 5 distinct ones (all other properties left untouched) :\n",
    "#                      the 2 1st ones with nMales = 1 and nFemales = 1, the last 3 ones with nMales = 1 and nFemales = 0.\n",
    "\n",
    "# Finally, of no use : simply chain ads.separateMultiCategoricalCounts and ads.individualiseMonoCategoricalCounts\n",
    "# And from far much slower !\n",
    "def individualiseMultiCategoricalCounts(dfInSightings, countColumns):\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "\n",
    "        # [a little optimisation ?] If this is a mono-individual sightings, simply append it as is.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        if sCounts.max() == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # If it is a multi-individual sightings, we need to split it down.\n",
    "        sCounts = sCounts.copy()\n",
    "\n",
    "        while sCounts.max() > 0:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[countColumns] = sCounts.apply(lambda n: 1 if n > 0 else 0)\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "            sCounts = sCounts.apply(lambda n: n-1 if n > 0 else 0)\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=list(range(len(outSightings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"abscence\" sightings to field data collected on transects for a given sample\n",
    "# Warning: A special version for an all-taxon data set\n",
    "# * dfInSights : input data table\n",
    "# * transectCol : the name of the transect id column\n",
    "# * taxonSampleCol : the name of the taxon id column\n",
    "# * otherSampleCols : the names of the other sample id columns (taxon id not included)\n",
    "# * expectedTaxa : the expected taxon ids : absence sightings are there to make sure\n",
    "#                  all of the taxa are found at least once in the output data table\n",
    "def addAbsenceSightings(dfInSights, transectCol, taxonSampleCol, otherSampleCols, expectedTaxa):\n",
    "    \n",
    "    def absenceSightings(taxonCol, taxon, dAbscSightTmpl):\n",
    "        dAbscSight = dAbscSightTmpl.copy()\n",
    "        dAbscSight[taxonCol] = taxon\n",
    "        return dAbscSight\n",
    "\n",
    "    assert not dfInSights.empty, 'Error : Empty sightings data to complete !'\n",
    "\n",
    "    ldfAbscSights = list()\n",
    "\n",
    "    # Use 1st sightings of the sample to build the absence sightings prototype\n",
    "    # (all null columns except for the sample identification ones)\n",
    "    dAbscSightTmpl = dfInSights.iloc[0].to_dict()\n",
    "    dAbscSightTmpl.update({ k: None for k in dAbscSightTmpl.keys() if k not in otherSampleCols })\n",
    "\n",
    "    # For each transect\n",
    "    for transect in dfInSights[transectCol].unique():\n",
    "\n",
    "        # Update absence sightings template with transect id\n",
    "        dAbscSightTmpl.update({ transectCol: transect })\n",
    "\n",
    "        # Generate the absence sightings from it : 1 per lacking taxon\n",
    "        lackingTaxa = \\\n",
    "          set(expectedTaxa) - set(dfInSights.loc[dfInSights[transectCol] == transect, taxonSampleCol].unique())\n",
    "        dfAbscSights = pd.DataFrame([absenceSightings(taxonSampleCol, txn, dAbscSightTmpl) for txn in lackingTaxa])\n",
    "\n",
    "        # Save the data frame for later\n",
    "        ldfAbscSights.append(dfAbscSights)\n",
    "\n",
    "    # Concat all absence data frames into one.\n",
    "    dfOutSights = pd.concat([dfInSights] + ldfAbscSights)\n",
    "\n",
    "    # Reset index (for unique labels).\n",
    "    dfOutSights.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Done.\n",
    "    return dfOutSights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests unitaires de addAbsenceSightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transect, taxon and sample columns\n",
    "transectCol = 'Point'\n",
    "taxonCol = 'Espece'\n",
    "sampleCols = ['Passage', 'Adulte', 'Duree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of expected taxa ... of which we'll look for abscence on every location\n",
    "expectedTaxa = list(dfObsIndiv[taxonCol].unique())\n",
    "\n",
    "assert len(expectedTaxa) == 58\n",
    "\n",
    "', '.join(expectedTaxa), len(expectedTaxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1 random sample\n",
    "passage = 'a'\n",
    "adulte = 'm'\n",
    "duree = '10'\n",
    "dfObsIndivSmpl = dfObsIndiv[(dfObsIndiv.Passage == passage) & (dfObsIndiv.Adulte == adulte) & (dfObsIndiv.Duree == duree)]\n",
    "\n",
    "assert len(dfObsIndivSmpl) == 322 and dfObsIndivSmpl[transectCol].nunique() == 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, transectCol, taxonCol, expectedTaxa, sampleCols)\n",
    "len(dfObsIndivAbscSmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for no change in sample columns\n",
    "assert list(dfObsIndivAbscSmpl.columns) == list(dfObsIndivSmpl.columns)\n",
    "\n",
    "# Check for number of added rows\n",
    "assert len(dfObsIndivAbscSmpl) == 1333\n",
    "\n",
    "# Check for no change in number of transect and taxa\n",
    "assert dfObsIndivAbscSmpl[transectCol].nunique() == 21 and dfObsIndivAbscSmpl[taxonCol].nunique() == 58\n",
    "\n",
    "# Check for no change in sample identification\n",
    "assert list(dfObsIndivAbscSmpl.Passage.unique()) == [passage]\n",
    "assert list(dfObsIndivAbscSmpl.Adulte.unique()) == [adulte]\n",
    "assert list(dfObsIndivAbscSmpl.Duree.unique()) == [duree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfObsIndivAbscSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem']).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Performance test\n",
    "print('Passage  Adulte Duree NbDonnees')\n",
    "\n",
    "for passage in ['a', 'b', 'a+b']: \n",
    "    \n",
    "    for adulte in ['m', 'a', 'm+a']:\n",
    "    \n",
    "        for duree in ['5', '10']:\n",
    "            \n",
    "            passages = passage.split('+')\n",
    "            adultes = adulte.split('+')\n",
    "            dfObsIndivSmpl = dfObsIndiv[dfObsIndiv.Passage.isin(passages) & dfObsIndiv.Adulte.isin(adultes) \\\n",
    "                                        & (dfObsIndiv.Duree == duree)]\n",
    "            \n",
    "            dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, transectCol, taxonCol, expectedTaxa, sampleCols)\n",
    "            \n",
    "            print(passage, adulte, duree, ':', len(dfObsIndivAbscSmpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
