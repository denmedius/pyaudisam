{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Auto table of contents -->\n",
    "<h1 class='tocIgnore'>AutoDS : Development and unit tests</h1>\n",
    "<p>(module <b>autods</b>: python interface to MCDS.exe)</p>\n",
    "<div style=\"overflow-y: auto\">\n",
    "  <h2 class='tocIgnore'>Table of contents</h2>\n",
    "  <div id=\"toc\"></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pathlib as pl\n",
    "\n",
    "import re\n",
    "\n",
    "import concurrent.futures as cofu\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate Warnings as Exception\n",
    "#import warnings\n",
    "#warnings.filterwarnings('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short identification string for a sample.\n",
    "def sampleAbbrev(sSample):\n",
    "    \n",
    "    abrvSpe = ''.join(word[:4].title() for word in sSample['Espèce'].split(' ')[:2])\n",
    "    \n",
    "    sampAbbrev = '{}-{}-{}-{}'.format(abrvSpe, sSample.Passage.replace('+', ''),\n",
    "                                      sSample.Adulte.replace('+', ''), sSample['Durée'])\n",
    "    \n",
    "    return sampAbbrev\n",
    "\n",
    "# Short identification string for an analysis.\n",
    "def analysisAbbrev(sAnlys):\n",
    "    \n",
    "    # Sample abbreviation\n",
    "    abbrevs = [sampleAbbrev(sAnlys)]\n",
    "\n",
    "    # Model + Parameters abbreviation\n",
    "    abbrevs += [sAnlys['FonctionClé'][:3].lower(), sAnlys['SérieAjust'][:3].lower()]\n",
    "    dTroncAbrv = { 'l': 'TrGche' if 'TrGche' in sAnlys.index else 'TroncGche',\n",
    "                   'r': 'TrDrte' if 'TrDrte' in sAnlys.index else 'TroncDrte',\n",
    "                   'm': 'NbTrches' if 'NbTrches' in sAnlys.index else 'NbTrModel'\n",
    "                                   if 'NbTrModel' in sAnlys.index else  'NbTrchMod',\n",
    "                   'd': 'NbTrDiscr' }\n",
    "    for abrv, name in dTroncAbrv.items():\n",
    "        if name in sAnlys.index and not pd.isnull(sAnlys[name]):\n",
    "            abbrevs.append('{}{}'.format(abrv, sAnlys[name][0].lower() if isinstance(sAnlys[name], str)\n",
    "                                               else int(sAnlys[name])))\n",
    "   \n",
    "    return '-'.join(abbrevs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. MCDS.exe detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autods as ads\n",
    "\n",
    "ads.runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration.\n",
    "ads.log.configure(handlers=[sys.stdout, 'tmp/unintst.log'], verbose=True, reset=True)\n",
    "\n",
    "ads.logger('matplotlib', level=ads.WARNING, reset=True)\n",
    "\n",
    "ads.logger('ads', level=ads.INFO, reset=True)\n",
    "ads.logger('ads.dat', level=ads.INFO, reset=True)\n",
    "ads.logger('ads.eng', level=ads.INFO2, reset=True)\n",
    "ads.logger('ads.opn', level=ads.INFO1, reset=True)\n",
    "ads.logger('ads.opr', level=ads.INFO1, reset=True)\n",
    "ads.logger('ads.anr', level=ads.INFO1, reset=True)\n",
    "\n",
    "logger = ads.logger('unintst', level=ads.DEBUG, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HTML('<a href=\"http://localhost:8888/notebooks/perso/autods/unintests.ipynb#3.-MCDSAnalyser-%3A-Ex%C3%A9cution-multi-analyses-avec-de-vraies-donn%C3%A9es\">TI MCDSAnalyser</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DataSet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish preparing import data set\n",
    "dfPapAlaArv = pd.read_excel('refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods')\n",
    "\n",
    "dfPapAlaArv.to_csv('tmp/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv', sep='\\t', index=False)\n",
    "dfPapAlaArv.to_excel('tmp/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls', index=False)  # Need for deprecated module xlwt !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet from multiple sources from various formats (same columns)\n",
    "# => ctor, _csv2df, _fromDataFrame, _fromDataFile, _addComputedColumns, addColumns, renameColumns\n",
    "sources = ['refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',   # Need for module odfpy\n",
    "           'refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',   # Need for module openpyxl (or xlrd)\n",
    "           'tmp/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xls',  # No need for module xlwt (openpyxl seems to just do it)\n",
    "           'tmp/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.csv', dfPapAlaArv]\n",
    "\n",
    "def male2bool(s):\n",
    "    return False if pd.isnull(s.MALE) or s.MALE.lower() != 'oui' else True\n",
    "\n",
    "ds = ads.DataSet(sources, importDecFields=['EFFORT', 'DISTANCE', 'NOMBRE'],\n",
    "                 dRenameCols={'NOMBRE': 'INDIVIDUS'}, dComputeCols={'MALE': male2bool},\n",
    "                 sheet='Sheet1', skipRows=None, separator='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => ctor, _csv2df, _fromDataFrame, _fromDataFile, __len__, columns, empty\n",
    "assert not ds.empty\n",
    "\n",
    "assert len(ds) == len(dfPapAlaArv) * len(sources)\n",
    "\n",
    "assert sorted(ds.columns) == sorted(['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',\n",
    "                                     'OBSERVATEUR', 'PASSAGE', 'INDIVIDUS', 'EFFORT'])\n",
    "\n",
    "dTypes = {'ZONE': 'object', 'HA': 'int', 'POINT': 'int', 'ESPECE': 'object',\n",
    "          'DISTANCE': 'float', 'MALE': 'bool', 'DATE': 'object', 'OBSERVATEUR': 'object',\n",
    "          'PASSAGE': 'object', 'INDIVIDUS': 'float', 'EFFORT': 'int'}\n",
    "assert all(typ.name.startswith(dTypes[col]) for col, typ in ds.dfData.dtypes.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => dfData\n",
    "ds.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => dfSubData, __len__, columns\n",
    "df = ds.dfSubData(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'])\n",
    "\n",
    "assert len(df) == len(dfPapAlaArv) * len(sources)\n",
    "assert df.columns.to_list() == ['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => dfSubData, __len__, columns\n",
    "df = ds.dfSubData(columns=['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT'], index=range(1, 300, 3))\n",
    "\n",
    "assert len(df) == 100\n",
    "assert df.columns.to_list() == ['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']\n",
    "assert df.index.to_list() == list(range(1, 300, 3))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctor, _csv2df, _fromDataFrame, _fromDataFile, dfData\n",
    "assert ds.dfData.MALE.value_counts()[True] == ds.dfData.INDIVIDUS.sum() == dfPapAlaArv.NOMBRE.sum() * len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => dropColumns, columns, __len__\n",
    "ds.dropColumns(['ZONE', 'HA', 'OBSERVATEUR'])\n",
    "\n",
    "assert len(ds) == len(dfPapAlaArv) * len(sources)\n",
    "assert ds.columns.to_list() == ['POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE', 'PASSAGE', 'INDIVIDUS', 'EFFORT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => dropRows, dfData, __len__\n",
    "ds.dropRows(ds.dfData.DISTANCE.isnull())\n",
    "\n",
    "assert len(ds) == len(dfPapAlaArv[dfPapAlaArv.DISTANCE.notnull()]) * len(sources)\n",
    "assert ds.dfData.MALE.value_counts()[True] == ds.dfData.INDIVIDUS.sum() == dfPapAlaArv.NOMBRE.sum() * len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# => toExcel, toOpenDoc, toPickle, compareDataFrames\n",
    "closenessThreshold = 15  # => max relative delta = 1e-15\n",
    "subsetCols = ['POINT', 'ESPECE', 'DISTANCE', 'INDIVIDUS', 'EFFORT']\n",
    "filePathName = pl.Path('tmp') / 'dataset-uni.ods'\n",
    "dfRef = ds.dfSubData(columns=subsetCols).reset_index(drop=True)\n",
    "\n",
    "for fpn in [filePathName, filePathName.with_suffix('.xlsx'), filePathName.with_suffix('.xls'),\n",
    "            filePathName.with_suffix('.pickle'), filePathName.with_suffix('.pickle.xz')]:\n",
    "    \n",
    "    print(fpn.as_posix(), end=' : ')\n",
    "    if fpn.suffix == '.ods':\n",
    "        ds.toOpenDoc(fpn, sheetName='utest', subset=subsetCols, index=False)\n",
    "    elif fpn.suffix in ['.xlsx', '.xls']:\n",
    "        ds.toExcel(fpn, sheetName='utest', subset=subsetCols, index=False)\n",
    "    elif fpn.suffix in ['.pickle', '.xz']:\n",
    "        ds.toPickle(fpn, subset=subsetCols, index=False)\n",
    "    assert fpn.is_file()\n",
    "\n",
    "    if fpn.suffix in ['.ods', '.xlsx', '.xls']:\n",
    "        df = pd.read_excel(fpn, sheet_name='utest')\n",
    "    elif fpn.suffix in ['.pickle', '.xz']:\n",
    "        df = pd.read_pickle(fpn)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "    assert ds.compareDataFrames(df.reset_index(), dfRef.reset_index(),\n",
    "                                subsetCols=['POINT', 'DISTANCE', 'INDIVIDUS', 'EFFORT'],\n",
    "                                indexCols=['index'], dropCloser=closenessThreshold, dropNans=True).empty\n",
    "    print('1e-{} comparison OK (df.equals(dfRef) is {}, df.compare(dfRef) {}empty)'\n",
    "          .format(closenessThreshold, df.equals(dfRef), '' if df.compare(dfRef).empty else 'not'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base function for comparison (test from static hard-coded data, not from loaded DataSets)\n",
    "# => _closeness\n",
    "values = [np.nan, -np.inf,\n",
    "          -1.0e12, -1.0e5, -1.0-1e-5, -1.0, -1.0+1e-5, -1.0e-8,\n",
    "          0.0, 1.0e-8, 1.0, 1.0e5, 1.0e12, np.inf]\n",
    "aClose = np.ndarray(shape=(len(values), len(values)))\n",
    "\n",
    "for r in range(len(values)):\n",
    "    for c in range(len(values)):\n",
    "        try:\n",
    "            aClose[r, c] = ds._closeness(pd.Series([values[r], values[c]]))\n",
    "        except Exception as exc:\n",
    "            print(exc, r, c, values[r], values[c])\n",
    "            raise\n",
    "\n",
    "# Proximité infinie sur la diagonale (sauf pour nan et +/-inf)\n",
    "assert all(np.isnan(values[i]) or np.isinf(values[i]) or np.isinf(aClose[i, i]) for i in range(len(values))), \\\n",
    "       'Error: Inequality on the diagonal'\n",
    "\n",
    "# Pas de proximité infinie ailleurs\n",
    "assert all(r == c or not np.isinf(aClose[r, c]) for r in range(len(values)) for c in range(len(values))), \\\n",
    "       'Error: No equality should be found outside the diagonal'\n",
    "\n",
    "# Bonne proximité uniquement autour de -1\n",
    "whereClose = [i for i in range(len(values)) if abs(values[i] + 1) <= 1.0e-5]\n",
    "assert all(aClose[r, c] > 4 for r in whereClose for c in whereClose), 'Error: Unexpectedly bad closeness around -1'\n",
    "\n",
    "pd.DataFrame(data=aClose, index=values, columns=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison (from other files data sources, the same as for ResultsSet.compare below, but through DataSet)\n",
    "# => compare, compareDataFrames, _toHashable, _closeness\n",
    "\n",
    "# a. Chargement référence Distance 7 et valeurs à comparer issues d'AutoDS\n",
    "dsDist = ads.DataSet('refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',\n",
    "                     sheet='RefDist73', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)\n",
    "\n",
    "dsAuto = ads.DataSet('refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods',\n",
    "                     sheet='ActAuto', skipRows=[3], headerRows=[0, 1, 2], indexCols=0)\n",
    "\n",
    "# b. Colonnes d'index pour la comparaison\n",
    "indexCols = [('sample', 'AnlysNum', 'Value')] \\\n",
    "            + [('sample', col, 'Value') for col in ['Species', 'Periods', 'Prec.', 'Duration']] \\\n",
    "            + [('model', 'Model', 'Value')] \\\n",
    "            + [('parameters', 'left truncation distance', 'Value'),\n",
    "               ('parameters', 'right truncation distance', 'Value'),\n",
    "               ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "               ('parameters', 'distance discretisation cut points', 'Value')]\n",
    "\n",
    "# c. Colonnes à comparer (on retire DeltaDCV et DeltaAIC car ils dépendent des ensembles d'analyses effectuées,\n",
    "#    différents entre la référence et l'exécution auto).\n",
    "subsetCols = [col for col in dsDist.dfData.columns.to_list() \\\n",
    "              if col not in indexCols + [('run output', 'run time', 'Value'),\n",
    "                                         ('density/abundance', 'density of animals', 'Delta Cv'),\n",
    "                                         ('detection probability', 'Delta AIC', 'Value')]]\n",
    "\n",
    "# d. Comparaison \"exacte\" : aucune ligne n'y réussit (majorité d'epsilons dûs à IO ODS)\n",
    "dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols)\n",
    "assert len(dfRelDiff) == len(dsDist)\n",
    "\n",
    "# e. Comparaison à 10**-16 près : presque toutes les lignes réussissent, sauf 3 (majorité d'epsilons dûs à IO ODS).\n",
    "dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=16, dropNans=True)\n",
    "assert len(dfRelDiff) == 3\n",
    "\n",
    "dfRelDiff = dsDist.compare(dsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=5, dropNans=True)\n",
    "assert len(dfRelDiff) == 2\n",
    "\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SampleDataSet class (and base DataSet)\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel source (path as simple string)\n",
    "sds = ads.SampleDataSet(source='refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                        decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "\n",
    "assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',\n",
    "                                 'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT']\n",
    "assert len(sds) == 256\n",
    "assert sds.dfData.NOMBRE.sum() == 217\n",
    "\n",
    "sds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libre / Open Office source (path as simple string)\n",
    "sds = ads.SampleDataSet(source='refin/ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.ods',\n",
    "                        decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "\n",
    "assert sds.columns.to_list() == ['ZONE', 'HA', 'POINT', 'ESPECE', 'DISTANCE', 'MALE', 'DATE',\n",
    "                                 'OBSERVATEUR', 'PASSAGE', 'NOMBRE', 'EFFORT']\n",
    "assert len(sds) == 256\n",
    "assert sds.dfData.NOMBRE.sum() == 217\n",
    "\n",
    "sds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV source with ',' as decimal point (path as pl.Path)\n",
    "sds = ads.SampleDataSet(source=pl.Path('refin/ACDC2019-Papyrus-TURMER-AB-5mn-1dec-dist.txt'),\n",
    "                        decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])\n",
    "\n",
    "assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \\\n",
    "       'Error: Some strings found in declared decimal fields ... any decimal format issue ?'\n",
    "\n",
    "assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',\n",
    "                                 'Point transect*Survey effort', 'Observation*Radial distance']\n",
    "assert len(sds) == 330\n",
    "assert sds.dfData['Observation*Radial distance'].notnull().sum() == 324\n",
    "\n",
    "sds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV source with '.' as decimal point\n",
    "sds = ads.SampleDataSet(source=pl.Path('refin/ACDC2019-Papyrus-ALAARV-AB-10mn-1dotdec-dist.txt'),\n",
    "                       decimalFields=['Point transect*Survey effort', 'Observation*Radial distance'])\n",
    "\n",
    "assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \\\n",
    "       'Error: Some strings found in declared decimal fields ... any decimal format issue ?'\n",
    "\n",
    "assert sds.columns.to_list() == ['Region*Label', 'Region*Area', 'Point transect*Label',\n",
    "                                 'Point transect*Survey effort', 'Observation*Radial distance']\n",
    "assert len(sds) == 256\n",
    "assert sds.dfData['Observation*Radial distance'].notnull().sum() == 217\n",
    "\n",
    "sds.dfData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame source.\n",
    "dfData = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],\n",
    "                      data=[('2019-05-13', 3.5, 'TURMER', 23, 2,   83),\n",
    "                            ('2019-05-15', np.nan, 'TURMER', 23, 2,   27.355),\n",
    "                            ('2019-05-13', 0, 'ALAARV', 29, 2,   56.85),\n",
    "                            ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3,  7.2),\n",
    "                            ('2019-06-01', 2, 'PHICOL', 12, 1,  np.nan),\n",
    "                            ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),\n",
    "                           ])\n",
    "dfData['Region'] = 'ACDC'\n",
    "dfData['Surface'] = '2400'\n",
    "\n",
    "sds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])\n",
    "\n",
    "assert not any(sds.dfData[col].dropna().apply(lambda v: isinstance(v, str)).any() for col in sds.decimalFields), \\\n",
    "       'Error: Some strings found in declared decimal fields ... any decimal format issue ?'\n",
    "\n",
    "assert sds.columns.equals(dfData.columns)\n",
    "assert len(sds) == len(dfData)\n",
    "assert sds.dfData.Distance.notnull().sum() == 4\n",
    "\n",
    "sds.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XXEngine classes\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Instance creation et loading of MCDS.exe output stat. specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eng = ads.MCDSEngine(workDir='tmp/test out') # Simple string path\n",
    "    print('Error: Should have raised an AssertionError !')\n",
    "except AssertionError as exc:\n",
    "    print('Good forbidden chars detection:', exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'test out')) # pl.Path path\n",
    "    print('Error: Should have raised an AssertionError !')\n",
    "except AssertionError as exc:\n",
    "    print('Good forbidden chars detection:', exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The old run method.\n",
    "eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'), runMethod='os.system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runDir = eng.setupRunFolder(runPrefix='uni') # Unit tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Generate input data file for MCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short dataset.\n",
    "dfData = pd.DataFrame(columns=['Date', 'TrucDec', 'Espece', 'Point', 'Effort', 'Distance'],\n",
    "                      data=[('2019-05-13', 3.5, 'TURMER', 23, 2,   83),\n",
    "                            ('2019-05-15', np.nan, 'TURMER', 23, 2,   27.355),\n",
    "                            ('2019-05-13', 0, 'ALAARV', 29, 2,   56.85),\n",
    "                            ('2019-04-03', 1.325, 'PRUMOD', 53, 1.3,  7.2),\n",
    "                            ('2019-06-01', 2, 'PHICOL', 12, 1,  np.nan),\n",
    "                            ('2019-06-19', np.nan, 'PHICOL', 17, 0.5, np.nan),\n",
    "                           ])\n",
    "dfData['Region'] = 'ACDC'\n",
    "dfData['Surface'] = '2400'\n",
    "\n",
    "sds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])\n",
    "\n",
    "sds.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFileName = eng.buildDataFile(sampleDataSet=sds, runDir=runDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Compute sample stats for MCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSmpStats = eng.computeSampleStats(sds)\n",
    "sSmpStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(sSmpStats.index == eng.MIStatSampCols)\n",
    "assert all(sSmpStats.values == [4, 7.2, 83.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Generate input command file for MCDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdFileName = eng.buildCmdFile(estimKeyFn='HNORMAL', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,\n",
    "                               runDir=runDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### e. Low level analysis execution (_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode\n",
    "runStatus, startTime, elapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=False, method=eng.runMethod)\n",
    "\n",
    "dict(runStatus=runStatus, startTime=startTime, elapsedTime=elapsedTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real mode\n",
    "runStatus, startTime, engElapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method=eng.runMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real mode\n",
    "runStatus, startTime, engElapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method=eng.runMethod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeout\n",
    "runStatus, startTime, engElapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method='subprocess.run', timeOut=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 10\n",
    "\n",
    "# Performance measures : method='os.system', Lenovo T490 (4-core i5-8350U with PCI-e SSD)\n",
    "# 2021-01-06: 132 ms ± 1.47 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "# 2021-08-06: 255 ms ± 40.2 ms per loop (mean ± std. dev. of 5 runs, 10 loops each) => WTF ??????\n",
    "runStatus, startTime, engElapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method='os.system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 10\n",
    "\n",
    "# Performance measures : method='subprocess.run', Lenovo T490 (4-core i5-8350U with PCI-e SSD)\n",
    "# 2021-01-06: 191 ms ± 3.75 ms per loop (mean ± std. dev. of 5 runs, 10 loops each) => os.system faster by 60-75ms\n",
    "# 2021-08-06: 425 ms ± 27.6 ms per loop (mean ± std. dev. of 5 runs, 10 loops each) => WTF ??????\n",
    "runStatus, startTime, engElapsedTime = \\\n",
    "    eng._run(eng.ExeFilePathName, cmdFileName, forReal=True, method='subprocess.run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** (06/01/2021) : os.system systematically faster by 60-75ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### f. High level analysis execution  (via executor), debug mode\n",
    "\n",
    "(generate cmd and data input files, but no call to executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A real life (reduced) dataset\n",
    "sds = ads.SampleDataSet(source=pl.Path('refin') / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                        decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asynchronous model, even if no parallelism involved : submitAnalysis() returns a \"future\" object\n",
    "# (see module concurrent)\n",
    "futRun = eng.submitAnalysis(sds, realRun=False, runPrefix='int',\n",
    "                            estimKeyFn='UNIFORM', estimAdjustFn='POLY',\n",
    "                            estimCriterion='AIC', cvInterval=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get run output from future object\n",
    "runCode, startTime, elapsedTime, runDir, sResults = futRun.result()\n",
    "\n",
    "assert runCode == ads.MCDSEngine.RCNotRun, 'Should have NOT run (run code = 0)'\n",
    "\n",
    "dict(runCode=runCode, runDir=runDir, startTime=startTime, elapsedTime=elapsedTime, sResults=sResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    eng.submitAnalysis(sds, realRun=False, runPrefix='int',\n",
    "                       estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95).result()\n",
    "\n",
    "assert runCode == ads.MCDSEngine.RCNotRun, 'Should have NOT run (run code = 0)'\n",
    "\n",
    "dict(runCode=runCode, runDir=runDir, startTime=startTime, elapsedTime=elapsedTime, sResults=sResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### g. High level analysis execution  (via executor), real mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRunCheckErrMsg = {ads.MCDSEngine.RCOK: 'Oh, oh, should have run smoothly and successfully !',\n",
    "                   ads.MCDSEngine.RCWarnings: 'Oh, oh, should have run smoothly (even if with warnings) !',\n",
    "                   ads.MCDSEngine.RCTimedOut: 'Oh, oh, should have timed-out !'}\n",
    "\n",
    "def checkEngineAnalysisRun(sampleDataSet, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=None, expectRunCode=ads.MCDSEngine.RCOK):\n",
    "    \n",
    "    # Need for an async. executor for time limit checking with os.system run method.\n",
    "    exor = None if runMethod != 'os.system' or timeOut is None else ads.Executor(threads=1)\n",
    "        \n",
    "    # Engine\n",
    "    eng = ads.MCDSEngine(executor=exor, workDir=pl.Path('tmp', 'mcds-out'),\n",
    "                         runMethod=runMethod, timeOut=timeOut)\n",
    "    \n",
    "    # Run analysis and get results\n",
    "    fut = eng.submitAnalysis(sampleDataSet, realRun=True, runPrefix='int',\n",
    "                             estimKeyFn=estimKeyFn, estimAdjustFn=estimAdjustFn,\n",
    "                             estimCriterion=estimCriterion, cvInterval=cvInterval,\n",
    "                             minDist=minDist, maxDist=maxDist,\n",
    "                             fitDistCuts=fitDistCuts, discrDistCuts=discrDistCuts)\n",
    "    \n",
    "    try:\n",
    "        if timeOut is not None:\n",
    "            startTime = pd.Timestamp.now()  # In case of cofu.TimeoutError\n",
    "        runCode, startTime, elapsedTime, runDir, sResults = fut.result(timeout=timeOut)\n",
    "    except cofu.TimeoutError:\n",
    "        logger.info('MCDS Analysis run timed-out after {}s'.format(timeOut))\n",
    "        runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "            eng.RCTimedOut, startTime, timeOut, None, None\n",
    "\n",
    "    # Check status\n",
    "    assert runCode == expectRunCode, KRunCheckErrMsg.get(expectRunCode, 'Oh, oh, unexpected expected run code ;-)')\n",
    "    \n",
    "    # Done\n",
    "    eng.shutdown()\n",
    "    if exor:\n",
    "        exor.shutdown()\n",
    "    \n",
    "    return runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No time limit\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='NEXPON', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='os.system', timeOut=None, expectRunCode=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some time limit, but too long to stop analysis.\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='HNORMAL', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='os.system', timeOut=3, expectRunCode=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too short time limit => analysis time-out (but MCDS goes on to its end : no kill done by executor)\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='os.system', timeOut=0.1, expectRunCode=ads.MCDSEngine.RCTimedOut)\n",
    "\n",
    "logger.info('Look: MCDS was not killed, it has gone to its end, whereas the analysis has timed-out')\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No time limit\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='NEXPON', estimAdjustFn='COSINE', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='subprocess.run', timeOut=None, expectRunCode=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some time limit, but too long to stop analysis.\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='HNORMAL', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='subprocess.run', timeOut=3, expectRunCode=ads.MCDSEngine.RCErrors)\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too short time limit => analysis time-out (but MCDS goes on to its end : no kill done by executor)\n",
    "runCode, startTime, elapsedTime, runDir, sResults = \\\n",
    "    checkEngineAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           runMethod='subprocess.run', timeOut=0.1, expectRunCode=ads.MCDSEngine.RCTimedOut)\n",
    "\n",
    "logger.info('Look: MCDS was actually killed on time-out')\n",
    "\n",
    "runCode, startTime, elapsedTime, runDir, sResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Generate input data files for interactive Distance software\n",
    "\n",
    "('point transect' mode only as for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgtDir = pl.Path(eng.workDir, 'distance-in')\n",
    "tgtDir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Point transect with radial distance, no extra fields, no clustering.\n",
    "distDataFileName = \\\n",
    "    eng.buildDistanceDataFile(sds, tgtFilePathName=tgtDir / 'import-data-noextra.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point transect with radial distance, with extra fields, no clustering.\n",
    "distDataFileName = \\\n",
    "    eng.buildDistanceDataFile(sds, tgtFilePathName=tgtDir / 'import-data-withextra.txt',\n",
    "                              withExtraFields=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2: Point transect with radial distance, no extra fields, with clustering.\n",
    "eng = ads.MCDSEngine(workDir=pl.Path('tmp', 'mcds-out'), clustering=True)\n",
    "\n",
    "# Add cluster data to the data set\n",
    "dfData['Nombre'] = [1, 2, 1, 1, 2, 3]\n",
    "sds = ads.SampleDataSet(source=dfData, decimalFields=['Effort', 'Distance', 'TrucDec'])\n",
    "\n",
    "# Generate distance file\n",
    "tgtDir = pl.Path(eng.workDir, 'distance-in')\n",
    "tgtDir.mkdir(exist_ok=True)\n",
    "\n",
    "distDataFileName = \\\n",
    "    eng.buildDistanceDataFile(sds, tgtFilePathName=tgtDir / 'import-data-clusters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MCDSAnalysis class\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkAnalysisRun(sampleDataSet, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                     minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                     runMethod='os.system', timeOut=None, expectStatus=ads.MCDSEngine.RCOK):\n",
    "    \n",
    "    # Need for a parallel executor for time limit checking with os.system run method.\n",
    "    exor = None if runMethod != 'os.system' or timeOut is None else ads.Executor(threads=1)\n",
    "        \n",
    "    # Engine\n",
    "    eng = ads.MCDSEngine(executor=exor, workDir=pl.Path('tmp', 'mcds-out'),\n",
    "                         runMethod=runMethod, timeOut=timeOut)\n",
    "    \n",
    "    # Analysis\n",
    "    anlys = ads.MCDSAnalysis(engine=eng, sampleDataSet=sds, name='anlys', logData=True,\n",
    "                             estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                             minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None)\n",
    "\n",
    "    # Run\n",
    "    anlys.submit()\n",
    "    \n",
    "    # Get result\n",
    "    sResult = anlys.getResults()\n",
    "\n",
    "    # Check status\n",
    "    sts = sResult[('run output', 'run status', 'Value')]\n",
    "    assert sts == expectStatus, KRunCheckErrMsg.get(expectStatus, 'Oh, oh, unexpected expected status ;-)')\n",
    "    \n",
    "    # Done\n",
    "    eng.shutdown()\n",
    "    if exor:\n",
    "        exor.shutdown()\n",
    "    \n",
    "    return sResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dataset to work with ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A real life (reduced) dataset\n",
    "sds = ads.SampleDataSet(source=pl.Path('refin') / 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx',\n",
    "                        decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "\n",
    "sds.dfData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Engine 'os.system' RunMethod and run time limit management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No time limit\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=None, expectStatus=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some time limit, but too long to stop analysis.\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=5, expectStatus=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too short time limit => analysis time-out\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=0.01, expectStatus=ads.MCDSEngine.RCTimedOut)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Engine 'subprocess.run' RunMethod and run time limit management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No time limit\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=None, expectStatus=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some time limit, but too long to stop analysis.\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=5, expectStatus=ads.MCDSEngine.RCWarnings)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too short time limit => analysis time-out\n",
    "sResult = checkAnalysisRun(sds, estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95,\n",
    "                           minDist=None, maxDist=None, fitDistCuts=None, discrDistCuts=None,\n",
    "                           runMethod='os.system', timeOut=0.01, expectStatus=ads.MCDSEngine.RCTimedOut)\n",
    "\n",
    "sResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunMethod='subprocess.run'\n",
    "eng = ads.MCDSEngine(workDir='tmp/mcds-out', runMethod='subprocess.run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 10\n",
    "\n",
    "# 2020-01-06: 347 ms ± 8.71 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "# 2021-08-07: 781 ms ± 36.7 ms per loop (mean ± std. dev. of 5 runs, 10 loops each) => WTF !!!???\n",
    "runCode, startTime, elapsedTime, runDir, sRes = \\\n",
    "    eng.submitAnalysis(sds, realRun=True, runPrefix='int',\n",
    "                       estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunMethod='os.system'\n",
    "eng = ads.MCDSEngine(workDir='tmp/mcds-out', runMethod='os.system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 10\n",
    "\n",
    "# 2020-01-06: 272 ms ± 7.57 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n",
    "# 2021-08-07: 605 ms ± 63.2 ms per loop (mean ± std. dev. of 5 runs, 10 loops each) => WTF !!!???\n",
    "runCode, startTime, elapsedTime, runDir, sRes = \\\n",
    "    eng.submitAnalysis(sds, realRun=True, runPrefix='int',\n",
    "                       estimKeyFn='UNIFORM', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AnalysisResultsSet and ResultsSet classes\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. ResultsSet class with specialised postComputeColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A specialized results set for the tests = with extra. post-computed columns : Delta AIC\n",
    "class TestAnalysisResultsSet(ads.analyser.AnalysisResultsSet):\n",
    "    \n",
    "    def __init__(self, miCustomCols=None, dfCustomColTrans=None,\n",
    "                       dComputedCols=None, dfComputedColTrans=None):\n",
    "        \n",
    "        # Initialise base.\n",
    "        super().__init__(ads.MCDSAnalysis, miCustomCols, dfCustomColTrans, dComputedCols, dfComputedColTrans)\n",
    "        \n",
    "    # Post-computations.\n",
    "    def postComputeColumns(self):\n",
    "        \n",
    "        # Compute Delta AIC (AIC - min(group)) per { species, sample, precision, duration } group.\n",
    "        # a. Minimum AIC per group\n",
    "        aicColInd = ('detection probability', 'AIC value', 'Value')\n",
    "        aicGroupColInds = [('sample', 'species', 'Value'), ('sample', 'periods', 'Value'),\n",
    "                           ('sample', 'duration', 'Value'), ('variant', 'precision', 'Value')]\n",
    "        df2Join = self._dfData.groupby(aicGroupColInds)[[aicColInd]].min()\n",
    "        \n",
    "        # b. Rename computed columns to target\n",
    "        deltaAicColInd = ('detection probability', 'Delta AIC', 'Value')\n",
    "        df2Join.columns = pd.MultiIndex.from_tuples([deltaAicColInd])\n",
    "        \n",
    "        # c. Join the column to the target data-frame\n",
    "        self._dfData = self._dfData.join(df2Join, on=aicGroupColInds)\n",
    "        \n",
    "        # d. Compute delta-AIC in-place\n",
    "        self._dfData[deltaAicColInd] = self._dfData[aicColInd] - self._dfData[deltaAicColInd]\n",
    "\n",
    "# Results object construction\n",
    "miCustCols = pd.MultiIndex.from_tuples([('id', 'index', 'Value'),\n",
    "                                        ('sample', 'species', 'Value'),\n",
    "                                        ('sample', 'periods', 'Value'),\n",
    "                                        ('sample', 'duration', 'Value'),\n",
    "                                        ('variant', 'precision', 'Value')])\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=['index', 'species', 'periods', 'duration', 'precision'],\n",
    "                           fr=['numéro', 'espèce', 'périodes', 'durée', 'précision']))\n",
    "\n",
    "dCompCols = {('detection probability', 'Delta AIC', 'Value'): \n",
    "             len(ads.MCDSEngine.statSampCols()) + len(ads.MCDSAnalysis.MIRunColumns) + 11} # Right before AIC\n",
    "dfCompColTrans = \\\n",
    "    pd.DataFrame(index=dCompCols.keys(),\n",
    "                 data=dict(en=['Delta AIC'], fr=['Delta AIC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                            dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Some getters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty\n",
    "assert rs.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len\n",
    "assert len(rs) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "assert len(rs.index) == 0\n",
    "assert rs.index.to_list() == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "assert len(rs.columns) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Append result rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append\n",
    "sHead = pd.Series(index=miCustCols, data=list(range(len(miCustCols))))\n",
    "\n",
    "miResCols = ads.MCDSEngine.statSampCols().append(ads.MCDSAnalysis.MIRunColumns).append(ads.MCDSEngine.statModCols())\n",
    "\n",
    "sResult = pd.Series(index=miResCols, data=list(range(len(miResCols)))) # Fictive data, never mind !\n",
    "rs.append(sResult, sCustomHead=sHead)\n",
    "\n",
    "sResult = pd.Series(index=miResCols, data=list(range(1, len(miResCols) + 1))) # Fictive data, never mind !\n",
    "rs.append(sResult, sCustomHead=sHead)\n",
    "\n",
    "sResult = pd.Series(index=miResCols, data=list(range(2, len(miResCols) + 2))) # Fictive data, never mind !\n",
    "rs.append(sResult, sCustomHead=sHead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Some getters again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfRawData (no post-computed columns)\n",
    "dfRaw = rs.dfRawData\n",
    "dfRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns (Beware: rs.columns does trigger computation of ... computed columns !)\n",
    "assert len(rs._dfData.columns) == len(dfRaw.columns) and len(dfRaw.columns) == 113\n",
    "rawCols = rs._dfData.columns.to_list()\n",
    "rawCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns\n",
    "assert len(rs.columns) == 114  # The proof here !\n",
    "postCols = rs.columns.to_list()\n",
    "postCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check added == compute column\n",
    "assert set(rs.columns.to_list()) - set(dfRaw.columns.to_list()) == { ('detection probability', 'Delta AIC', 'Value') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfData (post-computations already done, never mind)\n",
    "dfPost = rs.dfData\n",
    "dfPost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "assert len(rs.index) == 3\n",
    "assert rs.index.to_list() == [0, 1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Getters: dfSubData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [('id', 'index', 'Value'), ('sample', 'species', 'Value'),\n",
    "           ('sample', 'periods', 'Value'), ('sample', 'duration', 'Value'),\n",
    "           ('detection probability', 'Delta AIC', 'Value')]\n",
    "index = [0, 2]\n",
    "\n",
    "dfSub = rs.dfSubData(index=index, columns=columns)\n",
    "assert len(dfSub) == 2\n",
    "assert dfSub.index.to_list() == index\n",
    "assert dfSub.columns.to_list() == columns\n",
    "dfSub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Getters: Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfTransData\n",
    "dfTrans = rs.dfTransData('fr')\n",
    "assert len(dfPost.columns) == len(dfTrans.columns)\n",
    "dfTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrans.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrSub = rs.dfTransData('en', index=index, columns=columns)\n",
    "assert len(dfTrSub) == 2\n",
    "assert dfTrSub.index.to_list() == index\n",
    "assert dfTrSub.columns.to_list() == ['index', 'species', 'periods', 'duration', 'Delta AIC']\n",
    "dfTrSub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Specs management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.updateSpecs(d=dict(a=1, b=2), df=pd.DataFrame([dict(a=3, b=4), dict(a=7, b=9, v=90)]), reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.updateSpecs(l=[9, -9])\n",
    "rs.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rs.updateSpecs(l=[8, -8, 0])\n",
    "    assert False, \"Error: Should have refused to overwite already existing 'dd'\"\n",
    "except AssertionError:\n",
    "    print('Good: Refuse to overwrite existing spec if not explicitly authorised to')\n",
    "\n",
    "assert rs.specs['l'] == [9, -9]\n",
    "\n",
    "rs.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.updateSpecs(**dict(l = [7, -7, 77]), overwrite=True)\n",
    "\n",
    "print('Good: Accept to overwrite existing spec if explicitly authorised to')\n",
    "\n",
    "assert rs.specs['l'] == [7, -7, 77]\n",
    "\n",
    "rs.specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### h. Imports and exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Exports (with specs)\n",
    "\n",
    "(see imports tests below for exported content checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.toExcel('tmp/results-set-uni.xlsx', sheetName='utest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.toExcel('tmp/results-set-uni.xls', sheetName='utest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.toOpenDoc('tmp/results-set-uni.ods', sheetName='utest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.toPickle('tmp/results-set-uni.pickle.xz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs.toPickle('tmp/results-set-uni.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Imports with explicit format (with specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. XLSX Format\n",
    "rs1 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs1.fromExcel('tmp/results-set-uni.xlsx', sheetName='utest')\n",
    "\n",
    "rs1.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs1.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs1.specs['d'] == rs.specs['d']\n",
    "assert rs1.specs['df'].equals(rs.specs['df'])  # == fails on NaNs in same places ...\n",
    "assert rs1.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs1.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. XLS Format\n",
    "rs2 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs2.fromExcel('tmp/results-set-uni.xls', sheetName='utest')\n",
    "\n",
    "rs2.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs2.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs2.specs['d'] == rs.specs['d']\n",
    "assert rs2.specs['df'].equals(rs.specs['df'])  # == fails on NaNs in same places ...\n",
    "assert rs2.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs2.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Format ODS\n",
    "rs3 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs3.fromOpenDoc('tmp/results-set-uni.ods', sheetName='utest')\n",
    "\n",
    "rs3.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs3.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs3.specs['d'] == rs.specs['d']\n",
    "assert rs3.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs3.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs3.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Format pickle comprimé\n",
    "rs4 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs4.fromPickle('tmp/results-set-uni.pickle.xz')\n",
    "\n",
    "rs4.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs4.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs4.specs['d'] == rs.specs['d']\n",
    "assert rs4.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs4.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs4.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Format pickle non comprimé\n",
    "rs5 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs5.fromPickle('tmp/results-set-uni.pickle')\n",
    "\n",
    "rs5.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs5.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs5.specs['d'] == rs.specs['d']\n",
    "assert rs5.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs5.specs['l'] == rs.specs['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs5.specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Imports with auto-detected format (with specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. XLSX Format\n",
    "rs1 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs1.fromFile('tmp/results-set-uni.xlsx', sheetName='utest')\n",
    "\n",
    "rs1.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs1.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs1.specs['d'] == rs.specs['d']\n",
    "assert rs1.specs['df'].equals(rs.specs['df'])  # == fails on NaNs in same places ...\n",
    "assert rs1.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs1.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B. XLS Format\n",
    "rs2 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs2.fromFile('tmp/results-set-uni.xls', sheetName='utest')\n",
    "\n",
    "rs2.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs2.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs2.specs['d'] == rs.specs['d']\n",
    "assert rs2.specs['df'].equals(rs.specs['df'])  # == fails on NaNs in same places ...\n",
    "assert rs2.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs2.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C. Format ODS\n",
    "rs3 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs3.fromFile('tmp/results-set-uni.ods', sheetName='utest')\n",
    "\n",
    "rs3.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs3.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs3.specs['d'] == rs.specs['d']\n",
    "assert rs3.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs3.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs3.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D. Format pickle comprimé\n",
    "rs4 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs4.fromFile('tmp/results-set-uni.pickle.xz')\n",
    "\n",
    "rs4.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs4.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs4.specs['d'] == rs.specs['d']\n",
    "assert rs4.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs4.specs['l'] == rs.specs['l']\n",
    "\n",
    "rs4.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Format pickle non comprimé\n",
    "rs5 = TestAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                             dComputedCols=dCompCols, dfComputedColTrans=dfCompColTrans)\n",
    "\n",
    "rs5.fromFile('tmp/results-set-uni.pickle')\n",
    "\n",
    "rs5.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "assert rs5.dfData.equals(rs.dfData)  # == fails on NaNs in same places ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs\n",
    "assert rs5.specs['d'] == rs.specs['d']\n",
    "assert rs5.specs['df'].equals(rs.specs['df'])  # == fails on NaN ..\n",
    "assert rs5.specs['l'] == rs.specs['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs5.specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv. Imports with default values for missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# How ?\n",
    "# For each file format,\n",
    "# - read target file (written above) with pandas API (not ResultsSet one)\n",
    "# - remove some columns\n",
    "# - overwrite target file with pandas API\n",
    "# - load target file with ResultsSet API, specifying default valeus for the missing columns\n",
    "# - check that results is OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Comparison\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objets MCDSAnalysisResultsSet et chargement depuis fichiers.\n",
    "modelIdCols = ['Model']\n",
    "modelParamCols = ['LTrunc', 'RTrunc', 'FitDistCuts', 'DiscrDistCuts']\n",
    "sampleIdCols = ['Species', 'Periods', 'Prec.', 'Duration']\n",
    "caseIdCols = ['AnlysNum', 'SampNum'] + sampleIdCols + modelIdCols\n",
    "\n",
    "sampCols = [('sample', col, 'Value') for col in sampleIdCols]\n",
    "miSampCols = pd.MultiIndex.from_tuples(sampCols)\n",
    "\n",
    "custCols = [('sample', 'AnlysNum', 'Value'), ('sample', 'SampNum', 'Value')] + sampCols + [('model', 'Model', 'Value')]\n",
    "miCustCols = pd.MultiIndex.from_tuples(custCols)\n",
    "\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=caseIdCols, \n",
    "                           fr=['NumAnlys', 'NumSamp', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Modèle']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Référence (obtenue avec Distance 7.3)\n",
    "rsDist = ads.MCDSAnalysisResultsSet(miSampleCols=miSampCols, sampleIndCol=('sample', 'SampNum', 'Value'),\n",
    "                                    miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                    distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                    surveyType='Point', distanceType='Radial', clustering=False)\n",
    "\n",
    "rsDist.fromFile('refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods', sheetName='RefDist73',\n",
    "                postComputed=True) # Avoid recomputations, some columns are now missing, files are old actually !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultat obtenu via AutoDS.\n",
    "rsAuto = ads.MCDSAnalysisResultsSet(miSampleCols=miSampCols, sampleIndCol=('sample', 'SampNum', 'Value'),\n",
    "                                    miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                    distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                    surveyType='Point', distanceType='Radial', clustering=False)\n",
    "\n",
    "rsAuto.fromFile('refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods', sheetName='ActAuto',\n",
    "                postComputed=True) # Avoid recomputations, some columns are now missing, files are old actually !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes d'index\n",
    "indexCols = custCols + [('parameters', 'left truncation distance', 'Value'),\n",
    "                        ('parameters', 'right truncation distance', 'Value'),\n",
    "                        ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "                        ('parameters', 'distance discretisation cut points', 'Value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes à comparer (on retire DeltaDCV et DeltaAIC car ils dépendent des ensembles d'analyses effectuées,\n",
    "#                      différents entre la référence et l'exécution auto, et une colonne string : comparaison non implémentée).\n",
    "subsetCols = [col for col in rsDist.columns.to_list() \\\n",
    "              if col not in indexCols + [('run output', 'run time', 'Value'),\n",
    "                                         ('density/abundance', 'density of animals', 'Delta Cv'),\n",
    "                                         ('detection probability', 'Delta AIC', 'Value')]]\n",
    "#subsetCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelDiff = rsDist.compare(rsAuto, subsetCols=subsetCols, indexCols=indexCols)\n",
    "assert len(dfRelDiff.columns) == 21\n",
    "assert len(dfRelDiff) == len(rsDist)\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelDiff = rsDist.compare(rsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=16, dropNans=False)\n",
    "assert len(dfRelDiff.columns) == 21\n",
    "assert len(dfRelDiff) == 8\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelDiff = rsDist.compare(rsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=16, dropNans=True)\n",
    "assert len(dfRelDiff.columns) == 21\n",
    "assert len(dfRelDiff) == 3\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelDiff = rsDist.compare(rsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=5, dropNans=True)\n",
    "assert len(dfRelDiff.columns) == 21\n",
    "assert len(dfRelDiff) == 2\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop also closer columns\n",
    "dfRelDiff = rsDist.compare(rsAuto, subsetCols=subsetCols, indexCols=indexCols, dropCloser=5, dropNans=True, dropCloserCols=True)\n",
    "assert len(dfRelDiff.columns) == 19\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### j. Post-computations\n",
    "\n",
    "Note: Self-contained, nothing needing to be run before (but 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCDSAnalysisResultsSet object + loading from file\n",
    "modelIdCols = ['Model']\n",
    "modelParamCols = ['LTrunc', 'RTrunc', 'FitDistCuts', 'DiscrDistCuts']\n",
    "sampleIdCols = ['Species', 'Periods', 'Prec.', 'Duration']\n",
    "caseIdCols = ['AnlysNum', 'SampNum'] + sampleIdCols + modelIdCols\n",
    "\n",
    "sampCols = [('sample', col, 'Value') for col in sampleIdCols]\n",
    "miSampCols = pd.MultiIndex.from_tuples(sampCols)\n",
    "\n",
    "custCols = [('sample', 'AnlysNum', 'Value'), ('sample', 'SampNum', 'Value')] + sampCols + [('model', 'Model', 'Value')]\n",
    "miCustCols = pd.MultiIndex.from_tuples(custCols)\n",
    "\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols,\n",
    "                 data=dict(en=caseIdCols, \n",
    "                           fr=['NumAnlys', 'NumSamp', 'Espèce', 'Périodes', 'Préc.', 'Durée', 'Modèle']))\n",
    "\n",
    "rsAuto = ads.MCDSAnalysisResultsSet(miSampleCols=miSampCols, sampleIndCol=('sample', 'SampNum', 'Value'),\n",
    "                                    miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                    distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                    surveyType='Point', distanceType='Radial', clustering=False)\n",
    "\n",
    "rsAuto.fromFile('refin/ACDC2019-Papyrus-ALAARV-TURMER-comp-dist-auto.ods', sheetName='ActAuto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trigger post-computations\n",
    "rsAuto.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference from file\n",
    "rsAutoRef = ads.MCDSAnalysisResultsSet(miSampleCols=miSampCols, sampleIndCol=('sample', 'SampNum', 'Value'),\n",
    "                                       miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                       distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                       surveyType='Point', distanceType='Radial', clustering=False)\n",
    "\n",
    "rsAutoRef.fromFile('refout/ACDC2019-Papyrus-ALAARV-TURMER-resultats-postcomp.ods')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of loaded results to reference\n",
    "# a. Index columns\n",
    "indexCols = custCols + [('parameters', 'left truncation distance', 'Value'),\n",
    "                        ('parameters', 'right truncation distance', 'Value'),\n",
    "                        ('parameters', 'model fitting distance cut points', 'Value'),\n",
    "                        ('parameters', 'distance discretisation cut points', 'Value'),\n",
    "                        ('parameters', 'estimator key function', 'Value'),\n",
    "                        ('parameters', 'estimator adjustment series', 'Value'),\n",
    "                        ('parameters', 'estimator selection criterion', 'Value')]\n",
    "\n",
    "# b. Colonnes to compare : we ignore ...\n",
    "# * DeltaDCV et DeltaAIC because they depend on the whole set of analyses actually done to get the results,\n",
    "#   that is possibly different sets in the 2 cases.\n",
    "# * other string-typed columns (comparison not implemented)\n",
    "subsetCols = [col for col in rsAutoRef.columns.to_list() \\\n",
    "              if col not in indexCols + [('run output', 'run time', 'Value'), ('run output', 'run folder', 'Value'),\n",
    "                                         ('density/abundance', 'density of animals', 'Delta Cv'),\n",
    "                                         ('detection probability', 'Delta AIC', 'Value'),\n",
    "                                         ('detection probability', 'key function type', 'Value'),\n",
    "                                         ('detection probability', 'adjustment series type', 'Value')]]\n",
    "\n",
    "# c. Comparison\n",
    "dfRelDiff = rsAuto.compare(rsAutoRef, subsetCols=subsetCols, indexCols=indexCols, dropCloser=15)\n",
    "assert len(dfRelDiff) == 0\n",
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Class FieldDataSet (and base DataSet)\n",
    "\n",
    "Note: For real unit tests of DataSet, see `visionat` module, which defines the same class (have to be the same: check it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObs = pd.read_csv('refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt', sep='\\t', decimal=',')\n",
    "dfObs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countCols =  ['nMalAd10', 'nAutAd10', 'nMalAd5', 'nAutAd5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sCounts = dfObs[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfObs), sCounts.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObs) == 724\n",
    "assert not any(sCounts - pd.Series({'nMalAd10': 613, 'nAutAd10': 192, 'nMalAd5': 326, 'nAutAd5': 102}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. FieldDataSet._separateMultiCategoryCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsMonoCat_ = ads.FieldDataSet._separateMultiCategoryCounts(dfObs, countCols)\n",
    "len(dfObsMonoCat_), dfObsMonoCat_[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dfObs[countCols].apply(lambda s: len(s[s > 0]), axis='columns')\n",
    "\n",
    "print(len(s), s.value_counts().to_dict())\n",
    "\n",
    "assert len(s) - len(s[s < 1]) + sum((i-1)*len(s[s == i]) for i in range(1, s.max()+1)) == len(dfObsMonoCat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObsMonoCat_) == 1125\n",
    "assert not any(dfObsMonoCat_[countCols].sum() - sCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsMonoCat_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Categorise sightings\n",
    "\n",
    "Needed for adding absence data below\n",
    "\n",
    "(no more counts - by the way, all 0 or 1 - => only catgories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should not see any sightings with all null counts\n",
    "assert dfObsMonoCat_[~dfObsMonoCat_[countCols].any(axis='columns')].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count2AdultCat(sCounts):\n",
    "    return 'm' if 'Mal' in sCounts[sCounts > 0].index[0] else 'a'\n",
    "dfObsMonoCat_['Adulte'] = dfObsMonoCat_[countCols].apply(count2AdultCat, axis='columns')\n",
    "\n",
    "def count2DurationCat(sCounts):\n",
    "    return '5mn' if '5' in sCounts[sCounts > 0].index[0] else '10mn'\n",
    "dfObsMonoCat_['Durée'] = dfObsMonoCat_[countCols].apply(count2DurationCat, axis='columns')\n",
    "\n",
    "dfObsMonoCat_.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. FieldDataSet._individualiseMonoCategoryCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndiv_ = ads.FieldDataSet._individualiseMonoCategoryCounts(dfObsMonoCat_, countCols)\n",
    "len(dfObsIndiv_), dfObsIndiv_[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObsIndiv_) == 1233\n",
    "assert not any(dfObsIndiv_[countCols].sum() - sCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. FieldDataSet.monoCategorise\n",
    "\n",
    "(combines a, b, c and d above in one function : the one to use actually !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load FieldDataSet from dfObs\n",
    "fds = ads.FieldDataSet(source=dfObs, countCols=countCols,\n",
    "                       addMonoCatCols={ 'Adulte': count2AdultCat, 'Durée': count2DurationCat })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsMonoCat = fds.monoCategorise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsMonoCat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (dfObsMonoCat == dfObsMonoCat_).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. FieldDataSet.individualise\n",
    "(combines a, b, c and d above in one function : the one to use actually !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = fds.individualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (dfObsIndiv == dfObsIndiv_).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second, try from source CSV file\n",
    "fds = ads.FieldDataSet(source='refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt',\n",
    "                       importDecFields=['distMem'], countCols=countCols,\n",
    "                       addMonoCatCols={ 'Adulte': count2AdultCat, 'Durée': count2DurationCat })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv = fds.individualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (dfObsIndiv == dfObsIndiv_).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. Drop now unneeded count columns\n",
    "\n",
    "(only 0 or 1 inside + columns Adulte and Duree to explain what a 1 means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No more need for count cols then (only 0 or 1 inside + columns Adulte and Duree to explain what a 1 means)\n",
    "dfObsIndiv.drop(columns=countCols, inplace=True)\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Class MonoCategoryDataSet (and base DataSet)\n",
    "\n",
    "Note: For real unit tests of DataSet, see `visionat` module, which defines the same class (have to be the same: check it !)\n",
    "\n",
    "Note: Run 4 above before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Extract transect info\n",
    "\n",
    "(assuming that each transect x pass gave at least 1 sighting, otherwise the effort will be wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectPlaceCol = 'Point'\n",
    "transectPlaceCols = [transectPlaceCol]\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTransPassEffort = ads.MonoCategoryDataSet._extractTransects(dfObsIndiv, transectPlaceCols=transectPlaceCols,\n",
    "                                                              passIdCol=passIdCol,\n",
    "                                                              effortCol=effortCol, effortConstVal=1)\n",
    "dfTransPassEffort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfTransPassEffort) == 41 \\\n",
    "       and len(dfTransPassEffort[dfTransPassEffort.Passage == 'a']) == 21 \\\n",
    "       and len(dfTransPassEffort[dfTransPassEffort.Passage == 'b']) == 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Select sighting from 1 sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample columns\n",
    "sampleCols = ['Passage', 'Adulte', 'Durée']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1 sample\n",
    "espece = 'Sylvia atricapilla'\n",
    "passage = 'a'\n",
    "adulte = 'm'\n",
    "duree = '10mn'\n",
    "#dfObsIndivSmpl = dfObsIndiv[(dfObsIndiv.Passage == passage) & (dfObsIndiv.Adulte == adulte) \\\n",
    "#                            & (dfObsIndiv.Duree == duree) & (dfObsIndiv.Espece == espece)]\n",
    "\n",
    "dfObsIndivSmpl, dfTrPassEffSmpl = \\\n",
    "    ads.MonoCategoryDataSet._selectSampleSightings(dSample={ 'Passage': passage, 'Adulte': adulte,\n",
    "                                                            'Durée': duree, 'Espèce': espece },\n",
    "                                                  dfAllSights=dfObsIndiv, dfAllEffort=dfTransPassEffort,\n",
    "                                                  transectPlaceCols=['Point'], passIdCol='Passage',\n",
    "                                                  effortCol='Effort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfObsIndivSmpl) == 36 and dfObsIndivSmpl[transectPlaceCol].nunique() == 18\n",
    "assert len(dfTrPassEffSmpl) == 21 and dfTrPassEffSmpl.reset_index()[transectPlaceCol].nunique() == len(dfTrPassEffSmpl)\n",
    "assert len(dfTrPassEffSmpl[dfTrPassEffSmpl.Effort != 1]) == 0 # 1 seul passage, et sur tous les points sans exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Add abscence sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivSmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndivAbscSmpl = ads.MonoCategoryDataSet._addAbsenceSightings(dfObsIndivSmpl, sampleCols, dfTrPassEffSmpl)\n",
    "len(dfObsIndivAbscSmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for no change in sample columns\n",
    "assert list(dfObsIndivAbscSmpl.columns) == list(dfObsIndivSmpl.columns)\n",
    "\n",
    "# Check for number of added rows\n",
    "assert len(dfObsIndivAbscSmpl) == 39 # 36 sightings + 3 missings transects\n",
    "\n",
    "# Check for final number of transects\n",
    "assert dfObsIndivAbscSmpl[dfTrPassEffSmpl.index.name].nunique() == 21\n",
    "\n",
    "# Check for no change in sample identification\n",
    "assert list(dfObsIndivAbscSmpl['Espèce'].unique()) == [espece, None] # None for absence sightings !\n",
    "assert list(dfObsIndivAbscSmpl.Passage.unique()) == [passage]\n",
    "assert list(dfObsIndivAbscSmpl.Adulte.unique()) == [adulte]\n",
    "assert list(dfObsIndivAbscSmpl['Durée'].unique()) == [duree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dfObsIndiv['Espèce'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Performance test\n",
    "print('Espèce      Passage  Adulte Durée NbDonnées')\n",
    "\n",
    "for espece in ['Sylvia atricapilla', 'Alauda arvensis', 'Sylvia communis', 'Phylloscopus collybita']: \n",
    "    \n",
    "    for passage in ['a', 'b', 'a+b']: \n",
    "\n",
    "        for adulte in ['m', 'a', 'm+a']:\n",
    "\n",
    "            for duree in ['5mn', '10mn']:\n",
    "\n",
    "                passages = passage.split('+')\n",
    "                adultes = adulte.split('+')\n",
    "                #dfObsIndivSmpl = dfObsIndiv[dfObsIndiv.Passage.isin(passages) & dfObsIndiv.Adulte.isin(adultes) \\\n",
    "                #                            & (dfObsIndiv.Duree == duree) & (dfObsIndiv.Espece == espece)]\n",
    "                dfObsIndivSmpl, dfTrPassEffSmpl = \\\n",
    "                    ads.MonoCategoryDataSet._selectSampleSightings(dSample={ 'Passage': passage, 'Adulte': adulte,\n",
    "                                                                            'Durée': duree, 'Espèce': espece },\n",
    "                                                                  dfAllSights=dfObsIndiv,\n",
    "                                                                  dfAllEffort=dfTransPassEffort,\n",
    "                                                                  transectPlaceCols=['Point'], passIdCol='Passage', \n",
    "                                                                  effortCol='Effort')\n",
    "\n",
    "                try:\n",
    "                    print(espece, passage, adulte, duree, ':', len(dfObsIndivSmpl), '=> ', end='')\n",
    "                    dfObsIndivAbscSmpl_ = \\\n",
    "                        ads.MonoCategoryDataSet._addAbsenceSightings(dfObsIndivSmpl, sampleCols, dfTrPassEffSmpl)\n",
    "                    print(len(dfObsIndivAbscSmpl_))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    \n",
    "print('Should give around 1s on a Core i7 8850H (6 HT cores, 2.6-4.3GHz, cache 9Mb) + NVME SSD')\n",
    "print('Should give around 1s ??? on a Core i5 8365U (4 HT cores, 1.6-4.1GHz, cache 6Mb) + NVME SSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. ads.MonoCategoryDataSet._addSurveyAreaInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')\n",
    "\n",
    "dfObsIndivAbscSmpl = ads.MonoCategoryDataSet._addSurveyAreaInfo(dfObsIndivAbscSmpl, dSurveyArea=dSurveyArea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivAbscSmpl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. MonoCategoryDataSet.sampleDataSet\n",
    "\n",
    "(combines a, b, c and d above in one function : the one to use actually, of course !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = ads.MonoCategoryDataSet(dfObsIndiv, dSurveyArea=dSurveyArea, sampleDecFields=['Effort', 'distMem'],\n",
    "                             transectPlaceCols=transectPlaceCols, passIdCol=passIdCol,\n",
    "                             effortCol=effortCol, effortConstVal=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = mds.sampleDataSet(sSampleSpecs=pd.Series({ 'Passage': passage, 'Adulte': adulte, \n",
    "                                                 'Durée': duree, 'Espèce': espece }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.dfData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Abstract class Analyser\n",
    "\n",
    "Note: Run 4 above before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Generate implicit partial variant combination table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre d'individus par espèce, pour voir quelles espèces on va analyser\n",
    "dfIndivCounts = dfObsIndiv.loc[dfObsIndiv.Adulte == 'm', ['Espèce', 'Adulte']].groupby('Espèce').count()\n",
    "\n",
    "dfIndivCounts.rename(columns=dict(Adulte='Mâles'), inplace=True)\n",
    "dfIndivCounts.sort_values(by='Mâles', ascending=False, inplace=True)\n",
    "\n",
    "dfIndivCounts[dfIndivCounts['Mâles'] >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMaxMal10 = 30\n",
    "varEspeces = list(dfIndivCounts[dfIndivCounts['Mâles'] >= nMaxMal10].index) # 1 variante par espèce\n",
    "\n",
    "varPassages = [''] # Tous les passages ensemble => 1 seule variante\n",
    "varAdultes = ['m', 'm+a'] # Les mâles, et ensuite les mâles et autres adultes (=> 2 variantes)\n",
    "varDurees = ['5mn', '10mn'] # 5 1ères mn, ou toutes les 10 => 2 variantes\n",
    "\n",
    "dfImplSampSpecs = ads.Analyser.implicitPartialVariantSpecs({ 'Espèces':varEspeces, 'Passages': varPassages,\n",
    "                                                               'Adultes': varAdultes, 'Durées': varDurees })\n",
    "dfImplSampSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Explicit partial variant combination generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExplSampSpecs = ads.Analyser.explicitPartialVariantSpecs(dfImplSampSpecs)\n",
    "dfExplSampSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Direct explicitation of all variants\n",
    "\n",
    "from user specs (implicit and explict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userVariantSpecs = 'refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx'\n",
    "\n",
    "if False: # Both method MUST work, but this one needs more code :-)\n",
    "    userVariantSpecs = pd.read_excel(userVariantSpecs, sheet_name=None)\n",
    "    print('sheets:', ', '.join(userVariantSpecs.keys()))\n",
    "\n",
    "userVariantSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFinalExplSpecs = ads.Analyser.explicitVariantSpecs(userVariantSpecs, ignore=['Params3_expl'],\n",
    "                                                     varIndCol='IndAnlys',\n",
    "                                                     #convertCols={ 'Durée': int }, # float 'cause of Excel\n",
    "                                                     computedCols=dict(AbrevAnlys=analysisAbbrev))\n",
    "\n",
    "dfFinalExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to see by eye\n",
    "dfFinalExplSpecs.to_excel('tmp/tools-unitests-final-expl-specs.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational checks\n",
    "if isinstance(userVariantSpecs, dict):\n",
    "    ddfUserVariantSpecs = userVariantSpecs\n",
    "else:\n",
    "    ddfUserVariantSpecs = pd.read_excel('refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx', sheet_name=None)\n",
    "\n",
    "nEch1Vars = 1\n",
    "df = ddfUserVariantSpecs['Echant1_impl']\n",
    "for col in df.columns:\n",
    "    nEch1Vars *= len(df[col].dropna())\n",
    "    \n",
    "nEch2Vars = 1\n",
    "df = ddfUserVariantSpecs['Echant2_impl']\n",
    "for col in df.columns:\n",
    "    nEch2Vars *= len(df[col].dropna())\n",
    "    \n",
    "nModVars = 1\n",
    "df = ddfUserVariantSpecs['Modl_impl']\n",
    "for col in df.columns:\n",
    "    nModVars *= len(df[col].dropna())\n",
    "\n",
    "nEch1ParWithVars = \\\n",
    "  len(ddfUserVariantSpecs['Params1_expl'].drop_duplicates(subset=ddfUserVariantSpecs['Echant1_impl'].columns))\n",
    "\n",
    "nEch1Pars = len(ddfUserVariantSpecs['Params1_expl'])\n",
    "\n",
    "nEch2ParWithVars = \\\n",
    "  len(ddfUserVariantSpecs['Params2_expl'].drop_duplicates(subset=ddfUserVariantSpecs['Echant2_impl'].columns))\n",
    "\n",
    "nEch2Pars = len(ddfUserVariantSpecs['Params2_expl'])\n",
    "\n",
    "nExpdVars = nModVars * (nEch1Pars + nEch1Vars - nEch1ParWithVars + nEch2Pars + nEch2Vars - nEch2ParWithVars)\n",
    "assert len(dfFinalExplSpecs) == nExpdVars\n",
    "\n",
    "nModVars, nEch1Pars, nEch1Vars, nEch1ParWithVars, nEch2Pars, nEch2Vars, nEch2ParWithVars, nExpdVars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Abstract class DSAnalyser\n",
    "\n",
    "Note: Run 6 above before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. userSpec2ParamNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntSpecEstimKeyFn = 'EstimKeyFn'\n",
    "IntSpecEstimAdjustFn = 'EstimAdjustFn'\n",
    "IntSpecEstimCriterion = 'EstimCriterion'\n",
    "IntSpecCVInterval = 'CvInterval'\n",
    "IntSpecMinDist = 'MinDist' # Left truncation distance\n",
    "IntSpecMaxDist = 'MaxDist' # Right truncation distance\n",
    "IntSpecFitDistCuts = 'FitDistCuts'\n",
    "IntSpecDiscrDistCuts = 'DiscrDistCuts'\n",
    "\n",
    "int2UserSpecREs = \\\n",
    "  { IntSpecEstimKeyFn:     ['ke[a-z]*[\\.\\-_ ]*f', 'f[o]?n[a-z]*[\\.\\-_ ]*cl'],\n",
    "    IntSpecEstimAdjustFn:  ['ad[a-z]*[\\.\\-_ ]*s', 's[éa-z]*[\\.\\-_ ]*aj'],\n",
    "    IntSpecEstimCriterion: ['crit[èa-z]*[\\.\\-_ ]*'],\n",
    "    IntSpecCVInterval:     ['conf[a-z]*[\\.\\-_ ]*[a-z]*[\\.\\-_ ]*int',\n",
    "                            'in[o]?n[a-z]*[\\.\\-_ ]*conf'],\n",
    "    IntSpecMinDist:        ['min[a-z]*[\\.\\-_ ]*d', 'd[a-z]*[\\.\\-_ ]*min',\n",
    "                            'tr[a-z]*[\\.\\-_ ]*ga', 'tr[a-z]*[\\.\\-_ ]*gc', 'le[a-z]*[\\.\\-_ ]*tr'],\n",
    "    IntSpecMaxDist:        ['max[a-z]*[\\.\\-_ ]*d', 'd[a-z]*[\\.\\-_ ]*max',\n",
    "                            'tr[a-z]*[\\.\\-_ ]*dr', 'tr[a-z]*[\\.\\-_ ]*dt', 'le[a-z]*[\\.\\-_ ]*tr'],\n",
    "    IntSpecFitDistCuts:    ['fit[a-z]*[\\.\\-_ ]*d', 'tr[a-z]*[\\.\\-_ ]*[a-z]*[\\.\\-_ ]*mod'],\n",
    "    IntSpecDiscrDistCuts:  ['dis[a-z]*[\\.\\-_ ]*d', 'tr[a-z]*[\\.\\-_ ]*[a-z]*[\\.\\-_ ]*dis']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ads.DSAnalyser.userSpec2ParamNames(['key fn', 'série-aj', 'est.crit.', 'ConfInt',\n",
    "                                           'fit d', 'disc d', 'min dist', 'maxd'], int2UserSpecREs) \\\n",
    "       == [IntSpecEstimKeyFn, IntSpecEstimAdjustFn, IntSpecEstimCriterion, IntSpecCVInterval,\n",
    "           IntSpecFitDistCuts, IntSpecDiscrDistCuts, IntSpecMinDist, IntSpecMaxDist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. _explicitParamSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDistCol = 'distMem'\n",
    "sampleDecCols=[effortCol, sampleDistCol]\n",
    "\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "sampleIndCol = 'IndSamp'\n",
    "\n",
    "varIndCol = 'IndAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via combinaisons implicites, par fichier.\n",
    "dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = \\\n",
    "    ads.DSAnalyser._explicitParamSpecs(implParamSpecs='refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx',\n",
    "                                       int2UserSpecREs=int2UserSpecREs,\n",
    "                                       sampleSelCols=sampleSelCols, abbrevCol=anlysAbbrevCol,\n",
    "                                       abbrevBuilder=analysisAbbrev, anlysIndCol=varIndCol,\n",
    "                                       sampleIndCol=sampleIndCol, dropDupes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfExplParamSpecs), userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)\n",
    "\n",
    "assert len(dfExplParamSpecs) == 48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExplParamSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via combinaisons explicites, par dataframe, avec doublons nettoyés, et colonnes neutres traversantes.\n",
    "dfExplParamSpecs.drop(columns=[varIndCol, anlysAbbrevCol, sampleIndCol], inplace=True)\n",
    "dfExplParamSpecs = dfExplParamSpecs.append(dfExplParamSpecs, ignore_index=True)  # Pleins de doublons !\n",
    "dfExplParamSpecs['AvecTronc'] = dfExplParamSpecs[['TrGche', 'TrDrte']].apply(lambda s: s.isnull().all(), axis='columns')  # Neutre 1\n",
    "dfExplParamSpecs['AbrevEsp'] = dfExplParamSpecs['Espèce'].apply(lambda s: ''.join(m[:4] for m in s.split()))  # Neutre 2\n",
    "dfExplParamSpecs                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes neutres non signalées, doublons conservés\n",
    "dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = \\\n",
    "    ads.DSAnalyser._explicitParamSpecs(dfExplParamSpecs=dfExplParamSpecs, int2UserSpecREs=int2UserSpecREs,\n",
    "                                       sampleSelCols=sampleSelCols, abbrevCol=anlysAbbrevCol,\n",
    "                                       abbrevBuilder=analysisAbbrev, anlysIndCol=varIndCol,\n",
    "                                       sampleIndCol=sampleIndCol, dropDupes=False)\n",
    "\n",
    "print(len(dfExplParamSpecs), userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)\n",
    "\n",
    "assert len(dfExplParamSpecs) == 2*48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == ['AvecTronc', 'AbrevEsp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes neutres signalées, doublons supprimés\n",
    "dfExplParamSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols = \\\n",
    "    ads.DSAnalyser._explicitParamSpecs(dfExplParamSpecs=dfExplParamSpecs, int2UserSpecREs=int2UserSpecREs,\n",
    "                                       sampleSelCols=sampleSelCols, abbrevCol=anlysAbbrevCol,\n",
    "                                       abbrevBuilder=analysisAbbrev, anlysIndCol=varIndCol,\n",
    "                                       sampleIndCol=sampleIndCol, anlysSpecCustCols=['AvecTronc', 'AbrevEsp'],\n",
    "                                       dropDupes=True)\n",
    "\n",
    "print(len(dfExplParamSpecs), userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)\n",
    "\n",
    "assert len(dfExplParamSpecs) == 48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExplParamSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. MCDSZerothOrderTruncationOptimisation class and bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds = ads.SampleDataSet(source=pl.Path('refin', 'ACDC2019-Papyrus-ALAARV-saisie-ttes-cols.xlsx'),\n",
    "                        decimalFields=['EFFORT', 'DISTANCE', 'NOMBRE'])\n",
    "dict(nSights=len(sds), nSightDist=len(sds.dfData['DISTANCE'].dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. MCDS engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = ads.MCDSEngine(workDir='tmp/mcds-zooption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variant truncation params, absolute discrDistCuts (dict, tuple, list intervals) ; submit times, onlyBest, termExprValue\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HNO', estimAdjustFn='COS', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50), maxDist=dict(min=150, max=200),\n",
    "                                              fitDistCutsFctr=[0.5, 1.5], discrDistCuts=dict(min=3, max=8),\n",
    "                                              expr2Optimise='chi2', minimiseExpr=False,\n",
    "                                              maxIters=30, termExprValue=0.5)\n",
    "\n",
    "zoption.submit(times=3, onlyBest=2)\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All variant truncation params, absolute fitDistCuts ; Interval interval\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False, autoClean=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HNO', estimAdjustFn='COS', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50), maxDist=ads.Interval(150, 200),\n",
    "                                              fitDistCuts=(10, 20), discrDistCutsFctr=(0.5, 1.5),\n",
    "                                              expr2Optimise='chi2', minimiseExpr=False, maxIters=6)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only maxDist variant truncation param\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=True, autoClean=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HAZ', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              maxDist=dict(min=150, max=200),\n",
    "                                              expr2Optimise='ks', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only minDist variant truncation param, others absent\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='UNI', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50),\n",
    "                                              expr2Optimise='chi2*ks', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant minDist, maxDist, fitDistCutsFctr, and const discrDistCuts\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HNO', estimAdjustFn='COS', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50), maxDist=dict(min=150, max=200),\n",
    "                                              fitDistCutsFctr=[0.5, 1.5], discrDistCuts=7,\n",
    "                                              expr2Optimise='chi2', minimiseExpr=False,\n",
    "                                              maxIters=30, termExprValue=0.5)\n",
    "\n",
    "zoption.submit(times=3, onlyBest=2)\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant minDist, maxDist, discrDistCutsFctr, const fitDistCuts\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False, autoClean=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HNO', estimAdjustFn='COS', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50), maxDist=ads.Interval(150, 200),\n",
    "                                              fitDistCuts=15, discrDistCutsFctr=(0.5, 1.5),\n",
    "                                              expr2Optimise='chi2', minimiseExpr=False, maxIters=6)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant maxDist, const minDist, others absent\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=True, autoClean=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='HAZ', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              maxDist=dict(min=150, max=200), minDist=20,\n",
    "                                              expr2Optimise='ks', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant minDist, const maxDist, others absent\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='UNI', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50), maxDist=200.0,\n",
    "                                              expr2Optimise='chi2*ks', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup error, no real run\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              error='Setup error !',\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='UNI', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50),\n",
    "                                              expr2Optimise='chi2*ks', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit()\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit error, no real run\n",
    "zoption = \\\n",
    "    ads.MCDSZerothOrderTruncationOptimisation(engine=eng, name='alarv', logData=False,\n",
    "                                              sampleDataSet=sds, distanceField='DISTANCE',\n",
    "                                              estimKeyFn='UNI', estimAdjustFn='POLY', \n",
    "                                              estimCriterion='AIC', cvInterval=95,\n",
    "                                              minDist=(0, 50),\n",
    "                                              expr2Optimise='cvmuw', minimiseExpr=False, maxIters=5)\n",
    "\n",
    "zoption.submit(error='Submit error !')\n",
    "\n",
    "zoption.getResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. DSParamsOptimiser abstract class \n",
    "\n",
    "(class and static methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adspo = ads.DSParamsOptimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. _parseUserSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defs for param. spec. mini-language\n",
    "auto = adspo.Auto()\n",
    "def dist(min, max):\n",
    "    return adspo.DistInterval(int(min), int(max))\n",
    "def quant(pct):\n",
    "    return adspo.OutliersMethod('quant', int(pct))\n",
    "def tucquant(pct):\n",
    "    return adspo.OutliersMethod('tucquant', float(pct))\n",
    "def mult(min, max):\n",
    "    return adspo.MultInterval(float(min), float(max))\n",
    "def abs(min, max):\n",
    "    return adspo.AbsInterval(int(min), int(max))\n",
    "def min(expr):\n",
    "    return dict(op='min', expr=expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse spec : no error (note: look at case ;-).\n",
    "for spec in [5, 12.0, 'auto', 'Auto', 'dist(5, 12)', 'quant(8)', 'QUANT(12)', 'tucquant(5)', 'mult(1.4, 7.3)', 'Abs(4, 10)']:\n",
    "    r = adspo._parseUserSpec(spec, \n",
    "                             globals=dict(Auto=adspo.Auto,\n",
    "                                          DistInterval=adspo.DistInterval,\n",
    "                                          AbsInterval=adspo.AbsInterval,\n",
    "                                          MultInterval=adspo.MultInterval,\n",
    "                                          OutliersMethod=adspo.OutliersMethod),\n",
    "                             locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,\n",
    "                                         mult=mult, abs=abs))\n",
    "    assert r[0] is None\n",
    "    print(spec, '=>', ', '.join(str(x) for x in r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse spec : errors because of bad output types.\n",
    "for spec in [1, 6.0, 'auto', 'dist(5, 12)', 'quant(8)', 'tucquant(5)', 'mult(1.4, 7.3)', 'abs(4, 10)']:\n",
    "    r = adspo._parseUserSpec(spec, \n",
    "                             globals=dict(Auto=adspo.Auto,\n",
    "                                          DistInterval=adspo.DistInterval,\n",
    "                                          AbsInterval=adspo.AbsInterval,\n",
    "                                          MultInterval=adspo.MultInterval,\n",
    "                                          OutliersMethod=adspo.OutliersMethod),\n",
    "                             locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,\n",
    "                                         mult=mult, abs=abs),\n",
    "                             errIfNotA=[dict])\n",
    "    assert r[1] is None\n",
    "    print(spec, '=>', ', '.join(str(x) for x in r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse spec : empty and no error.\n",
    "for spec in [None, np.nan, '', '   ']:\n",
    "    r = adspo._parseUserSpec(spec, \n",
    "                             globals=dict(Auto=adspo.Auto,\n",
    "                                          DistInterval=adspo.DistInterval,\n",
    "                                          AbsInterval=adspo.AbsInterval,\n",
    "                                          MultInterval=adspo.MultInterval,\n",
    "                                          OutliersMethod=adspo.OutliersMethod),\n",
    "                             locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,\n",
    "                                         mult=mult, abs=abs),\n",
    "                             nullOrEmpty='rien', errIfNotA=[dict]) # Note that errIfNotA is ignored (feature).\n",
    "    assert r[0] is None and r[1] == 'rien'\n",
    "    print(spec, '=>', ', '.join(str(x) for x in r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse spec : oneStrArg and no error.\n",
    "for spec in ['min(ks*chi2/12)']:\n",
    "    r = adspo._parseUserSpec(spec, \n",
    "                             globals=dict(),\n",
    "                             locals=dict(min=min),\n",
    "                             oneStrArg=True)\n",
    "    assert r[0] is None and r[1] == dict(op='min', expr='ks*chi2/12')\n",
    "    print(spec, '=>', ', '.join(str(x) for x in r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse spec : errors.\n",
    "for spec in ['dist(5m, 12m)', 'quant(8%)', 'tucquant(t)', 'tuckey(5)', 'mult(1,4, 7.3)', 'abs(4, \\'m\\')']:\n",
    "    r = adspo._parseUserSpec(spec, \n",
    "                             globals=dict(Auto=adspo.Auto,\n",
    "                                          DistInterval=adspo.DistInterval,\n",
    "                                          AbsInterval=adspo.AbsInterval,\n",
    "                                          MultInterval=adspo.MultInterval,\n",
    "                                          OutliersMethod=adspo.OutliersMethod),\n",
    "                             locals=dict(auto=auto, dist=dist, quant=quant, tucquant=tucquant,\n",
    "                                         mult=mult, abs=abs))\n",
    "    assert r[1] is None\n",
    "    print(spec, '=>', ', '.join(str(x) for x in r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. _parseDistTruncationUserSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No error.\n",
    "r = adspo._parseDistTruncationUserSpec(2.0, errIfNotA=[float])\n",
    "print(r)\n",
    "assert r == (None, 2.0)\n",
    "             \n",
    "r = adspo._parseDistTruncationUserSpec(7, errIfNotA=[int])\n",
    "print(r)\n",
    "assert r == (None, 7)\n",
    "             \n",
    "r = adspo._parseDistTruncationUserSpec('auto', errIfNotA=[adspo.Auto])\n",
    "print(r)\n",
    "assert r == (None, adspo.Auto())\n",
    "             \n",
    "r = adspo._parseDistTruncationUserSpec('quant(5)', errIfNotA=[adspo.OutliersMethod])\n",
    "print(r)\n",
    "assert r == (None, adspo.OutliersMethod('quant', 5))\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('abs(8, 12)', errIfNotA=[adspo.AbsInterval])\n",
    "print(r)\n",
    "assert r == (None, adspo.AbsInterval(8, 12))\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('dist(0, 70)', errIfNotA=[adspo.DistInterval])\n",
    "print(r)\n",
    "assert r == (None, adspo.DistInterval(0, 70))\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('mult(0.6, 1.2)', errIfNotA=[adspo.MultInterval])\n",
    "print(r)\n",
    "assert r == (None, adspo.MultInterval(0.6, 1.2))\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('tucquant(2.5)')\n",
    "print(r)\n",
    "assert r == (None, adspo.OutliersMethod('tucquant', 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad type errors.\n",
    "r = adspo._parseDistTruncationUserSpec('auto', errIfNotA=(adspo.AbsInterval, adspo.MultInterval))\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('quant(5)', errIfNotA=[adspo.Auto])\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('abs(8, 12)', errIfNotA=(adspo.OutliersMethod,))\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('mult(0.6, 1.2)', errIfNotA=(adspo.DistInterval, adspo.OutliersMethod))\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('tucquant(2.5)', errIfNotA=(adspo.DistInterval, adspo.MultInterval))\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing errors.\n",
    "r = adspo._parseDistTruncationUserSpec('autox')\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('tuckey(5)')\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('abs(12)')\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('mult(0.6, x)')\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None\n",
    "\n",
    "r = adspo._parseDistTruncationUserSpec('tucquant(2.5%)')\n",
    "print(r[0])\n",
    "assert r[0] is not None and r[1] is None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. MCDSTruncationOptimiser abstract class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adsto = ads.MCDSTruncationOptimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Individualised data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countCols =  ['nMalAd10', 'nAutAd10', 'nMalAd5', 'nAutAd5']\n",
    "\n",
    "def count2AdultCat(sCounts):\n",
    "    return 'm' if 'Mal' in sCounts[sCounts > 0].index[0] else 'a'\n",
    "\n",
    "def count2DurationCat(sCounts):\n",
    "    return '5mn' if '5' in sCounts[sCounts > 0].index[0] else '10mn'\n",
    "\n",
    "fds = ads.FieldDataSet(source='refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt',\n",
    "                       importDecFields=['distMem'], countCols=countCols,\n",
    "                       addMonoCatCols={ 'Adulte': count2AdultCat, 'Durée': count2DurationCat })\n",
    "\n",
    "dfObsIndiv = fds.individualise()\n",
    "\n",
    "dfObsIndiv.drop(columns=countCols, inplace=True)\n",
    "\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectPlaceCols = ['Point']\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDistCol = 'distMem'\n",
    "sampleDecCols=[effortCol, sampleDistCol]\n",
    "\n",
    "sampleCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "\n",
    "varIndCol = 'IndAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show samples\n",
    "dfObsIndiv[sampleCols].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Ctor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check run method and time-out support\n",
    "try:\n",
    "    optr = ads.MCDSTruncationOptimiser \\\n",
    "                    (dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea, \n",
    "                     transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                     sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                     workDir='tmp/mcds-optr', runMethod='os.system', runTimeOut=120)\n",
    "except AssertionError as exc:\n",
    "    if re.search(\"Can't care about .+s execution time limit\", str(exc)):\n",
    "        print('Good: Expected refuse to work for incompatible params')\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An operational one for checks below\n",
    "optr = ads.MCDSTruncationOptimiser \\\n",
    "                (dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea, \n",
    "                 transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                 sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                 distanceUnit='Meter', areaUnit='Hectare',\n",
    "                 surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                 resultsHeadCols=dict(before=[varIndCol], sample=sampleCols, after=[anlysAbbrevCol]),\n",
    "                 abbrevCol=anlysAbbrevCol, workDir='tmp/mcds-optr', logData=False,                 \n",
    "                 defEstimKeyFn='HNO', defEstimAdjustFn='COS',\n",
    "                 defEstimCriterion='AIC', defCVInterval=95,\n",
    "                 defExpr2Optimise='chi2', defMinimiseExpr=False,\n",
    "                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,\n",
    "                 defFitDistCutsFctr=dict(min=2/3, max=3/2),\n",
    "                 defDiscrDistCutsFctr=dict(min=1/3, max=1),\n",
    "                 defSubmitTimes=4, defSubmitOnlyBest=2,\n",
    "                 dDefOptimCoreParams=dict(core='zoopt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. getAnalysisOptimExprParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec is present\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecExpr2Optimise: 'min(ks*chi2/12)' })\n",
    "r = optr.getAnalysisOptimExprParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(minimiseExpr=True, expr2Optimise='ks*chi2/12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec is null\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecExpr2Optimise: None })\n",
    "r = optr.getAnalysisOptimExprParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(minimiseExpr=False, expr2Optimise='chi2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spec is absent\n",
    "sAnIntSpec = pd.Series()\n",
    "r = optr.getAnalysisOptimExprParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(minimiseExpr=False, expr2Optimise='chi2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. getAnalysisFixedParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All specs present\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecEstimKeyFn:'HNO', adsto.IntSpecEstimAdjustFn:'POLY',\n",
    "                         adsto.IntSpecEstimCriterion:'AIC', adsto.IntSpecCVInterval:97 })\n",
    "r = optr.getAnalysisFixedParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(estimKeyFn='HNO', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some specs absent => default values\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecEstimKeyFn:'UNI', adsto.IntSpecEstimAdjustFn:'POLY'})\n",
    "r = optr.getAnalysisFixedParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(estimKeyFn='UNI', estimAdjustFn='POLY', estimCriterion='AIC', cvInterval=95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. getAnalysisOptimedParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a \"random\" sample from indiv. data set\n",
    "sAnSpec = pd.Series({ 'Espèce': 'Alauda arvensis', 'Passage': 'a+b', 'Adulte': 'm+a', 'Durée': '10mn'})\n",
    "sds = optr._mcDataSet.sampleDataSet(sAnSpec[sampleCols])\n",
    "sSampleDistances = sds.dfData[sampleDistCol].dropna()\n",
    "len(sSampleDistances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some base figures for checking results\n",
    "sqd = np.sqrt(len(sSampleDistances.dropna()))\n",
    "dMin = sSampleDistances.min()\n",
    "dMax = sSampleDistances.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present and variant (check computations) 1\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'auto', adsto.IntSpecMaxDist:'quant(5)',\n",
    "                         adsto.IntSpecFitDistCuts:'abs(8, 12)', adsto.IntSpecDiscrDistCuts:'mult(0.6, 1.2)',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(2.5)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[2.5, 95])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(8, 12), discrDistCuts=ads.Interval(int(round(sqd*0.6)), int(round(sqd*1.2))))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present and variant (check computations) 2\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'quant(5)', adsto.IntSpecMaxDist:'auto',\n",
    "                         adsto.IntSpecFitDistCuts:'mult(3/4, 5/4)', adsto.IntSpecDiscrDistCuts:'abs(4, 6)',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(1)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[5, 99])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(int(round(sqd*3/4)), int(round(sqd*5/4))), discrDistCuts=ads.Interval(4, 6))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present and variant (check computations) 3\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'auto', adsto.IntSpecMaxDist:'auto',\n",
    "                         adsto.IntSpecFitDistCuts:'auto', adsto.IntSpecDiscrDistCuts:'auto',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(2)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[2, 98])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(int(round(sqd*2/3)), int(round(sqd*3/2))),\n",
    "           discrDistCuts=ads.Interval(int(round(sqd/3)), int(round(sqd))))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present and variant (check computations) 4\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'auto', adsto.IntSpecMaxDist:'auto',\n",
    "                         adsto.IntSpecFitDistCuts:'auto', adsto.IntSpecDiscrDistCuts:'auto',\n",
    "                         adsto.IntSpecOutliersMethod:'auto'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[5, 95])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(int(round(sqd*2/3)), int(round(sqd*3/2))),\n",
    "           discrDistCuts=ads.Interval(int(round(sqd/3)), int(round(sqd))))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present, some variant, some consts (check computations) 1\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:12, adsto.IntSpecMaxDist:'quant(5)',\n",
    "                         adsto.IntSpecFitDistCuts:'abs(8, 12)', adsto.IntSpecDiscrDistCuts:'mult(0.6, 1.2)',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(2.5)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[2.5, 95])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=12, maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(8, 12), discrDistCuts=ads.Interval(int(round(sqd*0.6)), int(round(sqd*1.2))))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present, some variant, some consts (check computations) 2\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'quant(5)', adsto.IntSpecMaxDist:250.0,\n",
    "                         adsto.IntSpecFitDistCuts:'mult(3/4, 5/4)', adsto.IntSpecDiscrDistCuts:'abs(4, 6)',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(1)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[5, 99])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=250.0,\n",
    "           fitDistCuts=ads.Interval(int(round(sqd*3/4)), int(round(sqd*5/4))), discrDistCuts=ads.Interval(4, 6))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present, some variant, some consts (check computations) 3\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'auto', adsto.IntSpecMaxDist:'auto',\n",
    "                         adsto.IntSpecFitDistCuts:17, adsto.IntSpecDiscrDistCuts:'auto',\n",
    "                         adsto.IntSpecOutliersMethod:'tucquant(2)'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[2, 98])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=17, discrDistCuts=ads.Interval(int(round(sqd/3)), int(round(sqd))))\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All present, some variant, some consts (check computations) 4\n",
    "# a. Call method\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecMinDist:'auto', adsto.IntSpecMaxDist:'auto',\n",
    "                         adsto.IntSpecFitDistCuts:'auto', adsto.IntSpecDiscrDistCuts:6,\n",
    "                         adsto.IntSpecOutliersMethod:'auto'})\n",
    "e, r = optr.getAnalysisOptimedParams(sAnIntSpec, sSampleDistances)\n",
    "\n",
    "assert e is None\n",
    "\n",
    "sr = str({ k:str(v) for k,v in r.items() })\n",
    "print('Actual result   :', sr)\n",
    "\n",
    "# b. Compute theorical result\n",
    "qLeft, qRight = np.percentile(a=sSampleDistances, q=[5, 95])\n",
    "\n",
    "print('Base variables  :', dict(sqd=sqd, dMin=dMin, dMax=dMax, qLeft=qLeft, qRight=qRight))\n",
    "\n",
    "sol = dict(minDist=ads.Interval(dMin, qLeft), maxDist=ads.Interval(qRight, dMax),\n",
    "           fitDistCuts=ads.Interval(int(round(sqd*2/3)), int(round(sqd*3/2))), discrDistCuts=6)\n",
    "\n",
    "ssol = str({ k:str(v) for k,v in sol.items() })\n",
    "print('Theorical result:', ssol)\n",
    "\n",
    "# c. Check \"equality\" (for some reason, must use str repr for comparison ...)\n",
    "assert sr == ssol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. getOptimisationCoreParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs not present => default from ctor\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecOptimisationCore: np.nan })\n",
    "r = optr.getOptimisationCoreParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(core='zoopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs null => default from ctor\n",
    "sAnIntSpec = pd.Series()\n",
    "r = optr.getOptimisationCoreParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(core='zoopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some specs present, with all default values ; string as last param.\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecOptimisationCore: 'zoopt(mxi=0,a=racos)'})\n",
    "r = optr.getOptimisationCoreParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(core='zoopt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some specs present, some with default values, some not, 1 non keyword param.\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecOptimisationCore: 'zoopt(80, a=racos)'})\n",
    "r = optr.getOptimisationCoreParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(core='zoopt', maxIters=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All specs present, no default value\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecOptimisationCore: 'zoopt(a=sracos,mxi=450,tv=1,mxr=5)'})\n",
    "r = optr.getOptimisationCoreParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(core='zoopt', algorithm='sracos', maxIters=450, termExprValue=1, maxRetries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### g. getOptimisationSubmitParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs not present => default from ctor\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecSubmitParams: np.nan })\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(times=4, onlyBest=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specs null => default from ctor\n",
    "sAnIntSpec = pd.Series()\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(times=4, onlyBest=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some specs present, with default values\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecSubmitParams: 'times(n=9)'})\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(times=9, onlyBest=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All specs present, no default value\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecSubmitParams: 'times(100, b=22)'})\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[0] is None and r[1] == dict(times=100, onlyBest=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad times times\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecSubmitParams: 'times(n=0, b=22)'})\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[1] is None and str(r[0]).find('Run times must be > 0') >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad best kept values number\n",
    "sAnIntSpec = pd.Series({ adsto.IntSpecSubmitParams: 'times(2, b=0)'})\n",
    "r = optr.getOptimisationSubmitParams(sAnIntSpec)\n",
    "print(*r)\n",
    "assert r[1] is None and str(r[0]).find('Number of best kept values must be > 0') >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. MCDSAnalyser & OptanalyserResultsSet\n",
    "\n",
    "* Non-regression tests between notebook protopype and \"industrialised\" version\n",
    "  (Ehrrr ... well ... between the buggy notebook prototype and the industrialised-derived identically buggy version :-(\n",
    "* Quality tests for \"industrialised\" version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ads.logger('ads.dat', level=ads.INFO, reset=True)\n",
    "_ = ads.logger('ads.anr', level=ads.DEBUG3, reset=True)\n",
    "_ = ads.logger('ads.onr', level=ads.DEBUG3, reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The industrialised-derived optanalyser results set for reproducing the buggy prototype version\n",
    "class PrototypeConformResultsSet(ads.MCDSTruncOptanalysisResultsSet):\n",
    "\n",
    "    def __init__(self, miCustomCols=None, dfCustomColTrans=None, miSampleCols=None, sampleIndCol=None,\n",
    "                       sortCols=[], sortAscend=[], distanceUnit='Meter', areaUnit='Hectare',\n",
    "                       surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                       ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),\n",
    "                                          dict(col='right', minDist=25.0, maxLen=25.0)],\n",
    "                       truncIntrvEpsilon=1e-6):\n",
    "        \n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(miCustomCols=miCustomCols, dfCustomColTrans=dfCustomColTrans,\n",
    "                         miSampleCols=miSampleCols, sampleIndCol=sampleIndCol,\n",
    "                         sortCols=sortCols, sortAscend=sortAscend, distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                         ldTruncIntrvSpecs=ldTruncIntrvSpecs,\n",
    "                         truncIntrvEpsilon=truncIntrvEpsilon)\n",
    "\n",
    "    @classmethod\n",
    "    def _combinedQualityMoreChi2(cls, sRes):  # Prototype bug: (x*y*...*z)^(1/8) ; should be: x*y*...*z^(1/8)\n",
    "        return sRes[[cls.CLChi2, cls.CLChi2, cls.CLKS, cls.CLCvMUw, cls.CLCvMCw]].prod() \\\n",
    "               * cls._normNObs(sRes) * cls._normNTotPars(sRes, a=0.2, b=0.6) \\\n",
    "               * cls._normCVDens(sRes, a=12) ** (1.0/8)\n",
    "\n",
    "    @classmethod\n",
    "    def _combinedQualityMoreKS(cls, sRes):  # Prototype bug: idem\n",
    "        return sRes[[cls.CLChi2, cls.CLKS, cls.CLKS, cls.CLCvMUw, cls.CLCvMCw]].prod() \\\n",
    "               * cls._normNObs(sRes) * cls._normNTotPars(sRes, a=0.2, b=0.6) \\\n",
    "               * cls._normCVDens(sRes, a=12) ** (1.0/8)\n",
    "\n",
    "    @classmethod\n",
    "    def _combinedQualityMoreDCv(cls, sRes):  # Prototype bug: idem\n",
    "        return sRes[[cls.CLChi2, cls.CLKS, cls.CLCvMUw, cls.CLCvMCw]].prod() \\\n",
    "               * cls._normNObs(sRes) * cls._normNTotPars(sRes, a=0.2, b=0.6) \\\n",
    "               * (cls._normCVDens(sRes, a=12) ** 2) ** (1.0/8)\n",
    "\n",
    "    def _postComputeQualityIndicators(self):\n",
    "        \n",
    "        logger.debug('Post-computing Quality Indicators')\n",
    "\n",
    "        self._dfData[self.CLSightRate] = 100 * self._dfData.apply(self._normNObs, axis='columns') # [0,1] => %\n",
    "\n",
    "        # Prepare data for computations\n",
    "        miCompCols = [cls.CLNObs, cls.CLNTotObs, cls.CLNTotPars, \n",
    "                      cls.CLChi2, cls.CLKS, cls.CLCvMUw, cls.CLCvMCw, cls.CLDCv]\n",
    "        dfCompData = self._dfData[miCompCols].copy()\n",
    "\n",
    "        logger.debug1('* Balanced quality 1')\n",
    "        self._dfData[self.CLCmbQuaBal1] = dfCompData.apply(self._combinedQualityBalanced1, axis='columns')\n",
    "\n",
    "        logger.debug1('* Balanced quality 2')\n",
    "        self._dfData[self.CLCmbQuaBal2] = dfCompData.apply(self._combinedQualityBalanced2, axis='columns')\n",
    "\n",
    "        logger.debug1('* Balanced quality 3')\n",
    "        self._dfData[self.CLCmbQuaBal3] = dfCompData.apply(self._combinedQualityBalanced3, axis='columns')\n",
    "\n",
    "        logger.debug1('* Balanced quality Chi2+')\n",
    "        self._dfData[self.CLCmbQuaChi2] = dfCompData.apply(self._combinedQualityMoreChi2, axis='columns')\n",
    "\n",
    "        logger.debug1('* Balanced quality KS+')\n",
    "        self._dfData[self.CLCmbQuaKS]   = dfCompData.apply(self._combinedQualityMoreKS, axis='columns')\n",
    "\n",
    "        logger.debug1('* Balanced quality DCv+')\n",
    "        self._dfData[self.CLCmbQuaDCv]  = dfCompData.apply(self._combinedQualityMoreDCv, axis='columns')\n",
    "\n",
    "# And the industrialised-derived optanalyser for instanciating it easily\n",
    "class PrototypeConformOptanalyser(ads.MCDSTruncationOptanalyser):\n",
    "\n",
    "    def __init__(self, dfMonoCatObs, dfTransects=None, effortConstVal=1, dSurveyArea=dict(), \n",
    "                 transectPlaceCols=['Transect'], passIdCol='Pass', effortCol='Effort',\n",
    "                 sampleSelCols=['Species', 'Pass', 'Adult', 'Duration'], \n",
    "                 sampleDecCols=['Effort', 'Distance'], sampleDistCol='Distance', anlysSpecCustCols=[],\n",
    "                 abbrevCol='AnlysAbbrev', abbrevBuilder=None, anlysIndCol='AnlysNum', sampleIndCol='SampleNum',\n",
    "                 distanceUnit='Meter', areaUnit='Hectare',\n",
    "                 surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                 resultsHeadCols=dict(before=['AnlysNum', 'SampleNum'], after=['AnlysAbbrev'], \n",
    "                                      sample=['Species', 'Pass', 'Adult', 'Duration']),\n",
    "                 ldTruncIntrvSpecs=[dict(col='left', minDist=5.0, maxLen=5.0),\n",
    "                                    dict(col='right', minDist=25.0, maxLen=25.0)], truncIntrvEpsilon=1e-6,\n",
    "                 workDir='.', runMethod='subprocess.run', runTimeOut=300, logData=False,\n",
    "                 logAnlysProgressEvery=50, logOptimProgressEvery=5, backupOptimEvery=50, autoClean=True,\n",
    "                 defEstimKeyFn=ads.MCDSEngine.EstKeyFnDef, defEstimAdjustFn=ads.MCDSEngine.EstAdjustFnDef,\n",
    "                 defEstimCriterion=ads.MCDSEngine.EstCriterionDef, defCVInterval=ads.MCDSEngine.EstCVIntervalDef,\n",
    "                 defMinDist=ads.MCDSEngine.DistMinDef, defMaxDist=ads.MCDSEngine.DistMaxDef, \n",
    "                 defFitDistCuts=ads.MCDSEngine.DistFitCutsDef, defDiscrDistCuts=ads.MCDSEngine.DistDiscrCutsDef,\n",
    "                 defExpr2Optimise='chi2', defMinimiseExpr=False,\n",
    "                 defOutliersMethod='tucquant', defOutliersQuantCutPct=5,\n",
    "                 defFitDistCutsFctr=dict(min=2/3, max=3/2),\n",
    "                 defDiscrDistCutsFctr=dict(min=1/3, max=1),\n",
    "                 defSubmitTimes=1, defSubmitOnlyBest=None, dDefSubmitOtherParams=dict(),\n",
    "                 dDefOptimCoreParams=dict(core='zoopt', maxIters=100, termExprValue=None,\n",
    "                                          algorithm='racos', maxRetries=0)):\n",
    "\n",
    "        \n",
    "        super().__init__(dfMonoCatObs, dfTransects=dfTransects, effortConstVal=effortConstVal, dSurveyArea=dSurveyArea, \n",
    "                         transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                         sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                         sampleDistCol=sampleDistCol, anlysSpecCustCols=anlysSpecCustCols,\n",
    "                         abbrevCol=abbrevCol, abbrevBuilder=abbrevBuilder,\n",
    "                         anlysIndCol=anlysIndCol, sampleIndCol=sampleIndCol,\n",
    "                         distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                         surveyType=surveyType, distanceType=distanceType, clustering=clustering,\n",
    "                         resultsHeadCols=resultsHeadCols,\n",
    "                         ldTruncIntrvSpecs=ldTruncIntrvSpecs, truncIntrvEpsilon=truncIntrvEpsilon,\n",
    "                         workDir=workDir, runMethod=runMethod, runTimeOut=runTimeOut, logData=logData,\n",
    "                         logAnlysProgressEvery=logAnlysProgressEvery, logOptimProgressEvery=logOptimProgressEvery,\n",
    "                         backupOptimEvery=backupOptimEvery, autoClean=autoClean,\n",
    "                         defEstimKeyFn=defEstimKeyFn, defEstimAdjustFn=defEstimAdjustFn,\n",
    "                         defEstimCriterion=defEstimCriterion, defCVInterval=defCVInterval,\n",
    "                         defMinDist=defMinDist, defMaxDist=defMaxDist, \n",
    "                         defFitDistCuts=defFitDistCuts, defDiscrDistCuts=defDiscrDistCuts,\n",
    "                         defExpr2Optimise=defExpr2Optimise, defMinimiseExpr=defMinimiseExpr,\n",
    "                         defOutliersMethod=defOutliersMethod, defOutliersQuantCutPct=defOutliersQuantCutPct,\n",
    "                         defFitDistCutsFctr=defFitDistCutsFctr, defDiscrDistCutsFctr=defDiscrDistCutsFctr,\n",
    "                         defSubmitTimes=defSubmitTimes, defSubmitOnlyBest=defSubmitOnlyBest,\n",
    "                         dDefSubmitOtherParams=dDefSubmitOtherParams, dDefOptimCoreParams=dDefOptimCoreParams)\n",
    "\n",
    "    def setupResults(self):\n",
    "    \n",
    "        \"\"\"Build an empty results objects.\n",
    "        \"\"\"\n",
    "\n",
    "        miCustCols, dfCustColTrans, miSampCols, sampIndMCol, sortCols, sortAscend = \\\n",
    "            self.prepareResultsColumns()\n",
    "\n",
    "        return PrototypeConformResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                          miSampleCols=miSampCols, sampleIndCol=sampIndMCol,\n",
    "                                          sortCols=sortCols, sortAscend=sortAscend,\n",
    "                                          distanceUnit=self.distanceUnit, areaUnit=self.areaUnit,\n",
    "                                          surveyType=self.surveyType, distanceType=self.distanceType,\n",
    "                                          clustering=self.clustering,\n",
    "                                          ldTruncIntrvSpecs=self.ldTruncIntrvSpecs,\n",
    "                                          truncIntrvEpsilon=self.truncIntrvEpsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Load results to postCompute from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colEspece = 'Espèce'\n",
    "colPassage = 'Passage'\n",
    "colDistance = 'Distance'\n",
    "\n",
    "groupage = False\n",
    "effortConst = 1 # Valeur d'effort constante = 1 par passage sur chaque point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dossier = pl.Path('../donnees/acdc')\n",
    "\n",
    "nomEtude = 'ACDC2019'\n",
    "sousEtude = '-Nat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "fpn = dossier / f'{nomEtude}{sousEtude}-ObsIndivDist.xlsx'\n",
    "with pd.ExcelFile(fpn) as xlsFile:\n",
    "    dfObsCatIndiv = pd.read_excel(xlsFile, sheet_name='Donnees')\n",
    "    dfTransects = pd.read_excel(xlsFile, sheet_name='Inventaires')\n",
    "\n",
    "print(dict(etude=nomEtude+sousEtude, donnees=len(dfObsCatIndiv), inventaires=len(dfTransects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Description des données\n",
    "transectPlaceCols = ['Point']\n",
    "passIdCol = colPassage\n",
    "\n",
    "assert 'effortCol' not in dir() or effortCol == 'Effort'  # In rare cases, needs to be defined before here, but the same way !\n",
    "effortCol = 'Effort'\n",
    "\n",
    "colsSpeSelEchant = ['Adulte', 'Durée']  # Colonnes de sélection des échantillons : en plus de Espèce et Passage. \n",
    "sampleDistCol = colDistance\n",
    "sampleDecCols = [effortCol, sampleDistCol]\n",
    "\n",
    "sampleNumCol = 'Echant'\n",
    "sampleSelCols = [colEspece, passIdCol] + colsSpeSelEchant\n",
    "\n",
    "#sampleAbbrevCol = 'Abrev. Echant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Compléments pour les analyses.\n",
    "distanceUnit = 'Meter'\n",
    "areaUnit = 'Sq. Kilometer'\n",
    "surveyType = 'Point'\n",
    "distanceType = 'Radial'\n",
    "\n",
    "dZoneEtude = dict(Zone='ACDC', Surface=24) # km2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compléments pour les optanalyses.\n",
    "anlysIndCol = 'Analyse'\n",
    "anlysAbbrevCol = 'Abrev. Analyse'\n",
    "anlysParamCols = ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaîne courte d'identification d'une spec. d'échantillon.\n",
    "def sampleAbbrev(sSamp):\n",
    "    abbrvs = [''.join(word[:4].title() for word in sSamp[colEspece].split(' ')[:2])]\n",
    "    if colPassage in sSamp.index and not pd.isnull(sSamp.Passage) and sSamp.Passage:\n",
    "        abbrvs.append(sSamp.Passage.replace('+', ''))\n",
    "    if 'Durée' in sSamp.index:\n",
    "        abbrvs.append(sSamp['Durée'].replace('+', ''))\n",
    "    if 'Adulte' in sSamp.index:\n",
    "        abbrvs.append(sSamp.Adulte.replace('+', ''))\n",
    "    return '-'.join(abbrvs)\n",
    "\n",
    "# Chaîne courte d'identification d'une analyse.\n",
    "def analysisAbbrev(sAnlys):\n",
    "    \n",
    "    # Sample abbreviation\n",
    "    abbrevs = [sampleAbbrev(sAnlys)]\n",
    "\n",
    "    # Model + Parameters abbreviation\n",
    "    abbrevs += [sAnlys['FonctionClé'][:3].lower(), sAnlys['SérieAjust'][:3].lower()]\n",
    "    dTroncAbbrv = { 'l': 'TrGche' if 'TrGche' in sAnlys.index else 'TroncGche',\n",
    "                    'r': 'TrDrte' if 'TrDrte' in sAnlys.index else 'TroncDrte',\n",
    "                    'm': 'NbTrModel' if 'NbTrModel' in sAnlys.index else  'NbTrchMod',\n",
    "                    'd': 'NbTrDiscr' }\n",
    "    for abbrev, name in dTroncAbbrv.items():\n",
    "        if name in sAnlys.index and not pd.isnull(sAnlys[name]):\n",
    "            abbrevs.append('{}{}'.format(abbrev, sAnlys[name][0].lower() if isinstance(sAnlys[name], str)\n",
    "                                                 else int(sAnlys[name])))\n",
    "   \n",
    "    return '-'.join(abbrevs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An optanalyser object knowns how to build an empty results object ...\n",
    "optanlr = \\\n",
    "    PrototypeConformOptanalyser(dfObsCatIndiv, dfTransects=dfTransects,\n",
    "                                effortConstVal=effortConst, dSurveyArea=dZoneEtude, \n",
    "                                transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                                sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                                abbrevCol=anlysAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                                anlysIndCol=anlysIndCol, sampleIndCol=sampleNumCol,\n",
    "                                distanceUnit=distanceUnit, areaUnit=areaUnit,\n",
    "                                surveyType=surveyType, distanceType=distanceType, clustering=groupage,\n",
    "                                resultsHeadCols=dict(before=[anlysIndCol, sampleNumCol], sample=sampleSelCols,\n",
    "                                                     after=anlysParamCols + [anlysAbbrevCol]))\n",
    "\n",
    "results = optanlr.setupResults()\n",
    "\n",
    "optanlr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varEtude = ''\n",
    "\n",
    "resFileName = f'{nomEtude}{sousEtude}-OptAnalyses{varEtude}-resultats.xlsx'\n",
    "\n",
    "resFolders = [fn.name for fn in dossier.glob('[0-9]'*6+'-'+'[0-9]'*4) if (fn / resFileName).is_file()]\n",
    "\n",
    "print('Résultats historiques disponibles:', ', '.join(resFolders))\n",
    "\n",
    "workDir = dossier / resFolders[0]  # <=== Choisir le dossier de résultats ici.\n",
    "\n",
    "updatedResFileNameExists = (pl.Path('tmp') / resFileName).is_file()\n",
    "if not updatedResFileNameExists:\n",
    "\n",
    "    resFileName = workDir / resFileName\n",
    "\n",
    "    print(f'Fichier choisi : {resFileName.as_posix()}')\n",
    "    \n",
    "else:\n",
    "    \n",
    "    resFileName = pl.Path('tmp') / resFileName\n",
    "\n",
    "    print(f'... mais résultats mis à jour aussi: {resFileName.as_posix()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load results from file\n",
    "print('Lecture du fichier choisi:', resFileName.as_posix(), '...')\n",
    "results.fromExcel(resFileName, postComputed=updatedResFileNameExists)\n",
    "print('... terminé.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not updatedResFileNameExists:\n",
    "    \n",
    "    # Add sample stats a posteriori (these stats had not been implemented when the historical results were saved to disk)\n",
    "    dfSampleStats = pd.read_excel(dossier / f'{nomEtude}{sousEtude}-StatsEchantillons.xlsx')\n",
    "    dfSampleStats.rename(columns={'NTot Obs': 'NTot Obs0'}, inplace=True)\n",
    "    dfSampleStats.insert(dfSampleStats.columns.to_list().index('Distance Min'), 'NTot Obs', dfSampleStats['NTot Obs0'])\n",
    "    dfSampleStats.drop(columns=['NTot Obs0'], inplace=True)\n",
    "\n",
    "    miSampleCols = pd.MultiIndex.from_tuples([('header (sample)', colEspece, 'Value'),\n",
    "                                              ('header (sample)', colPassage, 'Value'),\n",
    "                                              ('header (sample)', colsSpeSelEchant[0], 'Value'),\n",
    "                                              ('header (sample)', colsSpeSelEchant[1], 'Value')])\n",
    "    dfSampleStats.columns = miSampleCols.append(ads.MCDSEngine.MIStatSampCols)\n",
    "\n",
    "    results.dfData = results._dfData.join(dfSampleStats.set_index(miSampleCols.to_list()), on=miSampleCols.to_list())\n",
    "    \n",
    "    print(len(results._dfData))\n",
    "    \n",
    "    print('Ecriture du fichier mis à jour:', (pl.Path('tmp') / resFileName.name).as_posix(), '...')\n",
    "    results.toExcel(pl.Path('tmp') / resFileName.name)\n",
    "    print('... terminé.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results._dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Trigger pos-computations now !\n",
    "dfRes = results.dfTransData('fr')\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRes.to_excel('tmp/res3.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Load reference post-computed results = prototype-enriched results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repFileName = workDir / f'{nomEtude}{sousEtude}-OptAnalyses{varEtude}-rapenrich.xlsx'\n",
    "\n",
    "print(f'Fichier choisi : {repFileName.as_posix()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfRefRep = pd.read_excel(repFileName, index_col=0)\n",
    "\n",
    "DRefRep2ResCols = {'Distance Min': 'Min Dist',\n",
    "                   'Distance Max': 'Max Dist',\n",
    "                   \n",
    "                   'Qual Equi': 'Qual Equi 1',\n",
    "                   'Qual Chi2': 'Qual Chi2+',\n",
    "                   'Qual DCV': 'Qual DCv+',\n",
    "                   'Qual KS': 'Qual KS+',\n",
    "                   \n",
    "                   'Grp Dist Tronc Gche': 'Groupe Tronc Gche',\n",
    "                   'Grp Dist Tronc Drte': 'Groupe Tronc Drte',\n",
    "                   \n",
    "                   'Meil AIC Tronc Id': 'Ordre Tronc Ident AIC',\n",
    "                   \n",
    "                   'Meil CKCv Tronc Proch'     : 'Ordre Tronc Proch Chi2 KS DCv',\n",
    "                   'Meil CVDens Tronc Proch'   : 'Ordre Tronc Proch DCv',\n",
    "                   'Meil Qual Equi Tronc Proch': 'Ordre Tronc Proch Qual Equi 1',\n",
    "                   'Meil Qual Chi2 Tronc Proch': 'Ordre Tronc Proch Qual Equi Chi2+',\n",
    "                   'Meil Qual KS Tronc Proch'  : 'Ordre Tronc Proch Qual Equi KS+',\n",
    "                   'Meil Qual DCV Tronc Proch' : 'Ordre Tronc Proch Qual Equi DCv+',\n",
    "                   \n",
    "                   'Ord CKCv'     : 'Ordre Global Chi2 KS DCv',\n",
    "                   'Ord Qual Equi': 'Ordre Global Qual Equi 1',\n",
    "                   'Ord Qual Chi2': 'Ordre Global Qual Equi Chi2+',\n",
    "                   'Ord Qual KS'  : 'Ordre Global Qual Equi KS+',\n",
    "                   'Ord Qual DCV' : 'Ordre Global Qual Equi DCv+',\n",
    "                   'Ord Simpl Tronc': 'Ordre Global DeltaAIC Chi2 KS DCv'}\n",
    "dfRefRep.rename(columns=DRefRep2ResCols, inplace=True)\n",
    "\n",
    "assert all(col in dfRefRep.columns for col in DRefRep2ResCols.values())\n",
    "\n",
    "dfRefRep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Compare loaded and post-computed to reference prototype intermediate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results columns\n",
    "resFrCols = dfRes.columns\n",
    "', '.join(resFrCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns in results, but not in reference intermediate report\n",
    "diffCols = set(resFrCols) - set(dfRefRep.columns)\n",
    "assert diffCols == {'Ordre Global Qual Equi 2', 'Ordre Global Qual Equi 3',\n",
    "                    'Ordre Tronc Proch Qual Equi 2', 'Ordre Tronc Proch Qual Equi 3',\n",
    "                    'Qual Equi 2', 'Qual Equi 3'}\n",
    "diffCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(sorted(resFrCols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "', '.join(sorted(dfRefRep.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Columns in reference intermediate report, but not in results\n",
    "diffCols = set(dfRefRep.columns) - set(resFrCols)\n",
    "assert not diffCols\n",
    "diffCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index columns for comparison\n",
    "indexCols = [sampleNumCol] + sampleSelCols + [anlysIndCol, anlysAbbrevCol] + anlysParamCols\n",
    "', '.join(indexCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round truncation distance parameters in order to be able to use them as part of the index columns\n",
    "# (Excel I/O changed some least significant after dot figures)\n",
    "dfRes['TrGche'] = dfRes['TrGche'].round(5)\n",
    "dfRes['TrDrte'] = dfRes['TrDrte'].round(5)\n",
    "\n",
    "dfRefRep['TrGche'] = dfRefRep['TrGche'].round(5)\n",
    "dfRefRep['TrDrte'] = dfRefRep['TrDrte'].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare: let's ignore ...\n",
    "# * DeltaDCV et DeltaAIC: they depend on actual analyses sets done at once, may differing from ref to actual results,\n",
    "# * other string columns (comparison not implemented)\n",
    "# * other neglectible (run time, ... etc) or newly implemented (not in ref) columns\n",
    "subsetCols = [col for col in dfRefRep.columns \\\n",
    "              if col not in indexCols + ['HeureExec', 'DuréeExec', 'DossierExec',\n",
    "                                         'Fn Clé Mod', 'Sér Ajust Mod', 'Crit Chx Mod', 'Interv Conf',\n",
    "                                         'Fn Clé', 'Sér Ajust',\n",
    "                                         'Delta AIC', 'Delta CoefVar Densité',\n",
    "                                         'Max Dist', 'Min Dist',\n",
    "                                         'Qual Equi 2', 'Qual Equi 3',\n",
    "                                         'Ordre Tronc Proch Qual Equi 2', 'Ordre Tronc Proch Qual Equi 3',\n",
    "                                         'Ordre Global Qual Equi 2', 'Ordre Global Qual Equi 3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Comparaison => bilan = 21% d'analyses à ordres légèrement différents (pas vu de différence != 1, mais pas tout regardé)\n",
    "dfRelDiff = ads.DataSet.compareDataFrames(dfRes, dfRefRep, dropCloser=13, dropNans=True, dropCloserCols=True,\n",
    "                                          subsetCols=subsetCols, indexCols=indexCols)\n",
    "#assert len(dfRelDiff) == 0\n",
    "dict(refRows=len(dfRefRep), resRows=len(dfRes), diffRows=len(dfRelDiff), diffCols=len(dfRelDiff.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordDiffCols = ['Ordre Tronc Proch DCv', 'Ordre Tronc Proch Qual Equi 1',\n",
    "               'Ordre Tronc Proch Qual Equi Chi2+', 'Ordre Tronc Proch Qual Equi KS+', 'Ordre Tronc Proch Qual Equi DCv+',\n",
    "               'Ordre Global Qual Equi Chi2+',\n",
    "               'Ordre Global Qual Equi KS+', 'Ordre Global Qual Equi DCv+', 'Ordre Global DeltaAIC Chi2 KS DCv']\n",
    "assert set(dfRelDiff.columns) == set(ordDiffCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absCompCols = ['Analyse'] + ordDiffCols\n",
    "dfAbsDiff = dfRes[absCompCols].set_index('Analyse').sort_index() \\\n",
    "              .compare(dfRefRep[absCompCols].set_index('Analyse').sort_index())\n",
    "dfAbsDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordr = 'Ordre Tronc Proch DCv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAbsDiff.loc[(dfAbsDiff[(ordr, 'self')] - dfAbsDiff[(ordr, 'other')]).notnull(), [(ordr, 'self'), (ordr, 'other')]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAbsDeltaDiff = pd.DataFrame(index=dfAbsDiff.index)\n",
    "for ordr in ordDiffCols:\n",
    "    dfAbsDeltaDiff[ordr] = dfAbsDiff[(ordr, 'self')] - dfAbsDiff[(ordr, 'other')]\n",
    "dfAbsDeltaDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAbsDeltaDiff.max().max(), dfAbsDeltaDiff.min().min(), dfAbsDeltaDiff.notnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bins = np.histogram(dfAbsDeltaDiff.values, bins=int(dfAbsDeltaDiff.max().max() - dfAbsDeltaDiff.min().min()),\n",
    "                          range=(dfAbsDeltaDiff.min().min(), dfAbsDeltaDiff.max().max()))\n",
    "hist, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(nAnlys=len(dfRes), nDiffAnlys=len(dfAbsDiff),\n",
    "     pctDiffAnlys=round(100*len(dfAbsDiff)/len(dfRes), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn = dossier / f'{nomEtude}{sousEtude}-autofilsor-indicators-diffs.xlsx'\n",
    "with pd.ExcelWriter(fpn) as xlsWrtr:\n",
    "    dfRelDiff.to_excel(xlsWrtr, sheet_name='rel-diff')\n",
    "    dfAbsDiff.to_excel(xlsWrtr, sheet_name='abs-diff')\n",
    "    \n",
    "fpn.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Debug: Internals of _postComputeFilterSortKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class RS(ads.MCDSTruncOptanalysisResultsSet):\n",
    "#    \n",
    "#    CLNObs = 'NObs'\n",
    "#    CLNTotObs = 'NTot Obs'\n",
    "#    CLNTotPars = 'NbTot Pars'\n",
    "#    CLChi2  = 'Chi2 P'\n",
    "#    CLDCv   = 'CoefVar Densité'\n",
    "#    CLKS    = 'KS P'\n",
    "#    CLCvMUw = 'CvM Uw P'\n",
    "#    CLCvMCw = 'CvM Cw P'\n",
    "#    \n",
    "#    def __init__(self):\n",
    "#        pass\n",
    "#\n",
    "#rs = RS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.AutoFilSorKeySchemes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRes2RefRepCols = {v:k for k, v in DRefRep2ResCols.items()}\n",
    "DRes2RefRepCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "lblSamp = 0\n",
    "dfSampRes = dfRes[dfRes.Echant == lblSamp].copy()\n",
    "\n",
    "scheme = results.AutoFilSorKeySchemes[6]\n",
    "print('group' in scheme, scheme)\n",
    "\n",
    "# Sort results\n",
    "dfSampRes.sort_values(by=results.transColumns(scheme['sort'], 'fr'), ascending=scheme['ascend'], \n",
    "                      na_position=scheme.get('napos', 'last'), inplace=True)\n",
    "dfSampRes.set_index('Analyse', inplace=True)\n",
    "\n",
    "# Compute order (specific to groups or global).\n",
    "if 'group' in scheme:\n",
    "    sSampOrder = dfSampRes.groupby(results.transColumns(scheme['group'], 'fr'), dropna=False).cumcount()\n",
    "else:\n",
    "    sSampOrder = pd.Series(data=range(len(dfSampRes)), index=dfSampRes.index)\n",
    "\n",
    "sSampOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old report, new method\n",
    "lblSamp = 0\n",
    "dfSampRep = dfRefRep[dfRefRep.Echant == lblSamp].rename(columns=DRes2RefRepCols).copy()\n",
    "\n",
    "optimTruncCol = 'OptimTrunc'\n",
    "#scheme = dict(name='Meil CKCv Tronc Proch',  # Meilleur Chi2&KS&DCV par groupe de troncatures proches\n",
    "#              sort=[optimTruncCol, 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "#                    'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx'],\n",
    "#              ascend=[True, True, True, False, False, True, False, True],\n",
    "#              group=[optimTruncCol, 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte'])\n",
    "scheme= dict(name='Meil Qual Chi2 Tronc Proch',  # Meilleur Qualité combinée Chi2+ par groupe de troncatures proches\n",
    "             sort=[optimTruncCol, 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "                   'Qual Chi2'],\n",
    "             ascend=[True, True, True, False],\n",
    "             group=[optimTruncCol, 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte'])\n",
    "\n",
    "print('group' in scheme, scheme)\n",
    "\n",
    "# Sort results\n",
    "dfSampRep.sort_values(by=scheme['sort'], ascending=scheme['ascend'], \n",
    "                      na_position=scheme.get('napos', 'last'), inplace=True)\n",
    "dfSampRep.set_index('Analyse', inplace=True)\n",
    "\n",
    "# Compute order (specific to groups or global).\n",
    "if 'group' in scheme:\n",
    "    sRepSampOrder = dfSampRep.groupby(scheme['group'], dropna=False).cumcount()\n",
    "else:\n",
    "    sRepSampOrder = pd.Series(data=range(len(dfSampRep)), index=dfSampRep.index)\n",
    "\n",
    "sRepSampOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp = sRepSampOrder.to_frame(name='rep').join(sSampOrder.to_frame(name='res')).sort_index()\n",
    "dfComp[dfComp.res != dfComp.rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce float resolution (pb with least significant bits ?)\n",
    "dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']] = \\\n",
    "    dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "               'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].astype(float)\n",
    "\n",
    "# Sort results\n",
    "dfSampRep = dfSampRep.sort_values(by=scheme['sort'], ascending=scheme['ascend'], \n",
    "                                  na_position=scheme.get('napos', 'last'))\n",
    "\n",
    "# Compute order (specific to groups or global).\n",
    "sRepSampOrder = dfSampRep.groupby(scheme['group'], dropna=False).cumcount() \\\n",
    "                 if 'group' in scheme else range(len(dfSampRep))\n",
    "\n",
    "sRepSampOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSampRes[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']] = \\\n",
    "    dfSampRes[['OptimTrunc', 'Groupe Tronc Gche', 'Groupe Tronc Drte',\n",
    "               'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].astype(float)\n",
    "\n",
    "# Sort results\n",
    "dfSampRes = dfSampRes.sort_values(by=results.transColumns(scheme['sort'], 'fr'), ascending=scheme['ascend'], \n",
    "                                  na_position=scheme.get('napos', 'last'))\n",
    "\n",
    "# Compute order (specific to groups or global).\n",
    "sSampOrder = dfSampRes.groupby(results.transColumns(scheme['group'], 'fr'), dropna=False).cumcount() \\\n",
    "             if 'group' in scheme else range(len(dfSampRes))\n",
    "\n",
    "sSampOrder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp = sRepSampOrder.to_frame(name='rep').join(sSampOrder.to_frame(name='res')).sort_index()\n",
    "dfComp[dfComp.res != dfComp.rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].to_excel('tmp/_.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSampRes[['OptimTrunc', 'Groupe Tronc Gche', 'Groupe Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].to_excel('tmp/__.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSampRes[['OptimTrunc', 'Groupe Tronc Gche', 'Groupe Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].rename(columns=DRes2RefRepCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "           'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp2 = dfSampRes[['OptimTrunc', 'Groupe Tronc Gche', 'Groupe Tronc Drte',\n",
    "                     'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']] \\\n",
    "             .rename(columns=DRes2RefRepCols).sort_index() \\\n",
    "             .compare(dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "                                 'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].sort_index())\n",
    "dfComp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp2[('Chi2 P', 'self')] - dfComp2[('Chi2 P', 'other')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp3 = dfSampRes[['OptimTrunc', 'Groupe Tronc Gche', 'Groupe Tronc Drte',\n",
    "                     'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].astype(np.float32) \\\n",
    "  .rename(columns=DRes2RefRepCols).sort_index() \\\n",
    "  .compare(dfSampRep[['OptimTrunc', 'Grp Dist Tronc Gche', 'Grp Dist Tronc Drte',\n",
    "                      'Chi2 P', 'KS P', 'CoefVar Densité', 'NObs', 'CodEx']].astype(np.float32).sort_index())\n",
    "dfComp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfComp4 = dfSampRes[ordDiffCols].sort_index().compare(dfSampRep.rename(columns=DRefRep2ResCols)[ordDiffCols].sort_index())\n",
    "dfComp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Quality and non-regression of filterSort*\n",
    "\n",
    "(prerequisite: run a.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplication algorithm params.\n",
    "R = results\n",
    "\n",
    "miDupSubsetDef = pd.MultiIndex.from_tuples([R.CLNObs, R.CLEffort, R.CLDeltaAic,\n",
    "                                            R.CLChi2, R.CLKS, R.CLCvMUw, R.CLCvMCw, R.CLDCv, \n",
    "                                            R.CLPDetec, R.CLPDetecMin, R.CLPDetecMax,\n",
    "                                            R.CLDensity, R.CLDensityMin, R.CLDensityMax])\n",
    "dDupRoundsDef = {R.CLDeltaAic: 1, R.CLChi2: 2, R.CLKS: 2, R.CLCvMUw: 2, R.CLCvMCw: 2, R.CLDCv: 2, \n",
    "                 R.CLPDetec: 3, R.CLPDetecMin: 3, R.CLPDetecMax: 3, R.CLDensity: 2, R.CLDensityMin: 2, R.CLDensityMax: 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index columns for comparison\n",
    "anlysParamCols = ['Fn Clé Mod', 'Sér Ajust Mod', 'Dist Tronc Gche', 'Dist Tronc Drte', 'Tranch Dist Mod']\n",
    "indexCols = [sampleNumCol] + sampleSelCols + [anlysIndCol] + anlysParamCols\n",
    "', '.join(indexCols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Load reference prototype final report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repFileName = workDir / f'{nomEtude}{sousEtude}-OptAnalyses{varEtude}-raptousech.ods'\n",
    "\n",
    "print(f'Fichier choisi : {repFileName.as_posix()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ddfRefFinRep = pd.read_excel(repFileName, sheet_name=None)\n",
    "', '.join(ddfRefFinRep.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all sub-reports (1 per method) display the same columns\n",
    "assert all(ddfRefFinRep[meth].columns.to_list() == ddfRefFinRep['codexec'].columns.to_list()\n",
    "           for meth in ddfRefFinRep.keys() if meth.startswith('c') and meth != 'codexec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ref. sub-reports for comparison :\n",
    "# * Drop pre-selection columns (added later in report module)\n",
    "# * Rename columns to \"industrialised\" names\n",
    "for meth in ddfRefFinRep:\n",
    "    if meth.startswith('c'):\n",
    "        ddfRefFinRep[meth].drop(columns=['Sélection finale', 'Sélection Qual Equi'], inplace=True)\n",
    "        ddfRefFinRep[meth].rename(columns=DRefRep2ResCols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some checks ...\n",
    "df = ddfRefFinRep['codexec'].copy()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainIndicCols = ['Delta AIC', 'Chi2 P', 'KS P', 'CvM Uw P', 'CvM Cw P', 'NObs', 'NTot Obs', 'CoefVar Densité', 'NbTot Pars']\n",
    "qualIndicCols = [col for col in df.columns if col.startswith('Qual')]\n",
    "resultCols = ['Densité', 'EDR/ESW', 'PDetec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There should be no analysis with NaN values for main MCDS goodness params\n",
    "# (should be filtered out at first, just as ExecCode > 2 ones)\n",
    "df = df.loc[df[mainIndicCols].isnull().any(axis='columns'),\n",
    "            ['Analyse'] + mainIndicCols + qualIndicCols + resultCols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But there are, and seems they are all due to NaN Chi2 ...\n",
    "assert df['Chi2 P'].isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exCodeBadAnalyses = df['Analyse'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Generate report (apply methods to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterSortReportSpecs = \\\n",
    "[dict(name='ExecCode', \n",
    "      method=mtoars.filterSortOnExecCode,\n",
    "      deduplicate=dict(dupSubset=miDupSubsetDef, dDupRounds=dDupRoundsDef),\n",
    "      filterSort=dict()),\n",
    " dict(name='AicCKCvQua-r{sightRate:.1f}d{nResults}', \n",
    "      method=mtoars.filterSortOnAicCKCvQua,\n",
    "      deduplicate=dict(dupSubset=miDupSubsetDef, dDupRounds=dDupRoundsDef),\n",
    "      filterSort=dict(sightRate=92.5, nBestAIC=3, nBestQua=1, nResults=12)),\n",
    " # ... etc.\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ddfFinRep = dict()\n",
    "repLog = list()\n",
    "for spec in filterSortReportSpecs:\n",
    "\n",
    "    subRepName = spec['name'].format_map(spec['filterSort']).replace('.', '')\n",
    "    \n",
    "    iSubRep, subSteps = spec['method'](results, **spec['filterSort'], **spec['deduplicate'])\n",
    "    \n",
    "    ddfFinRep[subRepName] = results.dfTransData(lang='fr', index=iSubRep)\n",
    "    repLog += subSteps\n",
    "\n",
    "', '.join(ddfFinRep.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel.\n",
    "fpn = pl.Path('tmp') / f'{nomEtude}{sousEtude}-autofilsor-raptousech.xlsx'\n",
    "with pd.ExcelWriter(fpn) as xlsWrtr:\n",
    "    for subRepName, dfSubRepData in ddfFinRep.items():\n",
    "        dfSubRepData.to_excel(xlsWrtr, sheet_name=subRepName, index=True)            \n",
    "\n",
    "fpn.as_posix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. filterSortOnExecCode checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfRes = ddfFinRep['ExecCode'].copy()\n",
    "dfRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results columns\n",
    "resFrCols = dfRes.columns\n",
    "', '.join(resFrCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round truncation distance parameters in order to be able to use them as part of the index columns\n",
    "# (Excel I/O changed some least significant after dot figures)\n",
    "dfRes['Dist Tronc Gche'] = dfRes['Dist Tronc Gche'].round(5)\n",
    "dfRes['Dist Tronc Drte'] = dfRes['Dist Tronc Drte'].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# There should be no analysis with NaN values for main MCDS goodness params\n",
    "# (should be filtered out at first, just as ExecCode > 2 ones)\n",
    "_mainIndicCols = [col for col in mainIndicCols if col in dfRes.columns]\n",
    "_qualIndicCols = [col for col in qualIndicCols if col in dfRes.columns]\n",
    "_resultCols = [col for col in resultCols if col in df.columns]\n",
    "\n",
    "df = dfRes.loc[dfRes[_mainIndicCols].isnull().any(axis='columns'), ['Analyse'] + _mainIndicCols + _qualIndicCols + _resultCols]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But there are, and seems they are all due to NaN Chi2 ...\n",
    "assert df['Chi2 P'].isnull().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And seems they are not all the same ones as in Ref report\n",
    "len(set(df['Analyse']) - set(exCodeBadAnalyses)), len(set(exCodeBadAnalyses) - set(df['Analyse']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference sub-report and check that all its columns are also in the \"industrialised\" sub-report\n",
    "dfRefRep = ddfRefFinRep['codexec']\n",
    "assert all(col in resFrCols for col in dfRefRep.columns)\n",
    "\n",
    "', '.join(dfRefRep.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round truncation distance parameters in order to be able to use them as part of the index columns\n",
    "# (Excel I/O changed some least significant after dot figures)\n",
    "dfRefRep['Dist Tronc Gche'] = dfRefRep['Dist Tronc Gche'].round(5)\n",
    "dfRefRep['Dist Tronc Drte'] = dfRefRep['Dist Tronc Drte'].round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to compare: let's ignore ...\n",
    "# * DeltaAIC: depends on actual analyses sets done at once, may be differing from ref to actual results,\n",
    "# * other duplicate columns (analysis params)\n",
    "subsetCols = [col for col in dfRefRep.columns\n",
    "              if col not in indexCols + ['Delta AIC', 'FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Comparaison => bilan = 28% d'analyses non présentes dans les 2 rapports ou à valeurs différentes\n",
    "# C'est assez cohérent avec les 21% présentant des différences de classement par ordre d'indicateurs ...\n",
    "dfRelDiff = ads.DataSet.compareDataFrames(dfRefRep, dfRes, indexCols=indexCols, subsetCols=subsetCols,\n",
    "                                          dropCloser=6, dropNans=True, dropCloserCols=True)\n",
    "#assert len(dfRelDiff) == 0\n",
    "dict(refRows=len(dfRefRep), resRows=len(dfRes), diffRows=len(dfRelDiff), diffCols=len(dfRelDiff.columns),\n",
    "     pctDiff=round(100 * len(dfRelDiff) / len(dfRes), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dfRelDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRelDiff.to_excel('tmp/codexec-reldiff.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List diffing analyses that are not in the 2 reports\n",
    "def allNullOrZero(s):\n",
    "    return sum(s.isnull() | (s == 0)) == len(s)\n",
    "\n",
    "df = dfRelDiff.loc[dfRelDiff.apply(allNullOrZero, axis='columns')]\n",
    "print('pctNotBoth:', round(100 * len(df) / len(dfRes), 1))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove them => remains those in the 2 reports, but with diffs\n",
    "df = dfRelDiff.drop(dfRelDiff.loc[dfRelDiff.apply(allNullOrZero, axis='columns')].index)\n",
    "print('pctBothButDiffs:', round(100 * len(df) / len(dfRes), 1))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove bad analyses (with NaN in main MCDS results)\n",
    "df = df.dropna(axis='index', how='any', subset=[col for col in _mainIndicCols if col in df.columns])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove columns with rel diffs lower than 1e-6 all along\n",
    "df = df.drop(columns=[col for col in df.columns if df[col].gt(6).all()])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And there only remains 'Ordre *' columns\n",
    "assert all(col.startswith('Ordre') for col in df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the end good news : differences are only due to :\n",
    "# * see above non-regression tests on enriched results/reports :\n",
    "#    (only) partially explained order => different lists of analyses (20-25%)\n",
    "# * prototype bug that keeps in the race ... bad analyses with NaN as main MCDS goodness indicator values (Chi2 ... etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('tmp/codexec-reldiff-both-but-nan-or-diff6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. filterSortOnAicCKCvQua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Apply filter and sort method\n",
    "sightRate = 92.5\n",
    "nBestAIC = 3\n",
    "nBestQua = 1\n",
    "nResults = 12\n",
    "\n",
    "iFilSorRes, filSorSteps = results.filterSortOnAicCKCvQua(sightRate=sightRate, nBestAIC=nBestAIC,\n",
    "                                                         nBestQua=nBestQua, nResults=nResults,\n",
    "                                                         dupSubset=miDupSubsetDef, dDupRounds=dDupRoundsDef)\n",
    "iFilSorRes, filSorSteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract filtered and sorted results and translate columns\n",
    "dfFSAicCKCvQua = results.dfTransData(lang='fr', index=iFilSorRes)\n",
    "dfFSAicCKCvQua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Quality tests of (industrialised) filterSort*\n",
    "\n",
    "(prerequisite: run a.)\n",
    "\n",
    "1. load results with the real industrialised MCDSTruncationOptanalysisResultsSet\n",
    "2. apply filter-sort methods\n",
    "3. check output quality\n",
    "    * TODO: define what to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddfSubRep['ExecCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddfSubRep['AicCKCvQua-r925d12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Integration tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweak trace levels.\n",
    "ads.logger('ads.eng', level=ads.INFO, reset=True)\n",
    "if False:\n",
    "    ads.logger('ads.dat', level=ads.DEBUG, reset=True)\n",
    "    ads.logger('ads.opr', level=ads.DEBUG, reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MCDSAnalyser : Run multiple analyses on real-life data (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Individualised data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countCols =  ['nMalAd10', 'nAutAd10', 'nMalAd5', 'nAutAd5']\n",
    "\n",
    "def count2AdultCat(sCounts):\n",
    "    return 'm' if 'Mal' in sCounts[sCounts > 0].index[0] else 'a'\n",
    "\n",
    "def count2DurationCat(sCounts):\n",
    "    return '5mn' if '5' in sCounts[sCounts > 0].index[0] else '10mn'\n",
    "\n",
    "fds = ads.FieldDataSet(source='refin/ACDC2019-Naturalist-ExtraitObsBrutesAvecDist.txt',\n",
    "                       importDecFields=['distMem'], countCols=countCols,\n",
    "                       addMonoCatCols={ 'Adulte': count2AdultCat, 'Durée': count2DurationCat })\n",
    "\n",
    "dfObsIndiv = fds.individualise()\n",
    "\n",
    "dfObsIndiv.drop(columns=countCols, inplace=True)\n",
    "\n",
    "dfObsIndiv.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.Explicit analysis specs\n",
    "\n",
    "(old method: manual explicitation before run, and pass explict specs to run ;\n",
    " see 2/2 below for the new simpler and recommended method, without prior explicitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectPlaceCol = 'Point'\n",
    "transectPlaceCols = [transectPlaceCol]\n",
    "passIdCol = 'Passage'\n",
    "effortCol = 'Effort'\n",
    "\n",
    "sampleDistCol = 'distMem'\n",
    "sampleDecCols=[effortCol, sampleDistCol]\n",
    "\n",
    "sampleSelCols = ['Espèce', passIdCol, 'Adulte', 'Durée']\n",
    "sampleIndCol = 'IndSamp'\n",
    "\n",
    "varIndCol = 'IndAnlys'\n",
    "anlysAbbrevCol = 'AbrevAnlys'\n",
    "\n",
    "withTruncCol = 'AvecTronc'\n",
    "\n",
    "dSurveyArea = dict(Zone='ACDC', Surface='2400')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysExplSpecs = ads.DSAnalyser.explicitVariantSpecs('refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx', \n",
    "                                                        keep=['Echant1_impl', 'Echant2_impl', 'Modl_impl',\n",
    "                                                              'Params1_expl', 'Params2_expl'],\n",
    "                                                        varIndCol='IndAnlys',\n",
    "                                                        #convertCols={ 'Durée': int }, # float 'cause of Excel\n",
    "                                                        computedCols={anlysAbbrevCol: analysisAbbrev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a pass-through neutral column (for richer results)\n",
    "dfAnlysExplSpecs[withTruncCol] = dfAnlysExplSpecs[['TrGche', 'TrDrte']].apply(lambda s: s.isnull().all(), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shorten analyses list to go faster\n",
    "if False:\n",
    "    dfAnlysExplSpecs = dfAnlysExplSpecs[(dfAnlysExplSpecs['Espèce'].isin(['Luscinia megarhynchos', 'Turdus merula']))]\n",
    "    len(dfAnlysExplSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysExplSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Objet MCDSAnalyser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the MCDSAnalyser object\n",
    "# * const effort per survey point x pass (= 1) => no need for passing transects infos (auto-generated)\n",
    "anlysr = ads.MCDSAnalyser(dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea,\n",
    "                          resultsHeadCols=dict(before=[varIndCol], sample=sampleSelCols, after=[withTruncCol, anlysAbbrevCol]),\n",
    "                          transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                          sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, anlysSpecCustCols=[withTruncCol],\n",
    "                          distanceUnit='Meter', areaUnit='Hectare',\n",
    "                          surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                          abbrevCol=anlysAbbrevCol, anlysIndCol=varIndCol, sampleIndCol=sampleIndCol,\n",
    "                          workDir='tmp/mcds-anlr', runMethod='subprocess.run', logProgressEvery=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(anlysr.specs) == 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.specs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Check analyses specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysExplSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(dfExplParamSpecs=dfAnlysExplSpecs, dropDupes=True, check=True)\n",
    "\n",
    "print(verdict, reasons, len(dfAnlysExplSpecs), userParamSpecCols, intParamSpecCols, unmUserParamSpecCols)\n",
    "\n",
    "assert len(dfAnlysExplSpecs) == 48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysExplSpecs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Run analyses\n",
    "\n",
    "(parallel mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 4.9s, 5.2s (Lenovo T490 = 4-core i5-8350U with PCI-e SSD)\n",
    "\n",
    "# Analyses\n",
    "results = anlysr.run(dfAnlysExplSpecs, threads=12)\n",
    "\n",
    "#results = anlysr.run(dfAnlysExplSpecs.iloc[:2], threads=1)  # Petit sous-ensemble pour aller vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert withTruncCol in results.dfTransData('fr').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(pl.Path(anlysr.workDir) / 'unintst-mcds-anlyser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MCDSAnalyser : Run multiple analyses on real-life data (2/2)\n",
    "\n",
    "(2nd, easier and recommended version, with analysis specs checks and auto-detection of analysis parameter columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Individualised data set and analysis specs abbreviator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run [1. MCDSAnalyser : Run multiple analyses on real-life data (1/2)](#1.-MCDSAnalyser-%3A-Run-multiple-analyses-on-real-life-data-(1%2F2)) / a. and b. before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Build MCDSAnalyser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction d'un MCDSAnalyser\n",
    "# * effort constant par point x passage (= 1) => pas besoin de passer les infos transects (auto-générées)\n",
    "anlysr = ads.MCDSAnalyser(dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea,\n",
    "                          transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                          sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols,\n",
    "                          abbrevCol=anlysAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                          anlysIndCol=varIndCol, sampleIndCol=sampleIndCol,\n",
    "                          distanceUnit='Meter', areaUnit='Hectare',\n",
    "                          surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                          resultsHeadCols=dict(before=[varIndCol], sample=sampleSelCols, after=[anlysAbbrevCol]),\n",
    "                          workDir='tmp/mcds-anlr', runMethod='subprocess.run', logProgressEvery=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Check (and explicitate) analyses specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysSpecFile = 'refin/ACDC2019-Naturalist-ExtraitSpecsAnalyses.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAnlysExplSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    anlysr.explicitParamSpecs(implParamSpecs=anlysSpecFile, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfAnlysExplSpecs) == 48\n",
    "assert userParamSpecCols == ['FonctionClé', 'SérieAjust', 'TrGche', 'TrDrte', 'NbTrchMod']\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'MinDist', 'MaxDist', 'FitDistCuts']\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run analyses\n",
    "\n",
    "(parallel mode, and straight from implicit specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 5.2s\n",
    "\n",
    "# Analyses (on a tout vérifié : go).\n",
    "results = anlysr.run(implParamSpecs=anlysSpecFile, threads=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(pl.Path(anlysr.workDir) / 'unintst-mcds-anlyser-results2-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MCDSPreAnalyser : Run multiple pre-analyses with real-life data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not implemented (See `valtest` notebook, chapter VIII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MCDSZerothOrderTruncationOptimiser : Optimise truncation params on real-life data\n",
    "\n",
    "Note: Only from explicit specs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Jeu de données individualisées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run [2. MCDSAnalyser : Run multiple analyses on real-life data (2/2)](#2.-MCDSAnalyser-%3A-Run-multiple-analyses-on-real-life-data-(2%2F2)) / a., b. and c. before (need for dfObsIndiv & dfAnlysExplSpecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anlysr.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Specs d'optimisation explicites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optIndCol = 'IndOptim'\n",
    "optAbbrevCol = 'AbrevOptim'\n",
    "speAbbrevCol = 'AbrevEsp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left part = standard analysis params withouth truncation specs, from 4. above\n",
    "dfOptimExplSpecs = dfAnlysExplSpecs[sampleSelCols + ['FonctionClé', 'SérieAjust']].drop_duplicates().reset_index(drop=True)\n",
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right part : as many as possible truncation optimisation params combinations\n",
    "dfMoreOptimCols = pd.DataFrame(dict(CritChx=[None, 'AIC']*6,\n",
    "                                    IntervConf=[None, 95, 97]*4,\n",
    "                                    TroncGche=['auto', None, 20, 'dist(5, 30)', 50.0, 'quant(3)']*2,\n",
    "                                    TroncDrte=[None, 'auto', 'dist(150, 300)', 200.0, 'tucquant(2)', 250]*2,\n",
    "                                    MethOutliers=[None, 'auto', None, None,\n",
    "                                                  None, 'quant(6)', None, None,\n",
    "                                                  None, 'tucquant(8)', None, None],\n",
    "                                    NbTrModel=[None, 9.0, 'auto', 17, 'abs(5, 10)', 'mult(0.5,5/4)']*2,\n",
    "                                    NbTrDiscr=[None, 'auto', 4, 'abs(5, 10)', 16.0, 'mult(0.5,5/4)']*2,\n",
    "                                    ExprOpt=[None, 'max(chi2)', 'min(1-chi2)', 'max(chi2)',\n",
    "                                             'max(ks)', 'max(cvmuw*cvmcw)']*2,\n",
    "                                    MoteurOpt=[None, 'zoopt', 'zoopt(mxi=20, a=racos)',\n",
    "                                               'zoopt(mxi=30, mxr=2, tv=0.5)']*3,\n",
    "                                    ParExec=[None, 'times(2)', 'times(3, b=2)']*4))\n",
    "dfMoreOptimCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat left and right parts\n",
    "dfOptimExplSpecs = pd.concat([dfOptimExplSpecs, dfMoreOptimCols], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add neutral and path-through columns (from specs to results) : no real use, but for testing this usefull feature\n",
    "dfOptimExplSpecs[speAbbrevCol] = dfOptimExplSpecs['Espèce'].apply(lambda s: ''.join(m[:4] for m in s.split()))\n",
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificially generate some duplicates (for testing auto-removal later :-)\n",
    "dfOptimExplSpecs = dfOptimExplSpecs.append(dfOptimExplSpecs, ignore_index=True)\n",
    "len(dfOptimExplSpecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. MCDSZerothOrderTruncationOptimiser object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes de dfOptimExplSpecs donnant les paramètres d'analyse / optimisation\n",
    "optimParamsSpecsCols  = ['FonctionClé', 'SérieAjust', 'CritChx', 'IntervConf',\n",
    "                         'TroncGche', 'TroncDrte', 'MethOutliers', 'NbTrModel', 'NbTrDiscr',\n",
    "                         'ExprOpt', 'MoteurOpt', 'ParExec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoptr = ads.MCDSZerothOrderTruncationOptimiser \\\n",
    "                (dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea, \n",
    "                 transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                 sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                 anlysSpecCustCols=[speAbbrevCol], abbrevCol=optAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                 anlysIndCol=optIndCol, sampleIndCol=sampleIndCol,\n",
    "                 distanceUnit='Meter', areaUnit='Hectare',\n",
    "                 surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                 resultsHeadCols=dict(before=[optIndCol], sample=sampleSelCols, after=optimParamsSpecsCols + [speAbbrevCol]),\n",
    "                 workDir='tmp/mcds-optr', runMethod='os.system', runTimeOut=None,\n",
    "                 logData=False, logProgressEvery=1, backupEvery=5,\n",
    "                 defEstimKeyFn='HAZ', defEstimAdjustFn='POLY', defEstimCriterion='AIC', defCVInterval=93,\n",
    "                 defExpr2Optimise='1-ks', defMinimiseExpr=True,\n",
    "                 defOutliersMethod='quant', defOutliersQuantCutPct=5.5,\n",
    "                 defFitDistCutsFctr=dict(min=1/2, max=4/3), defDiscrDistCutsFctr=dict(min=1/2, max=1.2),\n",
    "                 defSubmitTimes=4, defSubmitOnlyBest=1,\n",
    "                 defCoreMaxIters=45, defCoreTermExprValue=0.2, defCoreMaxRetries=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Vérification des specs d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOptimExplSpecs, userParamSpecCols, intParamSpecCols, unmUserParamSpecCols, verdict, reasons = \\\n",
    "    zoptr.explicitParamSpecs(dfExplParamSpecs=dfOptimExplSpecs, dropDupes=True, check=True)\n",
    "\n",
    "assert len(dfOptimExplSpecs) == 12\n",
    "assert userParamSpecCols == optimParamsSpecsCols\n",
    "assert intParamSpecCols == ['EstimKeyFn', 'EstimAdjustFn', 'EstimCriterion', 'CvInterval',\n",
    "                            'MinDist', 'MaxDist', 'OutliersMethod', 'FitDistCuts', 'DiscrDistCuts',\n",
    "                            'Expr2Optimise', 'OptimisationCore', 'SubmitParams']\n",
    "assert unmUserParamSpecCols == []\n",
    "assert verdict\n",
    "assert not reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfOptimExplSpecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Exécution des optimisations\n",
    "\n",
    "(en parallèle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Lenovo T490 (4-core i5-8350U with PCI-e SSD)\n",
    "# date ?: 12 optimisations, 1430 analyses, 12 threads : subprocess = 3mn13, system = 2mn35, 1mn54\n",
    "# 2021-08-08: idem : system 4mn38\n",
    "\n",
    "results = zoptr.run(dfOptimExplSpecs, threads=12)\n",
    "\n",
    "#results = zoptr.run(dfOptimExplSpecs.iloc[:3], threads=3)  # Petit sous-ensemble pour aller vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoptr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert speAbbrevCol in results.dfTransData('fr').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.toExcel(pl.Path(zoptr.workDir) / 'unintst-mcds-optimiser-results-fr.xlsx', lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f. Recovery : Run again optimisations, but from the last backup\n",
    "\n",
    "(use case: crash, or mandatory/auto reboot of computer in the middle of a long optimisation run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check presence, mtime and content (optims Id lists) of $workDir/optr-resbak-*.pickle.xz\n",
    "#with lzma.open(fileName, 'rb') as file:\n",
    "#    dfData, specs = pickle.load(file)\n",
    "#    \n",
    "#len(dfData), dfData.columns, len(dfData.columns), dfData.columns.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimiser object : have to be a clone of the one whose execution that was backed up\n",
    "zoptr = ads.MCDSZerothOrderTruncationOptimiser \\\n",
    "                (dfObsIndiv, effortConstVal=1, dSurveyArea=dSurveyArea, \n",
    "                 transectPlaceCols=transectPlaceCols, passIdCol=passIdCol, effortCol=effortCol,\n",
    "                 sampleSelCols=sampleSelCols, sampleDecCols=sampleDecCols, sampleDistCol=sampleDistCol,\n",
    "                 anlysSpecCustCols=[speAbbrevCol], abbrevCol=optAbbrevCol, abbrevBuilder=analysisAbbrev,\n",
    "                 anlysIndCol=optIndCol, sampleIndCol=sampleIndCol,\n",
    "                 distanceUnit='Meter', areaUnit='Hectare',\n",
    "                 surveyType='Point', distanceType='Radial', clustering=False,\n",
    "                 resultsHeadCols=dict(before=[optIndCol], sample=sampleSelCols, after=optimParamsSpecsCols + [speAbbrevCol]),\n",
    "                 workDir='tmp/mcds-optr', logProgressEvery=1,\n",
    "                 defEstimKeyFn='HAZ', defEstimAdjustFn='POLY', defEstimCriterion='AIC', defCVInterval=93,\n",
    "                 defExpr2Optimise='1-ks', defMinimiseExpr=True,\n",
    "                 defOutliersMethod='quant', defOutliersQuantCutPct=5.5,\n",
    "                 defFitDistCutsFctr=dict(min=1/2, max=4/3), defDiscrDistCutsFctr=dict(min=1/2, max=1.2),\n",
    "                 defSubmitTimes=4, defSubmitOnlyBest=1,\n",
    "                 defCoreMaxIters=45, defCoreTermExprValue=0.2, defCoreMaxRetries=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run optimisation with recovery results ... using exact same optim. specs (MANDATORY)\n",
    "results2 = zoptr.run(dfOptimExplSpecs, recover=True, threads=12)\n",
    "\n",
    "#results2 = zoptr.run(dfOptimExplSpecs.iloc[:3], recover=True, threads=3)  # Petit sous-ensemble pour aller vite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zoptr.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check equality of 1st 10 results in `results` and `results2`, + added num of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MCDSTruncationOptAnalyser : Run multiple analyses with optimised truncation params, on real-life data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not implemented (See `valtest` notebook, chapter VIII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe A. Mise au point du code du module autods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection de Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance software detection params.\n",
    "DistanceSuppVers = [7, 6] # Lastest first.\n",
    "DistancePossInstPaths = [pl.Path().resolve(), pl.Path('C:\\\\Program files (x86)'), pl.Path('C:\\\\Program files')]\n",
    "\n",
    "# Find given executable installation dir.\n",
    "def findExecutable(exeFileName):\n",
    "\n",
    "    exeFilePathName = None\n",
    "    print('Looking for {} ...'.format(exeFileName))\n",
    "    for path in DistancePossInstPaths:\n",
    "        for ver in DistanceSuppVers:\n",
    "            exeFileDir = path / 'Distance {}'.format(ver)\n",
    "            print(' - checking {} : '.format(exeFileDir), end='')\n",
    "            exeFN = exeFileDir / exeFileName\n",
    "            if not exeFN.exists():\n",
    "                print('no.')\n",
    "            else:\n",
    "                print('yes !')\n",
    "                exeFilePathName = exeFN\n",
    "                break\n",
    "        if exeFilePathName:\n",
    "            break\n",
    "\n",
    "    if exeFilePathName:\n",
    "        print('{} found in {}'.format(exeFileName, exeFileDir))\n",
    "    else:\n",
    "        raise Exception('Could not find {} ; please install Distance software (V6 or later)'.format(exeFileName))\n",
    "\n",
    "    return exeFilePathName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findExecutable('MCDS.exe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results reports styling\n",
    "\n",
    "(to stress interesting and/or important things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrSynRes = results.dfTransData('fr', columns=synthCols)\n",
    "dfTrSynRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cChrGray = '#869074'\n",
    "cBckGreen, cBckGray = '#e0ef8c', '#dae3cb'\n",
    "cSclGreen, cSclOrange, cSclRed = '#cbef8c', '#f9da56', '#fe835a' #'#f25e2d'\n",
    "scaledColors = [cSclGreen, cSclOrange, cSclRed]\n",
    "scaledColorsRvd = list(reversed(scaledColors))\n",
    "\n",
    "dExCodeColors = dict(zip([1, 2, 3], scaledColors))\n",
    "def colorExecCodes(sCodes):\n",
    "    return ['background-color: ' + dExCodeColors.get(c, dExCodeColors[3]) for c in sCodes]\n",
    "\n",
    "def scaledColorV(v, thresholds, colors): # len(thresholds) == len(colors) - 1\n",
    "    if pd.isnull(v):\n",
    "        return cBckGray\n",
    "    for ind, thresh in enumerate(thresholds):\n",
    "        if v > thresh:\n",
    "            return colors[ind]\n",
    "    return colors[-1]\n",
    "def scaledColorS(sValues, thresholds, colors):\n",
    "    return ['background-color: ' + scaledColorV(v, thresholds, colors) for v in sValues]\n",
    "\n",
    "densCVThresholds = [0.4, 0.1]\n",
    "\n",
    "dfs = dfTrSynRes \\\n",
    "        .sort_values(by=['Espèce', 'Echantillon', 'Précision', 'Durée', 'Delta AIC']) \\\n",
    "        .style \\\n",
    "        .set_precision(3) \\\n",
    "        .set_properties(subset=pd.IndexSlice[dfTrSynRes[dfTrSynRes['Delta AIC'] == 0].index, :],\n",
    "                        **{'background-color': cBckGreen}) \\\n",
    "        .apply(colorExecCodes, subset=['CodEx'], axis='columns') \\\n",
    "        .apply(scaledColorS, subset=['CoefVar Densité'], axis='columns',\n",
    "               thresholds=densCVThresholds, colors=scaledColors) \\\n",
    "        .set_properties(subset=pd.IndexSlice[dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index, :],\n",
    "                         **{'color': cChrGray}) \\\n",
    "        .where(pd.isnull, 'color: transparent')\n",
    "\n",
    "    #.format(lambda v: v if not pd.isnull(v) else '') # Détruit une partie des arrondis, auugmente la précision ???\n",
    "\n",
    "    #.set_precision(3) # Not really usable, as only for the whole frame\n",
    "\n",
    "    #.apply(lambda s: ['color: grey']*len(s), subset=pd.IndexSlice[dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index, :],\n",
    "    #       axis='index') # OK\n",
    "    \n",
    "    #.apply(lambda s: ['color: grey']*len(s), subset=dfTrSynRes[~dfTrSynRes.CodEx.isin([1, 2])].index,\n",
    "    #       axis='index') # KO\n",
    "    \n",
    "dfs.to_excel('tmp/styled-results.xlsx')\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode MCDS plots file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realRunWorkDir = pl.Path('../donnees/acdc/210118-1904/SylvAtri-b-5mn-m-haz-cos-xk2syfzw')\n",
    "[fpn.name for fpn in realRunWorkDir.iterdir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcFileName = pl.Path(realRunWorkDir, 'plots.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(srcFileName, 'r').readlines()\n",
    "lines = [line.strip() for line in lines]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itLines = iter(lines)\n",
    "chapters = list()\n",
    "for title in itLines:\n",
    "    #title = next(itLines)\n",
    "    subTitle = next(itLines)\n",
    "    xLabel = next(itLines)\n",
    "    yLabel = next(itLines)\n",
    "    xMin, xMax, yMin, yMax = [float(s) for s in next(itLines).split()]\n",
    "    nDataRows = int(next(itLines))\n",
    "    dataRows = list()\n",
    "    for l in range(nDataRows):\n",
    "        dataRows.append([float(s) for s in next(itLines).split()])\n",
    "    chapters.append(dict(title=title, subTitle=subTitle, dataRows=dataRows, #nDataRows=nDataRows,\n",
    "                         xLabel=xLabel, yLabel=yLabel, xMin=xMin, xMax=xMax, yMin=yMin, yMax=yMax))\n",
    "len(chapters), chapters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QQ-plot\n",
    "chapter = chapters[0]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(chapter['dataRows'])\n",
    "dfQqData = pd.DataFrame(data=chapter['dataRows'], columns=['If the fit was perfect ...', 'Real observations'],\n",
    "                        index=np.linspace(0.5/n, 1.0-0.5/n, n))\n",
    "dfQqData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1 : OK\n",
    "#fig = plt.figure(figsize=(16, 6))\n",
    "#axes = fig.subplots()\n",
    "#_ = dfQqData.plot(ax=axes, color=['blue', 'red'], grid=True,\n",
    "#                  xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "\n",
    "# Option 2 : OK\n",
    "axes = dfQqData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True,\n",
    "                     \n",
    "                     xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "fig = axes.figure\n",
    "\n",
    "axes.legend(['If the fit was perfect ...', 'Real observations'], fontsize=12)\n",
    "axes.set_facecolor('#f9fbf3')\n",
    "axes.figure.patch.set_facecolor('#f9fbf3')\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes.figure.savefig('tmp/mlb-qqplot.jpg', box_inches='tight')\n",
    "axes.figure.savefig('tmp/mlb-qqplot.png', box_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection probability 1\n",
    "chapter = chapters[1]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDetProbData = pd.DataFrame(data=chapter['dataRows'], \n",
    "                             columns=[chapter['xLabel'], chapter['yLabel'] + ' (sampled)', chapter['yLabel'] + ' (fitted)'])\n",
    "dfDetProbData.set_index(chapter['xLabel'], inplace=True)\n",
    "dfDetProbData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = dfDetProbData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True,\n",
    "                          xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.legend(dfDetProbData.columns, fontsize=12)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfQqData.index, y=dfQqData['If the fit was perfect ...'],\n",
    "                            name='If the fit was perfect ...', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfQqData.index, y=dfQqData['Real observations'],\n",
    "                            name='Real observations', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(x=0.09, y=0.90, bordercolor='black', borderwidth=1),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow ... VERY slooooooooow !\n",
    "fig.write_image(\"tmp/ply-qqplot.svg\")\n",
    "fig.write_image(\"tmp/ply-qqplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfDetProbData.index, y=dfDetProbData[chapter['yLabel'] + ' (sampled)'],\n",
    "                            name=chapter['yLabel'] + ' (sampled)', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfDetProbData.index, y=dfDetProbData[chapter['yLabel'] + ' (fitted)'],\n",
    "                            name=chapter['yLabel'] + ' (fitted)', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(x=0.65, y=0.85*chapter['yMax'], bordercolor='black', borderwidth=1),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 1\n",
    "chapter = chapters[2]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProdDensData = pd.DataFrame(data=chapter['dataRows'], \n",
    "                              columns=[chapter['xLabel'], chapter['yLabel'] + ' (sampled)', chapter['yLabel'] + ' (fitted)'])\n",
    "dfProdDensData.set_index(chapter['xLabel'], inplace=True)\n",
    "dfProdDensData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = dfProdDensData.plot(figsize=(16, 6), color=['blue', 'red'],\n",
    "                           xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.legend(dfProdDensData.columns, fontsize=12)\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "_ = axes.set_ylabel(chapter['yLabel'], fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotly 4\n",
    "fig = plygo.Figure()\n",
    "\n",
    "fig.add_trace(plygo.Scatter(x=dfProdDensData.index, y=dfProdDensData[chapter['yLabel'] + ' (sampled)'],\n",
    "                            name=chapter['yLabel'] + ' (sampled)', line=dict(color='blue', width=2), opacity=0.7))\n",
    "fig.add_trace(plygo.Scatter(x=dfProdDensData.index, y=dfProdDensData[chapter['yLabel'] + ' (fitted)'],\n",
    "                            name=chapter['yLabel'] + ' (fitted)', line=dict(color='red', width=2)))\n",
    "\n",
    "fig.update_layout(title=chapter['title'] + ' : ' + chapter['subTitle'],\n",
    "                  xaxis=dict(title=chapter['xLabel'], range=(chapter['xMin'], chapter['xMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  yaxis=dict(title=chapter['yLabel'], range=(chapter['yMin'], chapter['yMax']),\n",
    "                             zeroline=True, linewidth=1, linecolor='black'),\n",
    "                  legend=plygo.layout.Legend(xanchor='right', yanchor='top', bordercolor='black', borderwidth=1),\n",
    "                  #margin=plygo.layout.Margin(l=40, r=40, b=40, t=40, pad=0),\n",
    "                  shapes=[plygo.layout.Shape(type='line', x0=chapter['xMax'], y0=chapter['yMin'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax']),\n",
    "                          plygo.layout.Shape(type='line', x0=chapter['xMin'], y0=chapter['yMax'],\n",
    "                                                          x1=chapter['xMax'], y1=chapter['yMax'])],\n",
    "                  template='none')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 3, with stripplot\n",
    "chapter = chapters[6]\n",
    "chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProdDensData = pd.DataFrame(data=chapter['dataRows'], \n",
    "                              columns=[chapter['xLabel'], chapter['yLabel'] + ' (sampled)', chapter['yLabel'] + ' (fitted)'])\n",
    "dfProdDensData.set_index(chapter['xLabel'], inplace=True)\n",
    "dfProdDensData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pl.Path(realRunWorkDir, 'cmd.txt'), 'r') as cmdFile:\n",
    "    fieldsLine = next(line for line in cmdFile.readlines() if line.startswith('Fields='))\n",
    "dataCols = fieldsLine.strip('\\n;')[len('Fields='):].split(',')\n",
    "print(dataCols)\n",
    "\n",
    "dfData = pd.read_csv(pl.Path(realRunWorkDir, 'data.txt'), sep='\\t', names=dataCols)\n",
    "sDists = dfData.DISTANCE.dropna()\n",
    "sDists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import matplotlib.ticker as pltt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "axes = dfProdDensData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True, linewidth=1,\n",
    "                    xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "axes.set_ylabel(chapter['yLabel'], fontsize=12)\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "\n",
    "axes2 = axes.twinx()\n",
    "sb.stripplot(ax=axes2, x=sDists, color='green', size=8, alpha=0.4, jitter=0.3)\n",
    "\n",
    "aMTicks = axes.get_xticks()\n",
    "axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))\n",
    "axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)\n",
    "axes.grid(True, which='minor')\n",
    "\n",
    "axes.legend().remove()\n",
    "_ = axes.figure.legend(labels=list(dfProdDensData.columns) + ['Individual observations'], fontsize=12,\n",
    "                       bbox_to_anchor=(1, 1), bbox_transform=axes.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines, labels, lines2, labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "axes = dfProdDensData.plot(figsize=(16, 6), color=['blue', 'red'], grid=True,\n",
    "                           xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))\n",
    "axes.set_xlabel(chapter['xLabel'], fontsize=12)\n",
    "axes.set_ylabel(chapter['yLabel'], fontsize=12)\n",
    "axes.set_title(label=chapter['title'] + ' : ' + chapter['subTitle'], fontdict=dict(fontsize=16), pad=20)\n",
    "axes.legend(dfProdDensData.columns, fontsize=12)\n",
    "\n",
    "axes2 = axes.twinx()\n",
    "_ = sb.swarmplot(ax=axes2, x=sDists, color='green', size=8, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild distance histogram from plot data (intervals) and distance data\n",
    "sh = dfProdDensData['Probability Density (sampled)']\n",
    "bins = [0] + sh.loc[((sh.shift(-1) != sh) | (sh.shift(1) != sh)) & (sh == 0)].index.tolist()\n",
    "bins[-1] += 0.001\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.histogram(sDists, bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Dropping useless points actually doesn't save overall \"plotting\" time !!!\n",
    "_ = sh.loc[(sh.shift(-1) != sh) | (sh.shift(1) != sh)] \\\n",
    "      .plot(figsize=(16, 6), color='blue', grid=True,\n",
    "            xlim=(chapter['xMin'], chapter['xMax']), ylim=(chapter['yMin'], chapter['yMax']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme à tranches fixes (OK)\n",
    "distBinWidth = 10\n",
    "distBins = np.linspace(start=0, stop=distBinWidth * int(chapter['xMax'] / distBinWidth),\n",
    "                       num=1 + int(chapter['xMax'] / distBinWidth)).tolist()\n",
    "if distBins[-1] < chapter['xMax']:\n",
    "    distBins.append(chapter['xMax'])\n",
    "\n",
    "axes = dfData.DISTANCE.plot.hist(bins=distBins, #xmin=chapter['xMin'], xmax=chapter['xMax'],\n",
    "                                 figsize=(12, 6), fill=None, edgecolor='blue', rwidth=1.0, linewidth=1, zorder=10)\n",
    "axes.set_xlim((0, dfData.DISTANCE.max()))\n",
    "axes.set_xlabel('Distance')\n",
    "axes.grid(True, which='major', zorder=0)\n",
    "\n",
    "axes.grid(True, which='minor', zorder=0)\n",
    "aMTicks = axes.get_xticks()\n",
    "axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)\n",
    "axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme à tranches fixes => 2 tailles de tranches (???)\n",
    "distBinWidth = 40\n",
    "distBins = np.linspace(start=0, stop=distBinWidth * int(chapter['xMax'] / distBinWidth),\n",
    "                       num=1 + int(chapter['xMax'] / distBinWidth)).tolist()\n",
    "if distBins[-1] < chapter['xMax']:\n",
    "    distBins.append(chapter['xMax'])\n",
    "\n",
    "axes = dfData.DISTANCE.plot.hist(bins=distBins, #xmin=chapter['xMin'], xmax=chapter['xMax'],\n",
    "                                 figsize=(12, 6), fill=None, edgecolor='red', rwidth=1.00, linewidth=1, zorder=10)\n",
    "\n",
    "distBinWidth = 20\n",
    "distBins = np.linspace(start=0, stop=distBinWidth * int(chapter['xMax'] / distBinWidth),\n",
    "                       num=1 + int(chapter['xMax'] / distBinWidth)).tolist()\n",
    "if distBins[-1] < chapter['xMax']:\n",
    "    distBins.append(chapter['xMax'])\n",
    "\n",
    "_ = dfData.DISTANCE.plot.hist(ax=axes, bins=distBins, #xmin=chapter['xMin'], xmax=chapter['xMax'],\n",
    "                                 figsize=(12, 6), fill=None, edgecolor='green', rwidth=0.85, linewidth=1, zorder=20)\n",
    "\n",
    "distBinWidth = 10\n",
    "distBins = np.linspace(start=0, stop=distBinWidth * int(chapter['xMax'] / distBinWidth),\n",
    "                       num=1 + int(chapter['xMax'] / distBinWidth)).tolist()\n",
    "if distBins[-1] < chapter['xMax']:\n",
    "    distBins.append(chapter['xMax'])\n",
    "\n",
    "_ = dfData.DISTANCE.plot.hist(ax=axes, bins=distBins, #xmin=chapter['xMin'], xmax=chapter['xMax'],\n",
    "                                 figsize=(12, 6), fill=None, edgecolor='blue', rwidth=0.70, linewidth=1, zorder=30)\n",
    "\n",
    "axes.set_xlim((0, dfData.DISTANCE.max()))\n",
    "axes.set_xlabel('Distance')\n",
    "axes.grid(True, which='major', zorder=0)\n",
    "\n",
    "axes.grid(True, which='minor', zorder=0)\n",
    "aMTicks = axes.get_xticks()\n",
    "axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)\n",
    "axes.xaxis.set_minor_locator(pltt.MultipleLocator((aMTicks[1]-aMTicks[0])/5))\n",
    "axes.legend([f'{deltaDist} m' for deltaDist in [40, 20, 10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme à tranches fixes (KO : pas trouvé moyen forcer les ticks)\n",
    "distBinWidth = 10\n",
    "distBins = np.linspace(start=0, stop=distBinWidth * int(chapter['xMax'] / distBinWidth),\n",
    "                       num=1 + int(chapter['xMax'] / distBinWidth)).tolist()\n",
    "if distBins[-1] < chapter['xMax']:\n",
    "    distBins.append(chapter['xMax'])\n",
    "distHist, distBins = np.histogram(dfData.DISTANCE, bins=distBins)\n",
    "sDistHist = pd.Series(data=distHist, index=distBins[:-1]+distBinWidth/2)\n",
    "\n",
    "axes = sDistHist.plot.bar(figsize=(16, 4), fill=None, edgecolor='blue', width=0.9, zorder=10)\n",
    "\n",
    "#majTicksDelta = 100\n",
    "#aMajTicks = np.linspace(start=0, stop=majTicksDelta * int(chapter['xMax'] / majTicksDelta),\n",
    "#                      num=1 + int(chapter['xMax'] / majTicksDelta)).tolist()\n",
    "#axes.set_xticks(minor=False, ticks=aMajTicks)\n",
    "axes.grid(True, which='major', zorder=0)\n",
    "\n",
    "#axes.xaxis.set_minor_locator(pltt.MultipleLocator(majTicksDelta/5))\n",
    "#axes.tick_params(which='minor', grid_linestyle='-.', grid_alpha=0.6)\n",
    "#axes.grid(True, which='minor', zorder=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distHist, distBins, aMajTicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme à tranches \"optimisées\" (max(sturges, fd))\n",
    "distHist, distBins = np.histogram(dfData.DISTANCE, bins='auto', range=(0, 500))\n",
    "sDistHist = pd.Series(data=distHist, index=distBins[:-1])\n",
    "\n",
    "axes = sDistHist.plot.bar(figsize=(16, 4), fill=None, edgecolor='blue', width=0.9, zorder=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode stats (actual results) from MCDS work folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_ = implib.reload(ads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results set to store results into.\n",
    "miCustCols = pd.MultiIndex.from_tuples([('id', 'ExecCase', 'Value')])\n",
    "dfCustColTrans = \\\n",
    "    pd.DataFrame(index=miCustCols, data=dict(en=['ExecCase'], fr=['CasExec']))\n",
    "\n",
    "results = ads.MCDSAnalysisResultsSet(miCustomCols=miCustCols, dfCustomColTrans=dfCustColTrans,\n",
    "                                     distanceUnit='Meter', areaUnit='Hectare',\n",
    "                                     surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis engine\n",
    "mcds = ads.MCDSEngine(workDir='refout/dist-order-sens-min',\n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial', clustering=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process folders in engine work folder.\n",
    "for folder in os.listdir(mcds.workDir):\n",
    "    \n",
    "    # Skip folders that are not MCDS run ones.\n",
    "    folderPath = os.path.join(mcds.workDir, folder)\n",
    "    if not os.path.isdir(folderPath):\n",
    "        continue\n",
    "    if os.path.splitext(folder)[1] or 'stats.txt' not in os.listdir(folderPath):\n",
    "        print(f'Skipping {folderPath}, not an MCDS.exe run folder with a stats.txt file')\n",
    "        continue\n",
    "        \n",
    "    # Tell the engine were it has run (even it does not rember it ;-)\n",
    "    _ = mcds.setupRunFolder(forceSubFolder=folder)\n",
    "    \n",
    "    # Decode results.\n",
    "    sRes = mcds.decodeStats()\n",
    "    print()\n",
    "    \n",
    "    # Store them for later.\n",
    "    sHead = pd.Series(data=[folder], index=miCustCols)\n",
    "    results.append(sRes, sCustomHead=sHead)\n",
    "\n",
    "# Tadaaaaaaa !\n",
    "results.dfTransData('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dfTransData('en').to_excel(pl.Path('tmp', 'dist-order-sens-auto-results.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development of AnalysisResultsSet.compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.DataFrame([dict(i=1, a=1.0, b=2.0, c=np.nan),\n",
    "                    dict(i=2, a=2.0, b=3.0, c=4.0),\n",
    "                    dict(i=3, a=4.0, b=5.0, c=np.nan),\n",
    "                    dict(i=4, a=np.nan, b=6.0, c=-7.5)])\n",
    "dfl.set_index('i', inplace=True)\n",
    "dfr = pd.DataFrame([dict(i=0, a=0.0, b=np.nan, c=2.0),\n",
    "                    dict(i=2, a=2.01, b=3.0, c=np.nan),\n",
    "                    dict(i=3, a=np.nan, b=5.01, c=np.nan),\n",
    "                    dict(i=4, a=np.nan, b=6.0, c=-7.5)])\n",
    "dfr.set_index('i', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl1 = dfl.join(dfr[['a']], rsuffix='_r', how='outer').drop(columns='a_r')\n",
    "dfl1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr1 = dfr.join(dfl[['a']], rsuffix='_l', how='outer').drop(columns='a_l')\n",
    "dfr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nColLevels = dfl1.columns.nlevels\n",
    "KTmpCol = 'tmp' if nColLevels == 1 else tuple('tmp{}'.format(i) for i in range(nColLevels))\n",
    "dfd = dfl1.copy()\n",
    "for col2Diff in dfl1.columns:\n",
    "    dfd[KTmpCol] = dfr1[col2Diff]\n",
    "    dfd[col2Diff] = dfd[[col2Diff, KTmpCol]].apply(closeness, axis='columns')\n",
    "    dfd.drop(columns=[KTmpCol], inplace=True)\n",
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.loc[dfl[~dfl.index.isin(dfr.index)].index, :] = 0\n",
    "dfd.loc[dfr[~dfr.index.isin(dfl.index)].index, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropNans = True\n",
    "#sbRows2Drop = (dfd > 2 | dfd.isnull()).all(axis='columns')\n",
    "sbRows2Drop = dfd.apply(lambda s: s > 2 | s.isnull(), axis='index').all(axis='columns')\n",
    "#sbRows2Drop = dfd.applymap(lambda v: v > 2 or pd.isnull(v)).all(axis='columns')\n",
    "sbRows2Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd.drop(dfd[sbRows2Drop].index, axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr1 = dfr.append(dfl[~dfl.index.isin(dfr.index)], sort=False)\n",
    "dfr1.sort_index(inplace=True)\n",
    "dfr1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unitary tests for AnalysisResultsSet._closeness\n",
    "\n",
    "(analysis reference / actual results comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [np.nan, -np.inf,\n",
    "          -1.0e12, -1.0e5, -1.0-1e-5, -1.0, -1.0+1e-5, -1.0e-8,\n",
    "          0.0, 1.0e-8, 1.0, 1.0e5, 1.0e12, np.inf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual / reference closeness measure : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)\n",
    "# = Compute the order of magnitude that separate the difference from the absolute max. of the two values\n",
    "# The greater it is, the lower the relative difference\n",
    "#    Ex: 3 = 10**3 ratio between difference absolue max. of the two,\n",
    "#        +inf = NO difference at all,\n",
    "#        0 = bad, one of the two is 0, and the other not,\n",
    "# See unitary test below.\n",
    "def _closeness(sRefAct):\n",
    "    \n",
    "    x, y = sRefAct.to_list()\n",
    "    \n",
    "    # Special cases with 1 NaN, or 1 or more inf => all different\n",
    "    if np.isnan(x):\n",
    "        if not np.isnan(y):\n",
    "            return 0 # All different\n",
    "    elif np.isnan(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    if np.isinf(x) or np.isinf(y):\n",
    "        return 0 # All different\n",
    "    \n",
    "    # Normal case\n",
    "    c = abs(x - y)\n",
    "    if not np.isnan(c) and c != 0:\n",
    "        c /= max(abs(x), abs(y))\n",
    "    \n",
    "    return np.inf if c == 0 else round(-np.log10(c), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness = _closeness\n",
    "#closeness = ads.AnalysisResultsSet._closeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aClose = np.ndarray(shape=(len(values), len(values)))\n",
    "for r in range(len(values)):\n",
    "    for c in range(len(values)):\n",
    "        try:\n",
    "            aClose[r, c] = closeness(pd.Series([values[r], values[c]]))\n",
    "        except Exception as exc:\n",
    "            print(exc, r, c, values[r], values[c])\n",
    "pd.DataFrame(data=aClose, index=values, columns=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proximité infinie sur la diagonale (sauf pour nan et +/-inf)\n",
    "assert all(np.isnan(values[i]) or np.isinf(values[i]) or np.isinf(aClose[i, i]) for i in range(len(values))), \\\n",
    "       'Error: Inequality on the diagonal'\n",
    "\n",
    "# Pas de proximité infinie ailleurs\n",
    "assert all(r == c or not np.isinf(aClose[r, c]) for r in range(len(values)) for c in range(len(values))), \\\n",
    "       'Error: No equality should be found outside the diagonal'\n",
    "\n",
    "# Bonne proximité uniquement autour de -1\n",
    "whereClose = [i for i in range(len(values)) if abs(values[i] + 1) <= 1.0e-5]\n",
    "assert all(aClose[r, c] > 4 for r in whereClose for c in whereClose), 'Error: Unexpectedly bad closeness around -1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ancienne méthode qui ne marche pas.\n",
    "# Comparaison actual / reference : -round(log10((actual - reference) / max(abs(actual), abs(reference))), 1)\n",
    "# => Plus c'est grand, plus petite est la différence relative entre les 2\n",
    "#    Ex: 3 = facteur 10**3 entre différence et valeurs absolues ; +inf = AUCUNE différence\n",
    "#        0 = pas bon, l'un des 2 est nul n'autre pas du tout\n",
    "# Cf. tests unitaires plus bas.\n",
    "#dfRelDif = pd.DataFrame(index=dfRefRes4c.index)\n",
    "#for col in dfRefRes4c.columns:\n",
    "#    dfRelDif['NormalCases'] = ~((dfActRes4c[col].isnull() & dfRefRes4c.notnull()) \\\n",
    "#                                | (dfActRes4c[col].notnull() & dfRefRes4c.isnull()) \\\n",
    "#                                | dfActRes4c[col].notnull() | dfRefRes4c.isnull())\n",
    "#    dfRelDif[col] = abs(dfActRes4c[col] - dfRefRes4c[col])\n",
    "#    dfRelDif[col].where(dfRelDif[col].isnull() | dfRelDif[col] == 0,\n",
    "#                        dfRelDif[col] / pd.DataFrame(dict(act=dfActRes4c[col], ref=dfRefRes4c[col])).abs().max(axis='columns'),\n",
    "#                        inplace=True)\n",
    "#    dfRelDif[col].where(dfRelDif['NormalCases'], 1, inplace=True) # Force special case to \"all different\"\n",
    "#    dfRelDif.drop(columns=['NormalCases'], inplace=True)\n",
    "#    dfRelDif[col] = np.round(-np.log10(dfRelDif[col]), 1)\n",
    "#    \n",
    "#dfRelDif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate stats columns translation file\n",
    "\n",
    "(from documentation stats & modules specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgtTransFileName = 'tmp/stat-mod-trans.auto.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(object):\n",
    "    \n",
    "    def __init__(self, dTrans, lang='en'):\n",
    "        assert 'en' in dTrans, 'At least \"en\" translation must be defined'\n",
    "        self.dTrans = dTrans\n",
    "        self.setLang(lang)\n",
    "        \n",
    "    def setLang(self, lang):\n",
    "        self.lang = lang.lower()\n",
    "        assert self.lang in ['en', 'fr'], 'No support for \"{}\" language'.format(lang)\n",
    "        \n",
    "    def __call__(self, s):\n",
    "        return self.dTrans.get(self.lang, self.dTrans['en']).get(s, self.dTrans['en'].get(s, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFigureTrans = \\\n",
    "    dict(en=dict(Value='', Cv='CoefVar', Lcl='Min', Ucl='Max', Df='DoF'),\n",
    "         fr=dict(Value='', Cv='CoefVar', Lcl='Min', Ucl='Max', Df='DegLib'))\n",
    "\n",
    "figtr = Translator(DFigureTrans, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DStatisticTrans = \\\n",
    "    dict(en={ 'number of observations (n)': 'NObs',\n",
    "              'number of samples (k)': 'NSamp',\n",
    "              'effort (L or K or T)': 'Effort',\n",
    "              'encounter rate (n/L or n/K or n/T)': 'EncRate',\n",
    "              'left truncation distance': 'LeftTruncDist',\n",
    "              'right truncation distance (w)': 'RightTruncDist',\n",
    "              'total number of parameters (m)': 'TotNumPars',\n",
    "              'AIC value': 'AIC',\n",
    "              'chi-square test probability (distance set 1)': 'Chi2 P 1',\n",
    "              'chi-square test probability (distance set 2)': 'Chi2 P 2',\n",
    "              'chi-square test probability (distance set 3)': 'Chi2 P 3',\n",
    "              'f(0) or h(0)': 'f/h(0)',\n",
    "              'probability of detection (Pw)': 'PDetec',\n",
    "              'effective strip width (ESW) or effective detection radius (EDR)': 'EDR/ESW',\n",
    "              'AICc': 'AICc',\n",
    "              'BIC': 'BIC',\n",
    "              'Log likelihood': 'LogLhood',\n",
    "              'Kolmogorov-Smirnov test probability': 'KS P',\n",
    "              'Cramér-von Mises (uniform weighting) test probability': 'CvM Uw P',\n",
    "              'Cramér-von Mises (cosine weighting) test probability': 'CvM Cw P',\n",
    "              'key function type': 'KeyFn',\n",
    "              'adjustment series type': 'AdjSer',\n",
    "              'number of key function parameters (NKP)': 'NumKFnPars',\n",
    "              'number of adjustment term parameters (NAP)': 'NumASerPars',\n",
    "              'number of covariate parameters (NCP)': 'NumCovars',\n",
    "              'estimated value of A(1) adjustment term parameter': 'EstA(1)',\n",
    "              'estimated value of A(2) adjustment term parameter': 'EstA(2)',\n",
    "              'estimated value of A(3) adjustment term parameter': 'EstA(3)',\n",
    "              'estimated value of A(4) adjustment term parameter': 'EstA(4)',\n",
    "              'estimated value of A(5) adjustment term parameter': 'EstA(5)',\n",
    "              'estimated value of A(6) adjustment term parameter': 'EstA(6)',\n",
    "              'estimated value of A(7) adjustment term parameter': 'EstA(7)',\n",
    "              'estimated value of A(8) adjustment term parameter': 'EstA(8)',\n",
    "              'estimated value of A(9) adjustment term parameter': 'EstA(9)',\n",
    "              'estimated value of A(10) adjustment term parameter': 'EstA(10)',\n",
    "              'average cluster size': 'AvgClustSz',\n",
    "              'size-bias regression correlation (r)': 'SzBias RegCorr',\n",
    "              'p-value for correlation significance (r-p)': 'CorSignPVal',\n",
    "              'estimate of expected cluster size corrected for size bias': 'EstExpFixedCluSz',\n",
    "              'density of clusters (or animal density if non-clustered)': 'DensClu',\n",
    "              'density of animals': 'Density',\n",
    "              'number of animals, if survey area is specified': 'Number',\n",
    "              'bootstrap density of clusters': 'BootsDensClu',\n",
    "              'bootstrap density of animals': 'BootDensity',\n",
    "              'bootstrap number of animals': 'BootNumber' },\n",
    "         fr={ 'number of samples (k)': 'NEchant',\n",
    "              'encounter rate (n/L or n/K or n/T)': 'TxContact',\n",
    "              'left truncation distance': 'DistTroncGche',\n",
    "              'right truncation distance (w)': 'DistTroncDte',\n",
    "              'total number of parameters (m)': 'NbTotPars',\n",
    "              'Log likelihood': 'LogProba',\n",
    "              'key function type': 'FnClé',\n",
    "              'adjustment series type': 'SérAjust',\n",
    "              'number of key function parameters (NKP)': 'NbParsFnClé',\n",
    "              'number of adjustment term parameters (NAP)': 'NbParsSérAjust',\n",
    "              'number of covariate parameters (NCP)': 'NbCovars',\n",
    "              'average cluster size': 'TailMoyClust',\n",
    "              'size-bias regression correlation (r)': 'CorrReg BiaisTail',\n",
    "              'p-value for correlation significance (r-p)': 'PVal SignifCorr',\n",
    "              'estimate of expected cluster size corrected for size bias': 'TailCorrCluAttEst',\n",
    "              'density of animals': 'Densité',\n",
    "              'number of animals, if survey area is specified': 'Nombre',\n",
    "              'bootstrap density of clusters': 'BootsDensClu',\n",
    "              'bootstrap density of animals': 'DensitéBoot',\n",
    "              'bootstrap number of animals': 'NombreBoot' })\n",
    "\n",
    "statr = Translator(DStatisticTrans, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans = ads.MCDSEngine.statModCols().to_frame()\n",
    "dfStatModTrans.reset_index(drop=True, inplace=True)\n",
    "dfStatModTrans.rename(columns={ 0: 'Module', 1: 'Statistic', 2: 'Figure' }, inplace=True)\n",
    "for lang in ['en', 'fr']:\n",
    "    figtr.setLang(lang)\n",
    "    statr.setLang(lang)\n",
    "    dfStatModTrans[lang] = \\\n",
    "        dfStatModTrans.apply(lambda sRow: '{} {}'.format(figtr(sRow.Figure), statr(sRow.Statistic)).strip(),\n",
    "                             axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTrans.to_csv(tgtTransFileName, sep='\\t', index=False)\n",
    "tgtTransFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=ads.MCDSAnalysis.MIRunColumns,\n",
    "             data=dict(en=['ModKeyFn', 'ModAdjSer', 'ModChcCrit', 'ConfInterv', 'LeftTrunc', 'RightTrunc',\n",
    "                           'FitDistCuts', 'DiscrDistCuts', 'RunCode', 'StartTime', 'ElapsedTime', 'RunFolder'],\n",
    "                       fr=['FnCléMod', 'SérAjustMod', 'CritChxMod', 'IntervConf', 'TroncGauche', 'TroncDroite',\n",
    "                           'TranchDistFit', 'TranchDistDiscr', 'CodeExec', 'DébutExec', 'DuréeExec', 'DossierExec']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTransExt = pd.read_csv(tgtTransFileName, sep='\\t')\n",
    "dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'fr'\n",
    "dTrans = dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])[lang].to_dict()\n",
    "results.dfData.columns = [dTrans.get(col, col) for col in results.dfData.columns]\n",
    "results.dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatModTransExt.set_index(['Module', 'Statistic', 'Figure'])[lang].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test case class\n",
    "\n",
    "(no use actually : pd.DataFrame already does the job !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super-class for test cases\n",
    "class TestCase(object):\n",
    "    def __init__(self, **attrs):\n",
    "        if not hasattr(self.__class__, 'AttributeNames'):\n",
    "            self.__class__.AttributeNames = set(attrs.keys())\n",
    "        else:\n",
    "            assert set(attrs.keys()) == self.AttributeNames, \\\n",
    "                   'Some attribute name not in frozen set {{{}}}'.format(','.join(self.AttributeNames))\n",
    "        for attrName, AttrValue in attrs.items():\n",
    "            setattr(self, attrName, AttrValue)\n",
    "    def __repr__(self):\n",
    "        return '{}({})'.format(self.__class__.__name__, ','.join('{}:{}'.format(k, v) for k, v in self.__dict__.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this super-class.\n",
    "class TCTest(TestCase):\n",
    "    pass\n",
    "\n",
    "tstTestCases = list()\n",
    "tstTestCases.append(TCTest(x=1, y='a')) # Define attributes\n",
    "tstTestCases.append(TCTest(x=2, y='b')) # Check attributes\n",
    "try:\n",
    "    tstTestCases.append(TCTest(x=2, z=None)) # Refuse new attributes\n",
    "    assert False, 'Error: New attributes should be refused'\n",
    "except AssertionError as exc:\n",
    "    print('Good refuse of new attributes:', exc)\n",
    "    \n",
    "[str(tc) for tc in tstTestCases]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise au point décodage sorties de MCDS : fichier de stats\n",
    "\n",
    "TODO: Add french translation of variables / parameters names and descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Nom et description des colonnes du tableau de stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-row-specs.txt'\n",
    "\n",
    "fStatRowSpecs = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statRowSpecLines = [line.rstrip('\\n') for line in fStatRowSpecs.readlines() if not line.startswith('#')]\n",
    "statRowSpecs =  [(statRowSpecLines[i].strip(), statRowSpecLines[i+1].strip()) \\\n",
    "                 for i in range(0, len(statRowSpecLines)-2, 3)]\n",
    "dfStatRowSpecs = pd.DataFrame(columns=['Name', 'Description'], data=statRowSpecs).set_index('Name')\n",
    "\n",
    "dfStatRowSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRowSpecs.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Numéro et description des modules et statistiques associées\n",
    "\n",
    "(colonnes Module et Statistic du tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-mod-specs.txt'\n",
    "\n",
    "fStatModSpecs = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nMaxAdjParams = 10\n",
    "\n",
    "statModSpecLines = [line.rstrip('\\n') for line in fStatModSpecs.readlines() if not line.startswith('#')]\n",
    "reModSpecNumName = re.compile('(.+) – (.+)')\n",
    "statModSpecs = list()\n",
    "moModule = None\n",
    "for line in statModSpecLines:\n",
    "    if not line:\n",
    "        continue\n",
    "    if moModule is None:\n",
    "        moModule = reModSpecNumName.match(line.strip())\n",
    "        continue\n",
    "    if line == ' ':\n",
    "        moModule = None\n",
    "        continue\n",
    "    moStatistic = reModSpecNumName.match(line.strip())\n",
    "    modNum, modDesc, statNum, statDescNotes = \\\n",
    "        moModule.group(1), moModule.group(2), moStatistic.group(1), moStatistic.group(2)\n",
    "    for i in range(len(statDescNotes)-1, -1, -1):\n",
    "        if not re.match('[\\d ,]', statDescNotes[i]):\n",
    "            statDesc = statDescNotes[:i+1]\n",
    "            statNotes = statDescNotes[i+1:].replace(' ', '')\n",
    "            break\n",
    "    modNum = int(modNum)\n",
    "    if statNum.startswith('101 '):\n",
    "        for num in range(nMaxAdjParams): # Assume no more than that ... a bit hacky !\n",
    "            statModSpecs.append((modNum, modDesc, 101+num, # Make statDesc unique for later indexing\n",
    "                                 statDesc.replace('each', 'A({})'.format(num+1)), statNotes))\n",
    "    else:\n",
    "        statNum = int(statNum)\n",
    "        if modNum == 2 and statNum == 3: # Actually, there are 0 or 3 of these ...\n",
    "            for num in range(3):\n",
    "                statModSpecs.append((modNum, modDesc, num+201,\n",
    "                                     # Change statNum & Make statDesc unique for later indexing\n",
    "                                     statDesc+' (distance set {})'.format(num+1), statNotes))\n",
    "        else:\n",
    "            statModSpecs.append((modNum, modDesc, statNum, statDesc, statNotes))\n",
    "dfStatModSpecs = pd.DataFrame(columns=['modNum', 'modDesc', 'statNum', 'statDesc', 'statNotes'],\n",
    "                              data=statModSpecs).set_index(['modNum', 'statNum'])\n",
    "\n",
    "dfStatModSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "dfStatModSpecs.modDesc.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Notes sur les statistiques des modules\n",
    "\n",
    "(infos supplémentaire indiquant comment utiliser ou pas les 5 dernières colonnes Value, Cv, Lcl, Ucl, Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'mcds/stat-mod-notes.txt'\n",
    "\n",
    "fStatModNotes = open(fileName, mode='r', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statModNoteLines = [line.rstrip('\\n') for line in fStatModNotes.readlines() if not line.startswith('#')]\n",
    "statModNotes =  [(int(line[:2]), line[2:].strip()) for line in statModNoteLines if line]\n",
    "\n",
    "dfStatModNotes = pd.DataFrame(data=statModNotes, columns=['Note', 'Text']).set_index('Note')\n",
    "\n",
    "dfStatModNotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Lecture du tableau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = mcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng.statsFileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRows = pd.read_csv(eng.statsFileName, sep=' +', engine='python', names=dfStatRowSpecs.index)\n",
    "dfStatRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Décodage du tableau\n",
    "\n",
    "Attention: On suppose 1 seule strate '0' (Stratum), 1 seul échantillon '0' (Sample) et 1 seul estimateur '1' (Estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Suppression des colonnes Stratum, Sample et Estimator\n",
    "\n",
    "(puisqu'on se limite ici aux cas où il n'y a qu'1 de chaque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStatRows.drop(columns=['Stratum', 'Sample', 'Estimator'], inplace=True)\n",
    "dfStatRows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Nettoyage des données sans objets\n",
    "\n",
    "(selon les notes descriptives des statistiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empilage des \"chiffres\" (Figures) Value, Cv, Lcl, Ucl, Df pour chaque statistique / module\n",
    "dfStats = dfStatRows.set_index(['Module', 'Statistic'], append=True).stack() \\\n",
    "                    .reset_index().rename(columns={'level_0': 'id', 'level_3': 'Figure', 0: 'Value'})\n",
    "dfStats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fix multiple Module=2 & Statistic=3 rows (before joining with self.DfStatModSpecs)\n",
    "newStatNum = 200\n",
    "for lbl, sRow in dfStats[(dfStats.Module == 2) & (dfStats.Statistic == 3)].iterrows():\n",
    "    if dfStats.loc[lbl, 'Figure'] == 'Value':\n",
    "        newStatNum += 1\n",
    "    dfStats.loc[lbl, 'Statistic'] = newStatNum\n",
    "dfStats[(dfStats.Module == 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des colonnes de description/nommage des modules et statistiques\n",
    "dfStats = dfStats.join(dfStatModSpecs, on=['Module', 'Statistic'])\n",
    "dfStats.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfStats[(dfStats.Module == 2) & (dfStats.Statistic > 200)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification que les chiffres sans objet le sont vraiment (tous à 0.0 ?)\n",
    "# Attention: Il doit y avoir un bug dans MCDS avec Module 2 / Statistic 10x : certains Cv ne sont pas nuls ...\n",
    "sKeepOnlyValueFig = ~dfStats.statNotes.str.contains('1')\n",
    "sFigs2Drop = (dfStats.Figure != 'Value') & sKeepOnlyValueFig\n",
    "assert ~dfStats[sFigs2Drop & ((dfStats.Module != 2) | (dfStats.Statistic < 100))].Value.any(), \\\n",
    "       'Attention: Des chiffres supposés \"sans objet\" on des valeurs non nulles !'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nde vérif. visuelle\n",
    "dfStats[sFigs2Drop & dfStats.Value != 0].sort_values(by='Value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des lignes / chiffres sans objet.\n",
    "dfStats.drop(dfStats[sFigs2Drop].index, inplace=True)\n",
    "dfStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats = dfStats.reindex(columns=['modDesc', 'statDesc', 'Figure', 'Value'])\n",
    "dfStats.set_index(['modDesc', 'statDesc', 'Figure'], inplace=True)\n",
    "dfStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStats.T.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance cut specs for MCDS\n",
    "\n",
    "* Mise au point\n",
    "* tests unitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distanceCutSpecs(minDist, maxDist, distCuts):\n",
    "    \n",
    "    distCutSpecs = ''\n",
    "        \n",
    "    #distCuts = params['distCuts']\n",
    "    if distCuts is not None:\n",
    "\n",
    "        if isinstance(distCuts, list) and minDist is not None and maxDist is not None:\n",
    "            distCutSpecs += ' /Int=' + ','.join(str(d) for d in [minDist] + distCuts + [maxDist])\n",
    "        elif isinstance(distCuts, int):\n",
    "            distCutSpecs += ' /NClass=' + str(distCuts)\n",
    "\n",
    "    if not isinstance(distCuts, list): # None or int\n",
    "\n",
    "        #minDist = params['minDist']\n",
    "        if minDist is not None:\n",
    "            distCutSpecs += ' /Left=' + str(minDist)\n",
    "\n",
    "        #maxDist = params['maxDist']\n",
    "        if maxDist is not None:\n",
    "            distCutSpecs += ' /Width=' + str(maxDist)\n",
    "            \n",
    "    return distCutSpecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=None) == ''\n",
    "\n",
    "assert distanceCutSpecs(minDist=5, maxDist=None, distCuts=None) == ' /Left=5'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=100, distCuts=None) == ' /Width=100'\n",
    "assert distanceCutSpecs(minDist=25.2, maxDist=100.5, distCuts=None) == ' /Left=25.2 /Width=100.5'\n",
    "\n",
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=3) == ' /NClass=3'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=300, distCuts=8) == ' /NClass=8 /Width=300'\n",
    "assert distanceCutSpecs(minDist=20, maxDist=None, distCuts=8) == ' /NClass=8 /Left=20'\n",
    "assert distanceCutSpecs(minDist=20, maxDist=300, distCuts=8) == ' /NClass=8 /Left=20 /Width=300'\n",
    "\n",
    "assert distanceCutSpecs(minDist=20, maxDist=300, distCuts=[100, 200, 230, 290]) == ' /Int=20,100,200,230,290,300'\n",
    "assert distanceCutSpecs(minDist=None, maxDist=None, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined\n",
    "assert distanceCutSpecs(minDist=0, maxDist=None, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined\n",
    "assert distanceCutSpecs(minDist=None, maxDist=4, distCuts=[1, 2, 3]) == '' # min & maxDist have to be both defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data tools development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### addAbsenceSightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfInSightings = dfObsIndiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transectCol = 'Point'\n",
    "taxonCol = 'Espece'\n",
    "sampleCols = ['Passage', 'Adulte', 'Duree']\n",
    "\n",
    "# The set of expected taxa ... of which we'll look for abscence on every location\n",
    "expectedTaxa = list(dfObsIndiv[taxonCol].unique())\n",
    "', '.join(expectedTaxa), len(expectedTaxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"abscence\" sightings to field data collected on transects for a given sample\n",
    "#def addAbsenceSightings(dfInSightings, transectCol, taxonCol, expectedTaxa, sampleCols):\n",
    "    \n",
    "def absenceSightings(taxonCol, taxon, dAbscSightTmpl):\n",
    "    dAbscSight = dAbscSightTmpl.copy()\n",
    "    dAbscSight[taxonCol] = taxon\n",
    "    return dAbscSight\n",
    "\n",
    "assert not dfInSightings.empty, 'Error : Empty sightings data to complete !'\n",
    "\n",
    "ldfAbscSightings = list()\n",
    "\n",
    "# Use 1st sightings of the sample to build the absence sightings prototype\n",
    "# (all null columns except for the sample identification ones)\n",
    "dAbscSightTmpl = dfInSightings.iloc[0].to_dict()\n",
    "dAbscSightTmpl.update({ k: None for k in dAbscSightTmpl.keys() if k not in sampleCols })\n",
    "\n",
    "# For each transect\n",
    "for transect in dfInSightings[transectCol].unique():\n",
    "\n",
    "    # Update absence sightings template with transect id\n",
    "    dAbscSightTmpl.update({ transectCol: transect })\n",
    "    \n",
    "    # Generate the absence sightings from it : 1 per lacking taxon\n",
    "    lackingTaxa = set(expectedTaxa) - set(dfInSightings.loc[dfInSightings[transectCol] == transect, taxonCol].unique())\n",
    "    dfAbscSights = pd.DataFrame([absenceSightings(taxonCol, txn, dAbscSightTmpl) for txn in lackingTaxa])\n",
    "    \n",
    "    ldfAbscSightings.append(dfAbscSights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all data frames into one.\n",
    "dfOutSightings = pd.concat([dfInSightings] + ldfAbscSightings)\n",
    "\n",
    "# Reset index (for unique labels).\n",
    "dfOutSightings.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfOutSightings), len(dfInSightings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance troncations : auto-generation of variants\n",
    "\n",
    "(at least a try ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data for playing : Individualised ones ...\n",
    "\n",
    "(output from \"4. AutoDS data tools\" above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndiv.groupby('Espece').size().sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc = dfObsIndiv[dfObsIndiv.Espece == 'Merle noir'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogramme uniforme\n",
    "_ = dfObsIndSpc.distMem.hist(figsize=(16, 3), bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.distributions.empirical_distribution as sted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecdf = sted.ECDF(dfObsIndSpc.distMem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sEcdf = pd.Series({ x : ecdf(x) for x in dfObsIndSpc.distMem.unique() }).sort_index()\n",
    "_ = sEcdf.plot(figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantiles : 2.5, 5 et 10%, left and right sides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqLims = np.array([0.025, 0.05, 0.1, 0.9, 0.95, 0.975])\n",
    "aqLims * len(dfObsIndSpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(a=dfObsIndSpc.distMem, q=aqLims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc[dfObsIndSpc.distMem <= 11.61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force combination algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lParams = list() # of dict(ltr=<left trunc dist or None>, rtr=<right trunc dist or None>, nc=<nb of cuts>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqtlLims = np.array([1.25, 2.5, 3.75, 5, 7.5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sLeftTruncs = pd.Series(index=aqtlLims, data=np.percentile(a=dfObsIndSpc.distMem, q=aqtlLims))\n",
    "for leftPct, leftTrunc in sLeftTruncs.items():\n",
    "    nRetSights = len(dfObsIndSpc[dfObsIndSpc.distMem >= leftTrunc])\n",
    "    sqrNRetSights = math.sqrt(nRetSights)\n",
    "    for nCuts in [2*sqrNRetSights/3, sqrNRetSights, 3*sqrNRetSights/2]:\n",
    "        lParams.append(dict(ltr=leftTrunc, rtr=None, nc=round(nCuts), nrs=nRetSights, pct=100-leftPct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sRightTruncs = pd.Series(index=100-aqtlLims, data=np.percentile(a=dfObsIndSpc.distMem, q=100-aqtlLims)).sort_index()\n",
    "for rightPct, rightTrunc in sRightTruncs.items():\n",
    "    nRetSights = len(dfObsIndSpc[dfObsIndSpc.distMem <= rightTrunc])\n",
    "    sqrNRetSights = math.sqrt(nRetSights)\n",
    "    for nCuts in [2*sqrNRetSights/3, sqrNRetSights, 3*sqrNRetSights/2]:\n",
    "        lParams.append(dict(ltr=None, rtr=rightTrunc, nc=round(nCuts), nrs=nRetSights, pct=rightPct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... etc ... but, why not use an optimisation engine then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(lParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimising algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndSpc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcds = ads.MCDSEngine(workDir=os.path.join('ACDC', '2019-nat-opt'),\n",
    "                      distanceUnit='Meter', areaUnit='Hectare',\n",
    "                      surveyType='Point', distanceType='Radial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDecFields = ['Effort', 'Distance']\n",
    "\n",
    "dAreaInfo = dict(Zone='ACDC', Surface=2400) # ha\n",
    "dfObsIndSpc = ads.addSurveyAreaInfo(dfObsIndSpc, dAreaInfo=dAreaInfo)\n",
    "\n",
    "dfObsIndSpc.rename(columns=dict(distMem='Distance'), inplace=True)\n",
    "dfObsIndSpc.sort_values(by='Point', inplace=True)\n",
    "\n",
    "sampDataSet = ads.DataSet(dfObsIndSpc, decimalFields=sampleDecFields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSamp = dfObsIndSpc.iloc[0]\n",
    "abrvSpe = ''.join(word[:4].title() for word in sSamp['Espece'].split(' '))\n",
    "sampAbbrev = '{}-{}-{}-{}'.format(abrvSpe, sSamp.Passage.replace('+', ''),\n",
    "                                  sSamp.Adulte.replace('+', ''), sSamp['Duree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KPreEstimCrit = 'AICC'\n",
    "KPreCVInterval = 95\n",
    "\n",
    "def dsAnalyser3(aParams, mcdsEngine, sampDataSet, sampAbbrev, keyFn, adjSer):\n",
    "\n",
    "    minDist = aParams[0]\n",
    "    maxDist = aParams[1]\n",
    "    fitDistCuts = round(aParams[2])\n",
    "    print(minDist, maxDist, fitDistCuts)\n",
    "    \n",
    "    modAbbrev = keyFn[:3].lower() + '-' + adjSer[:3].lower()\n",
    "\n",
    "    analysis = ads.MCDSAnalysis(engine=mcdsEngine, sampleDataSet=sampDataSet,\n",
    "                                name=sampAbbrev + '-' + modAbbrev, logData=False,\n",
    "                                estimKeyFn=keyFn, estimAdjustFn=adjSer,\n",
    "                                estimCriterion=KPreEstimCrit, cvInterval=KPreCVInterval,\n",
    "                                minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts)\n",
    "    \n",
    "    sResult = analysis.submit().getResults()\n",
    "\n",
    "    aic = sResult[('detection probability', 'AIC value', 'Value')]\n",
    "    \n",
    "    return aic\n",
    "\n",
    "def dsAnalyser2(aParams, mcdsEngine, sampDataSet, sampAbbrev, keyFn, adjSer, fitDistCuts):\n",
    "\n",
    "    minDist = aParams[0]\n",
    "    maxDist = aParams[1]\n",
    "    print(minDist, maxDist)\n",
    "    \n",
    "    modAbbrev = keyFn[:3].lower() + '-' + adjSer[:3].lower()\n",
    "\n",
    "    analysis = ads.MCDSAnalysis(engine=mcdsEngine, sampleDataSet=sampDataSet,\n",
    "                                name=sampAbbrev + '-' + modAbbrev, logData=False,\n",
    "                                estimKeyFn=keyFn, estimAdjustFn=adjSer,\n",
    "                                estimCriterion=KPreEstimCrit, cvInterval=KPreCVInterval,\n",
    "                                minDist=minDist, maxDist=maxDist, fitDistCuts=fitDistCuts)\n",
    "    \n",
    "    sResult = analysis.submit().getResults()\n",
    "    #print(sResult.to_dict())\n",
    "\n",
    "    #return sResult[('detection probability', 'AIC value', 'Value')]\n",
    "    return sResult[('detection probability', 'AICc', 'Value')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjSer = 'COSINE'\n",
    "keyFn = 'HNORMAL'\n",
    "#keyFn = 'HAZARD'\n",
    "#keyFn = 'UNIFORM'\n",
    "#keyFn = 'NEXPON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juste une analyse pour tester la fonction à r (AIC)\n",
    "#              minDist, maxDist, fitDistCuts\n",
    "aParams = np.array([0, 250, round(math.sqrt(len(sampDataSet.dfData)))])\n",
    "dsAnalyser3(aParams, mcds, sampDataSet, sampAbbrev, keyFn, adjSer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et maintenant, on lance l'optimisation.\n",
    "#              minDist, maxDist, fitDistCuts\n",
    "#maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[20, 80])\n",
    "#maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[40, 60])\n",
    "maxMinDist, minMaxDist = np.percentile(a=dfObsIndSpc.Distance, q=[49, 51])\n",
    "sqrNRetSights = math.sqrt(len(dfObsIndSpc))\n",
    "minFitDistCuts, maxFitDistCuts = round(2*sqrNSights/3), round(3*sqrNRetSights/2)\n",
    "paramBounds = [(0, maxMinDist), (minMaxDist, dfObsIndSpc.Distance.max()), (minFitDistCuts, maxFitDistCuts)]\n",
    "paramBounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitDistCuts = 12\n",
    "dOptRes['shgo'] = optimize.shgo(func=dsAnalyser2, bounds=paramBounds[:2], iters=2,\n",
    "                                args=(mcds, sampDataSet, sampAbbrev, keyFn, adjSer, fitDistCuts))\n",
    "dOptRes['shgo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['shgo'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['shgo'] = optimize.shgo(func=dsAnalyser3, bounds=paramBounds, iters=2,\n",
    "                                args=(mcds, sampDataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['shgo']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['da'] = optimize.dual_annealing(func=dsAnalyser3, bounds=paramBounds,\n",
    "                                        args=(mcds, sampDataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['da']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['de'] = optimize.differential_evolution(func=dsAnalyser3, bounds=paramBounds,\n",
    "                                                args=(mcds, sampDataSet, sampAbbrev, keyFn, adjSer))\n",
    "dOptRes['de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dOptRes['bh'] = optimize.basinhopping(func=dsAnalyser3, x0=[(mx+mn)/2 for mx, mn in paramBounds], stepsize=2,\n",
    "                                      minimizer_kwargs=dict(args=(mcds, sampDataSet, sampAbbrev, keyFn, adjSer)))\n",
    "dOptRes['bh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function parameters discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "    def __init__(self, a, b, xa=1, xb=2):\n",
    "        frame = inspect.currentframe()\n",
    "        args, _, _, values = inspect.getargvalues(frame)\n",
    "        print(args, values)\n",
    "        print('function name \"{}\"'.format(inspect.getframeinfo(frame)[2]))\n",
    "        for i in args:\n",
    "            print('    {} = {}'.format(i, values[i]))\n",
    "        print([(i, values[i]) for i in args])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Base(4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named tuple from dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple as ntuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(a=1, b=[3, 2], c='xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT = ntuple('NT', d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = NT(**d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending series to series ... index order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.append(pd.Series(index=[('A', 'b'), ('A', 'a'), ('B', 'c')], data=[1, 2, 3], name=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending series to DataFrame ... columns order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "#df = df.append(s, ignore_index=False) # => df.columns pas MultiIndex !\n",
    "df = df.append([s], ignore_index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('A', 'c'), ('B', 'b'), ('B', 'a')], data=[4, 5, 6], name=1)  # Mêmes colonnes : append ne retrie pas\n",
    "#s = pd.Series(index=[('A', 'a'), ('A', 'b'), ('B', 'c')], data=[4, 5, 6], name=1)  # Nouvelle colonne : append retrie\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('A', 'a'), ('B', 'c')], data=[7, 8])\n",
    "df = df.append(s, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[], data=[])\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('C', 'd')], data=[9])\n",
    "df = df.append([s], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('d',)], data=[10])\n",
    "df = df.append(s, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=pd.MultiIndex.from_tuples([('B', 'b'), ('B', 'a'), ('A', 'c')]), data=[1, 2, 3], name=0)\n",
    "df = pd.concat([df, s], axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(index=[('B', 'b'), ('B', 'a'), ('A', 'c')], data=[4, 5, 6], name=1) # Mêmes colonnes : concat ne retrie pas\n",
    "#s = pd.Series(index=[('A', 'a'), ('A', 'b'), ('B', 'c')], data=[4, 5, 6], name=1) # Nouvelle colonne : concat retrie\n",
    "df = pd.concat([df, s], axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Restore desired columns\n",
    "\n",
    "* desired order,\n",
    "* desired list of columns : new ones, and / or ignored ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new A/b, D/a and remove B/c and C/d\n",
    "i = pd.MultiIndex.from_tuples([('A', 'c'), ('A', 'b'), ('A', 'a'), ('B', 'b'), ('B', 'a'), ('D', 'a')])\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep added columns (with no data inside)\n",
    "df2 = df.reindex(i, axis='columns')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove added columns (with no data inside)\n",
    "df2 .dropna(how='all', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending partially-columned DataFrame to DataFrame\n",
    "\n",
    "with generation of lacking columns by duplicating a series = a row template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([dict(a=1, b=2, c=3), dict(a=3, b=4, c=5), dict(a=4, b=5, c=6)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(dict(x=0, y=1), name=9)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([s]*len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame([s]*len(df)).reset_index(drop=True), df], axis='columns')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=[(1, 2, 3), (4, 5, 6)], columns=pd.MultiIndex.from_tuples([('a', 'b'), ('a', 'c'), ('b', 'd')]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0, ('a', 'b')] = 9\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK: no need for passing a MultiIndex to []\n",
    "df[[('a', 'c'), ('b', 'd')]] = 9\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assymetric index and columns indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([dict(a=3, b=3, c=2), dict(a=3, b=3, c=3), dict(a=2, b=3, c=3)])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfb = df.applymap(lambda v: v == 3)\n",
    "dfb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label boolean indexing : easy !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.all(axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works ...\n",
    "df[dfb.all(axis='columns')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... and it's fast\n",
    "df[dfb.all(axis='columns')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping only works this more classical way...\n",
    "df.drop(index=df[dfb.all(axis='columns')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... and it's fast\n",
    "df.drop(index=df[dfb.all(axis='columns')].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns boolean indexing : unsymetric API (as of pandas 1.3) !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb.all(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works this tortuous way, ...\n",
    "df.T[dfb.all(axis='index')].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... but it's 3 times slower than for label indexing !\n",
    "df.T[dfb.all(axis='index')].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It works also this more classical way, ...\n",
    "df[[col for col, b in dfb.all(axis='index').items() if b]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... but it's also 3 times slower than for label indexing !\n",
    "df[[col for col, b in dfb.all(axis='index').items() if b]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.loc[df.T[dfb.all(axis='index')].index].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T[dfb.all(axis='index')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping works this classical and tortuous way also ...\n",
    "df.drop(columns=df.T[dfb.all(axis='index')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... but it's 50% slower than the following even more classical way\n",
    "df.drop(columns=df.T[dfb.all(axis='index')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping works this even more classical way also ...\n",
    "df.drop(columns=[col for col, b in dfb.all(axis='index').items() if b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "# ... and it's quite fast\n",
    "df.drop(columns=[col for col, b in dfb.all(axis='index').items() if b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/(3.15/1.92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Not in place\n",
    "dfd = pd.DataFrame([dict(e=0, g=0, v=1), dict(e=0, g=0, v=2), dict(e=0, g=0, v=3),\n",
    "                    dict(e=0, g=1, v=2), dict(e=0, g=1, v=1), dict(e=0, g=1, v=0)],\n",
    "                   index=[5, 4, 3, 2, 1, 0])\n",
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df = dfd[dfd.e == 0].sort_values(by=['g', 'v'])\n",
    "s = df.groupby('g', dropna=False).cumcount()\n",
    "dfd.loc[dfd.e == 0, 'o1'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df = dfd[dfd.e == 0]\n",
    "df = df.sort_values(by=['v'])\n",
    "s = pd.Series(data=range(len(df)), index=df.index)\n",
    "dfd.loc[dfd.e == 0, 'o2'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. in place\n",
    "dfd = pd.DataFrame([dict(e=0, g=0, v=1), dict(e=0, g=0, v=2), dict(e=0, g=0, v=3),\n",
    "                    dict(e=0, g=1, v=2), dict(e=0, g=1, v=1), dict(e=0, g=1, v=0)],\n",
    "                   index=[5, 4, 3, 2, 1, 0])\n",
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df = dfd[dfd.e == 0].copy()\n",
    "df.sort_values(by=['g', 'v'], inplace=True)\n",
    "s = df.groupby('g', dropna=False).cumcount()\n",
    "dfd.loc[dfd.e == 0, 'o1'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "df.sort_values(by=['v'], inplace=True)\n",
    "s = pd.Series(data=range(len(df)), index=df.index)\n",
    "dfd.loc[dfd.e == 0, 'o2'] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annexe B. Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-Category sightings set into an equivalent mono-Category sightings set,\n",
    "# that is where no sightings has more that one category with positive count (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 Category count columns nMales and nFemales\n",
    "#     * in the input set, you may have 1 sightings with nMales = 5 and nFemales = 2\n",
    "#     * in the output set, this sightings have been separated in 2 distinct ones (all other properties left untouched) :\n",
    "#       the 1st with nMales = 5 and nFemales = 0, the 2nd with nMales = 0 and nFemales = 2.\n",
    "\n",
    "# A slower version or ads.separateMultiCategoryCounts :\n",
    "#  from 9.5s to 0.1s with countColumns = [nMalAd1,nAutAd10,nJuv10,nDetTot10,nMalAd5,nAutAd5,nJuv5,nDetTot5,nTotAd5,nTotAd10]\n",
    "#  on the \"ACDC 2019 Naturalist\" multi-Category data set (~4000 rows)\n",
    "def separateMultiCategoryCounts_slow_version(dfInSightings, countColumns):\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "        \n",
    "        # [a little optimisation ?] If this is already a mono-Category sightings, simply append it as is.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        sCounts = sCounts[sCounts > 0]\n",
    "        if len(sCounts) == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # If it is a multi-Category sightings, we need to split it down.\n",
    "        for col in sCounts.index:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[countColumns] = 0\n",
    "            sOutSight[col] = sInSight[col]\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=np.arange(len(outSightings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsMonoCat_slow = separateMultiCategoryCounts_slow_version(dfObs, countCols)\n",
    "len(dfObsMonoCat_slow), dfObsMonoCat_slow[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-individual mono-Category sightings set into an equivalent mono-individual mono-Category sightings set\n",
    "# that is where no sightings has more that one individual per category (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 mono-Category count columns nMales and nFemales\n",
    "#     In input set, you may have 1 sightings with nMales = 3 and nFemales = 0 (but none with nMales and nFemales > 0)\n",
    "#     In out set, no : this sightings have been separated in 3 distinct ones (all other properties left untouched) :\n",
    "#                      all with nMales = 1 and nFemales = 0.\n",
    "\n",
    "# A slower version or ads.individualiseMonoCategoryCounts :\n",
    "#  from 15.5s to 0.06s with countColumns = [nMalAd1,nAutAd10,nJuv10,nDetTot10,nMalAd5,nAutAd5,nJuv5,nDetTot5,nTotAd5,nTotAd10]\n",
    "#  on the \"ACDC 2019 Naturalist\" mono-Category data set (~20000 rows)\n",
    "def individualiseMonoCategoryCounts_slow(dfInSightings, countColumns):\n",
    "\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "\n",
    "        # [a little check] Multi-Category sightings not supported here.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        sCounts = sCounts[sCounts > 0]\n",
    "        assert len(sCounts) == 1, 'Error: Multi-Category sightings not supported ' + str(lbl, sInSight)\n",
    "        \n",
    "        # Get the positive count column and its value\n",
    "        posCol = sCounts.index[0]\n",
    "        count = sCounts[posCol]\n",
    "\n",
    "        # [a little optimisation ?] If this is a mono-individual sightings, simply append it as is.\n",
    "        if count == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # If it is a multi-individual sightings, we need to split it down.\n",
    "        while count > 0:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[posCol] = 1\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "            count -= 1\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=np.arange(len(outSightings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndiv_slow = individualiseMonoCategoryCounts_slow(dfObsMonoCat_slow, countCols)\n",
    "len(dfObsIndiv_slow), dfObsIndiv_slow[countCols].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a multi-individual multi-Category sightings set into an equivalent mono-individual multi-Category sightings set\n",
    "# that is where no sightings has more that one individual per category (keeping the same total counts).\n",
    "# Ex: A sightings set with 2 Category count columns nMales and nFemales\n",
    "#     In input set, you may have 1 sightings with nMales = 5 and nFemales = 2\n",
    "#     In out set, no : this sightings have been separated in 5 distinct ones (all other properties left untouched) :\n",
    "#                      the 2 1st ones with nMales = 1 and nFemales = 1, the last 3 ones with nMales = 1 and nFemales = 0.\n",
    "\n",
    "# Finally, of no use : simply chain ads.separateMultiCategoryCounts and ads.individualiseMonoCategoryCounts\n",
    "# And from far much slower !\n",
    "def individualiseMultiCategoryCounts(dfInSightings, countColumns):\n",
    "    \n",
    "    outSightings = list()\n",
    "\n",
    "    for lbl, sInSight in dfInSightings.iterrows():\n",
    "\n",
    "        # [a little optimisation ?] If this is a mono-individual sightings, simply append it as is.\n",
    "        sCounts = sInSight[countColumns]\n",
    "        if sCounts.max() == 1:\n",
    "            \n",
    "            outSightings.append(sInSight)\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # If it is a multi-individual sightings, we need to split it down.\n",
    "        sCounts = sCounts.copy()\n",
    "\n",
    "        while sCounts.max() > 0:\n",
    "\n",
    "            sOutSight = sInSight.copy()\n",
    "            sOutSight[countColumns] = sCounts.apply(lambda n: 1 if n > 0 else 0)\n",
    "\n",
    "            outSightings.append(sOutSight)\n",
    "\n",
    "            sCounts = sCounts.apply(lambda n: n-1 if n > 0 else 0)\n",
    "\n",
    "    return pd.DataFrame(data=outSightings, index=list(range(len(outSightings))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \"abscence\" sightings to field data collected on transects for a given sample\n",
    "# Warning: A special version for an all-taxon data set\n",
    "# * dfInSights : input data table\n",
    "# * transectCol : the name of the transect id column\n",
    "# * taxonSampleCol : the name of the taxon id column\n",
    "# * otherSampleCols : the names of the other sample id columns (taxon id not included)\n",
    "# * expectedTaxa : the expected taxon ids : absence sightings are there to make sure\n",
    "#                  all of the taxa are found at least once in the output data table\n",
    "def addAbsenceSightings(dfInSights, transectCol, taxonSampleCol, otherSampleCols, expectedTaxa):\n",
    "    \n",
    "    def absenceSightings(taxonCol, taxon, dAbscSightTmpl):\n",
    "        dAbscSight = dAbscSightTmpl.copy()\n",
    "        dAbscSight[taxonCol] = taxon\n",
    "        return dAbscSight\n",
    "\n",
    "    assert not dfInSights.empty, 'Error : Empty sightings data to complete !'\n",
    "\n",
    "    ldfAbscSights = list()\n",
    "\n",
    "    # Use 1st sightings of the sample to build the absence sightings prototype\n",
    "    # (all null columns except for the sample identification ones)\n",
    "    dAbscSightTmpl = dfInSights.iloc[0].to_dict()\n",
    "    dAbscSightTmpl.update({ k: None for k in dAbscSightTmpl.keys() if k not in otherSampleCols })\n",
    "\n",
    "    # For each transect\n",
    "    for transect in dfInSights[transectCol].unique():\n",
    "\n",
    "        # Update absence sightings template with transect id\n",
    "        dAbscSightTmpl.update({ transectCol: transect })\n",
    "\n",
    "        # Generate the absence sightings from it : 1 per lacking taxon\n",
    "        lackingTaxa = \\\n",
    "          set(expectedTaxa) - set(dfInSights.loc[dfInSights[transectCol] == transect, taxonSampleCol].unique())\n",
    "        dfAbscSights = pd.DataFrame([absenceSightings(taxonSampleCol, txn, dAbscSightTmpl) for txn in lackingTaxa])\n",
    "\n",
    "        # Save the data frame for later\n",
    "        ldfAbscSights.append(dfAbscSights)\n",
    "\n",
    "    # Concat all absence data frames into one.\n",
    "    dfOutSights = pd.concat([dfInSights] + ldfAbscSights)\n",
    "\n",
    "    # Reset index (for unique labels).\n",
    "    dfOutSights.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    # Done.\n",
    "    return dfOutSights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests unitaires de addAbsenceSightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transect, taxon and sample columns\n",
    "transectCol = 'Point'\n",
    "taxonCol = 'Espece'\n",
    "sampleCols = ['Passage', 'Adulte', 'Duree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The set of expected taxa ... of which we'll look for abscence on every location\n",
    "expectedTaxa = list(dfObsIndiv[taxonCol].unique())\n",
    "\n",
    "assert len(expectedTaxa) == 58\n",
    "\n",
    "', '.join(expectedTaxa), len(expectedTaxa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 1 random sample\n",
    "passage = 'a'\n",
    "adulte = 'm'\n",
    "duree = '10'\n",
    "dfObsIndivSmpl = dfObsIndiv[(dfObsIndiv.Passage == passage) & (dfObsIndiv.Adulte == adulte) & (dfObsIndiv.Duree == duree)]\n",
    "\n",
    "assert len(dfObsIndivSmpl) == 322 and dfObsIndivSmpl[transectCol].nunique() == 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, transectCol, taxonCol, expectedTaxa, sampleCols)\n",
    "len(dfObsIndivAbscSmpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for no change in sample columns\n",
    "assert list(dfObsIndivAbscSmpl.columns) == list(dfObsIndivSmpl.columns)\n",
    "\n",
    "# Check for number of added rows\n",
    "assert len(dfObsIndivAbscSmpl) == 1333\n",
    "\n",
    "# Check for no change in number of transect and taxa\n",
    "assert dfObsIndivAbscSmpl[transectCol].nunique() == 21 and dfObsIndivAbscSmpl[taxonCol].nunique() == 58\n",
    "\n",
    "# Check for no change in sample identification\n",
    "assert list(dfObsIndivAbscSmpl.Passage.unique()) == [passage]\n",
    "assert list(dfObsIndivAbscSmpl.Adulte.unique()) == [adulte]\n",
    "assert list(dfObsIndivAbscSmpl.Duree.unique()) == [duree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem']).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfObsIndivAbscSmpl.sort_values(by=['Passage', 'Observateur', 'Point', 'Espece', 'distMem']).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Performance test\n",
    "print('Passage  Adulte Duree NbDonnees')\n",
    "\n",
    "for passage in ['a', 'b', 'a+b']: \n",
    "    \n",
    "    for adulte in ['m', 'a', 'm+a']:\n",
    "    \n",
    "        for duree in ['5', '10']:\n",
    "            \n",
    "            passages = passage.split('+')\n",
    "            adultes = adulte.split('+')\n",
    "            dfObsIndivSmpl = dfObsIndiv[dfObsIndiv.Passage.isin(passages) & dfObsIndiv.Adulte.isin(adultes) \\\n",
    "                                        & (dfObsIndiv.Duree == duree)]\n",
    "            \n",
    "            dfObsIndivAbscSmpl = ads.addAbsenceSightings(dfObsIndivSmpl, transectCol, taxonCol, expectedTaxa, sampleCols)\n",
    "            \n",
    "            print(passage, adulte, duree, ':', len(dfObsIndivAbscSmpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCDSAnalysisResultsSet quality indicators functions dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as plygo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normNTotPars(nTotPars, a=0.2, b=0.6, c=2, d=1):\n",
    "        #return 1 / (a * sRes[cls.CLNTotPars] + b)  # Trop pénalisant: a=0.2, b=1\n",
    "        return 1 / (a * max(c, nTotPars)**d + b)  # Mieux: a=0.2, b=0.6 / a=0.2, b=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDomain = [0, 1, 2, 3, 4, 5, 6, 8, 10 , 12]\n",
    "paramSets = [dict(a=0.2, b=0.6, c=2, d=1), dict(a=0.2, b=0.8, c=1, d=1), dict(a=0.3, b=0.7, c=1, d=1),\n",
    "             dict(a=0.2, b=0.8, c=1, d=2), dict(a=0.2, b=0.8, c=1, d=1.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plygo.Figure()\n",
    "\n",
    "for params in paramSets:\n",
    "    fig.add_trace(plygo.Scatter(x=xDomain,\n",
    "                                y=[normNTotPars(nTotPars, **params) for nTotPars in xDomain],\n",
    "                                name=str(params)))\n",
    "\n",
    "fig.update_layout(title='NTotPars normalisation')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normCVDens(dCv, a=12):\n",
    "        #return max(0, 1 - a * sRes[cls.CLDCv]) # Pas très pénalisant: a=1\n",
    "        return math.exp(-a * dCv ** 2) # Mieux : déjà ~0.33 à 30% (a=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xDomain = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.15, 0.30, 0.40, 0.5, 0.7, 1.0]\n",
    "paramSets = [dict(a=8), dict(a=12), dict(a=14), dict(a=16), dict(a=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plygo.Figure()\n",
    "\n",
    "for params in paramSets:\n",
    "    fig.add_trace(plygo.Scatter(x=xDomain,\n",
    "                                y=[normCVDens(cvDens, **params) for cvDens in xDomain],\n",
    "                                name=str(params)))\n",
    "\n",
    "fig.update_layout(title='CVDens normalisation')\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check python derivation and class methods / attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base(object):\n",
    "    \n",
    "    A = 'Base.A'\n",
    "    B = 'Base.B'\n",
    "    \n",
    "    def f(self):\n",
    "        print('Base.f')\n",
    "        return self.g()\n",
    "        \n",
    "    def g(self):\n",
    "        print('Base.g')\n",
    "        return self.A\n",
    "        \n",
    "class Derived(Base):\n",
    "    \n",
    "    A = 'Derived.A'\n",
    "    \n",
    "    @classmethod\n",
    "    def h(cls):\n",
    "        print('Derived.h: A=', cls.A)\n",
    "        return cls.A\n",
    "\n",
    "    def g(self):\n",
    "        print('Derived.g')\n",
    "        return self.h()\n",
    "\n",
    "d = Derived()\n",
    "\n",
    "assert d.f() == 'Derived.A'\n",
    "\n",
    "d.B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of python code lines in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fn in ['./unintests.ipynb', './valtests.ipynb']:\n",
    "    print(fn, end=': ')\n",
    "    with open(fn) as file:\n",
    "        d = json.load(file)\n",
    "    print(sum(len(cell['source']) for cell in d['cells']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(dict(a=1, b=2))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = pd.Series(dict(c=1, d=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s2.append(s)\n",
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 is s2, s3 is s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = 'tmp/mcds-optr/optr-resbak-1.pickle.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.open(fileName, 'rb') as file:\n",
    "    dfData, specs = pickle.load(file)\n",
    "    \n",
    "len(dfData), dfData.columns, len(dfData.columns), dfData.columns.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileName = '../tmp/acdc2019-nat-optr-resbak-0.pickle.xz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with lzma.open(fileName, 'rb') as file:\n",
    "    res = pickle.load(file)\n",
    "    \n",
    "type(res), len(res._dfData), res._dfData.columns, len(res._dfData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res._dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = pd.MultiIndex.from_tuples([('sample stats', 'total number of observations', 'Value'),\n",
    "                                                ('sample stats', 'minimal observation distance', 'Value'),\n",
    "                                                ('sample stats', 'maximal observation distance', 'Value')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ('sample stats', 'minimal observation distance', 'Value') in mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'a':1, **{k:k for k in ('z', 'e')}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = json.load(open('./valtests.ipynb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
